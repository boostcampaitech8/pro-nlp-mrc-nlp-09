import logging
import shutil
import os
import sys
import random
import numpy as np
import torch
import evaluate
from typing import NoReturn
from train_process import normalize_text, safe_normalize, apply_clean
from src.arguments import DataTrainingArguments, ModelArguments, CustomTrainingArguments
from datasets import DatasetDict, load_from_disk
from src.trainer_qa import QuestionAnsweringTrainer
from transformers import (
    AutoConfig,
    AutoModelForQuestionAnswering,
    AutoTokenizer,
    DataCollatorWithPadding,
    EvalPrediction,
    HfArgumentParser,
    TrainingArguments,
    set_seed,
)

from src.utils import (
    check_no_error,
    postprocess_qa_predictions,
    wait_for_gpu_availability,
    get_config, to_serializable, print_section,
    get_logger
)

seed = 2024
deterministic = False

random.seed(seed)  # python random seed ê³ ì •
np.random.seed(seed)  # numpy random seed ê³ ì •
torch.manual_seed(seed)  # torch random seed ê³ ì •
torch.cuda.manual_seed_all(seed)
if deterministic:  # cudnn random seed ê³ ì • - ê³ ì • ì‹œ í•™ìŠµ ì†ë„ê°€ ëŠë ¤ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

logger = get_logger(__name__)


def main():
    # ê°€ëŠ¥í•œ arguments ë“¤ì€ ./arguments.py ë‚˜ transformer package ì•ˆì˜ src/transformers/training_args.py ì—ì„œ í™•ì¸ ê°€ëŠ¥í•©ë‹ˆë‹¤.
    # --help flag ë¥¼ ì‹¤í–‰ì‹œì¼œì„œ í™•ì¸í•  ìˆ˜ ë„ ìˆìŠµë‹ˆë‹¤.

    parser = HfArgumentParser(
        (ModelArguments, DataTrainingArguments, CustomTrainingArguments)
    )

    model_args, data_args, training_args = get_config(parser)

    #
    training_args.do_train = True
    training_args.do_eval = True

    # train.pyëŠ” "í•™ìŠµ ì „ìš©" ìŠ¤í¬ë¦½íŠ¸ë¡œ ì‚¬ìš©
    if not training_args.do_train:
        raise ValueError(
            "train.pyëŠ” í•™ìŠµ ì „ìš© ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤. "
            "TrainingArguments.do_train=Trueë¡œ ì„¤ì •í•œ YAMLì„ ì‚¬ìš©í•˜ì„¸ìš”."
        )

    logger.info("model is from: %s", model_args.model_name_or_path)
    logger.info("data is from: %s", data_args.train_dataset_name)
    logger.info("output_dir is: %s", training_args.output_dir)

    # gpu ì‚¬ìš© ê°€ëŠ¥í•œì§€ ì²´í¬
    wait_for_gpu_availability()

    # í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ argumentsë¥¼ í•œ ë²ˆì— ë¡œê·¸ë¡œ ë‚¨ê²¨ë‘ê¸°
    print_section("Model Arguments", model_args)
    print_section("Data Training Arguments", data_args)
    print("Trainging Arguments:")
    print(f"output_dir: {training_args.output_dir})")
    print(f"num_train_epochs: {training_args.num_train_epochs}")
    print(f"per_device_train_batch_size: {training_args.per_device_train_batch_size}")
    print(f"per_device_eval_batch_size: {training_args.per_device_eval_batch_size}")
    print(f"learning_rate: {training_args.learning_rate}")
    print(f"warmup_ratio: {training_args.warmup_ratio}")
    print(f"weight_decay: {training_args.weight_decay}")
    print(f"logging_steps: {training_args.logging_steps}")
    print(f"logging_first_step: {training_args.logging_first_step}")
    print(f"evaluation_strategy: {training_args.evaluation_strategy}")
    print(f"save_strategy: {training_args.save_strategy}")
    print(f"save_total_limit: {training_args.save_total_limit}")
    print(f"load_best_model_at_end: {training_args.load_best_model_at_end}")
    print(f"metric_for_best_model: {training_args.metric_for_best_model}")
    print(f"greater_is_better: {training_args.greater_is_better}")
    print(f"fp16: {training_args.fp16}")
    print(f"dataloader_num_workers: {training_args.dataloader_num_workers}")
    print(f"gradient_accumulation_steps: {training_args.gradient_accumulation_steps}")
    print(f"report_to: {training_args.report_to}")

    # ëª¨ë¸ì„ ì´ˆê¸°í™”í•˜ê¸° ì „ì— ë‚œìˆ˜ë¥¼ ê³ ì •í•©ë‹ˆë‹¤.
    set_seed(training_args.seed)

    datasets = load_from_disk(data_args.train_dataset_name)
    logger.info("load datasets: \n", datasets)

    # AutoConfigë¥¼ ì´ìš©í•˜ì—¬ pretrained model ê³¼ tokenizerë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
    # argumentë¡œ ì›í•˜ëŠ” ëª¨ë¸ ì´ë¦„ì„ ì„¤ì •í•˜ë©´ ì˜µì…˜ì„ ë°”ê¿€ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    config = AutoConfig.from_pretrained(
        model_args.config_name
        if model_args.config_name is not None
        else model_args.model_name_or_path,
    )
    tokenizer = AutoTokenizer.from_pretrained(
        model_args.tokenizer_name
        if model_args.tokenizer_name is not None
        else model_args.model_name_or_path,
        # 'use_fast' argumentë¥¼ Trueë¡œ ì„¤ì •í•  ê²½ìš° rustë¡œ êµ¬í˜„ëœ tokenizerë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        # Falseë¡œ ì„¤ì •í•  ê²½ìš° pythonìœ¼ë¡œ êµ¬í˜„ëœ tokenizerë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìœ¼ë©°,
        # rust versionì´ ë¹„êµì  ì†ë„ê°€ ë¹ ë¦…ë‹ˆë‹¤.
        use_fast=True,
    )
    model = AutoModelForQuestionAnswering.from_pretrained(
        model_args.model_name_or_path,
        from_tf=bool(".ckpt" in model_args.model_name_or_path),
        config=config,
    )

    logger.info(
        f"training_args type: {type(training_args)}, "
        f"model_args type: {type(model_args)}, "
        f"datasets type: {type(datasets)}, "
        f"tokenizer type: {type(tokenizer)}, "
        f"model type: {type(model)}"
    )

    run_mrc(data_args, training_args, model_args, datasets, tokenizer, model,config)


def run_mrc(
    data_args: DataTrainingArguments,
    training_args: TrainingArguments,
    model_args: ModelArguments,
    datasets: DatasetDict,
    tokenizer,
    model,
    config,
) -> NoReturn:

    # datasetì„ ì „ì²˜ë¦¬í•©ë‹ˆë‹¤.
    if training_args.do_train:
        column_names = datasets["train"].column_names
    else:
        column_names = datasets["validation"].column_names

    question_column_name = "question" if "question" in column_names else column_names[0]
    context_column_name = "context" if "context" in column_names else column_names[1]
    answer_column_name = "answers" if "answers" in column_names else column_names[2]

    # Paddingì— ëŒ€í•œ ì˜µì…˜ì„ ì„¤ì •í•©ë‹ˆë‹¤.
    # (question|context) í˜¹ì€ (context|question)ë¡œ ì„¸íŒ… ê°€ëŠ¥í•©ë‹ˆë‹¤.
    pad_on_right = tokenizer.padding_side == "right"

    # ëª¨ë¸ íƒ€ì…ì— ë”°ë¼ token_type_ids ì‚¬ìš© ì—¬ë¶€ ê²°ì •
    # RoBERTa, DeBERTa, ELECTRA ë“±ì€ token_type_idsë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
    # ì €ì¥ëœ ëª¨ë¸ì˜ ê²½ìš° configì—ì„œ model_typeì„ í™•ì¸
    model_type = getattr(config, 'model_type', '').lower()
    model_name_lower = model_args.model_name_or_path.lower()
    use_token_type_ids = not any(
        mt in model_name_lower or mt in model_type
        for mt in ['roberta', 'deberta', 'electra', 'xlm']
    )
    print(f"Model type: {model_type}, use_token_type_ids: {use_token_type_ids}")

    # ì˜¤ë¥˜ê°€ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
    last_checkpoint, max_seq_length = check_no_error(
        data_args, training_args, datasets, tokenizer
    )

    # Train preprocessing / ì „ì²˜ë¦¬ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.

    def prepare_train_features(examples):
        # truncationê³¼ padding(lengthê°€ ì§§ì„ë•Œë§Œ)ì„ í†µí•´ toknizationì„ ì§„í–‰í•˜ë©°, strideë¥¼ ì´ìš©í•˜ì—¬ overflowë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.
        # ê° exampleë“¤ì€ ì´ì „ì˜ contextì™€ ì¡°ê¸ˆì”© ê²¹ì¹˜ê²Œë©ë‹ˆë‹¤.
        tokenized_examples = tokenizer(
            examples[question_column_name if pad_on_right else context_column_name],
            examples[context_column_name if pad_on_right else question_column_name],
            truncation="only_second" if pad_on_right else "only_first",
            max_length=max_seq_length,
            stride=data_args.doc_stride,
            return_overflowing_tokens=True,
            return_offsets_mapping=True,
            return_token_type_ids=use_token_type_ids,  # BERT: True, RoBERTa/DeBERTa/ELECTRA: False
            padding="max_length" if data_args.pad_to_max_length else False,
        )

        # ê¸¸ì´ê°€ ê¸´ contextê°€ ë“±ì¥í•  ê²½ìš° truncateë¥¼ ì§„í–‰í•´ì•¼í•˜ë¯€ë¡œ, í•´ë‹¹ ë°ì´í„°ì…‹ì„ ì°¾ì„ ìˆ˜ ìˆë„ë¡ mapping ê°€ëŠ¥í•œ ê°’ì´ í•„ìš”í•©ë‹ˆë‹¤.
        sample_mapping = tokenized_examples.pop("overflow_to_sample_mapping")
        # tokenì˜ ìºë¦­í„° ë‹¨ìœ„ positionë¥¼ ì°¾ì„ ìˆ˜ ìˆë„ë¡ offset mappingì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
        # start_positionsê³¼ end_positionsì„ ì°¾ëŠ”ë° ë„ì›€ì„ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        offset_mapping = tokenized_examples.pop("offset_mapping")

        # ë°ì´í„°ì…‹ì— "start position", "enc position" labelì„ ë¶€ì—¬í•©ë‹ˆë‹¤.
        tokenized_examples["start_positions"] = []
        tokenized_examples["end_positions"] = []

        for i, offsets in enumerate(offset_mapping):
            input_ids = tokenized_examples["input_ids"][i]
            cls_index = input_ids.index(tokenizer.cls_token_id)  # cls index

            # sequence idë¥¼ ì„¤ì •í•©ë‹ˆë‹¤ (to know what is the context and what is the question).
            sequence_ids = tokenized_examples.sequence_ids(i)

            # í•˜ë‚˜ì˜ exampleì´ ì—¬ëŸ¬ê°œì˜ spanì„ ê°€ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            sample_index = sample_mapping[i]
            answers = examples[answer_column_name][sample_index]

            # answerê°€ ì—†ì„ ê²½ìš° cls_indexë¥¼ answerë¡œ ì„¤ì •í•©ë‹ˆë‹¤(== exampleì—ì„œ ì •ë‹µì´ ì—†ëŠ” ê²½ìš° ì¡´ì¬í•  ìˆ˜ ìˆìŒ).
            if len(answers["answer_start"]) == 0:
                tokenized_examples["start_positions"].append(cls_index)
                tokenized_examples["end_positions"].append(cls_index)
            else:
                # textì—ì„œ ì •ë‹µì˜ Start/end character index
                start_char = answers["answer_start"][0]
                end_char = start_char + len(answers["text"][0])

                # textì—ì„œ current spanì˜ Start token index
                token_start_index = 0
                while sequence_ids[token_start_index] != (1 if pad_on_right else 0):
                    token_start_index += 1

                # textì—ì„œ current spanì˜ End token index
                token_end_index = len(input_ids) - 1
                while sequence_ids[token_end_index] != (1 if pad_on_right else 0):
                    token_end_index -= 1

                # ì •ë‹µì´ spanì„ ë²—ì–´ë‚¬ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤(ì •ë‹µì´ ì—†ëŠ” ê²½ìš° CLS indexë¡œ labelë˜ì–´ìˆìŒ).
                if not (
                    offsets[token_start_index][0] <= start_char
                    and offsets[token_end_index][1] >= end_char
                ):
                    tokenized_examples["start_positions"].append(cls_index)
                    tokenized_examples["end_positions"].append(cls_index)
                else:
                    # token_start_index ë° token_end_indexë¥¼ answerì˜ ëìœ¼ë¡œ ì´ë™í•©ë‹ˆë‹¤.
                    # Note: answerê°€ ë§ˆì§€ë§‰ ë‹¨ì–´ì¸ ê²½ìš° last offsetì„ ë”°ë¼ê°ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤(edge case).
                    while (
                        token_start_index < len(offsets)
                        and offsets[token_start_index][0] <= start_char
                    ):
                        token_start_index += 1
                    tokenized_examples["start_positions"].append(token_start_index - 1)
                    while offsets[token_end_index][1] >= end_char:
                        token_end_index -= 1
                    tokenized_examples["end_positions"].append(token_end_index + 1)

        return tokenized_examples

    if training_args.do_train:
        if "train" not in datasets:
            raise ValueError("--do_train requires a train dataset")
        train_dataset = datasets["train"]
    
        # ==============================================
        # â­ Cleaning ì ìš© (context/question ì •ê·œí™”)
        # ==============================================
        if data_args.apply_cleaning:
            logger.info("ğŸ”§ Applying text normalization to TRAIN dataset...")
            train_dataset = train_dataset.map(
                lambda x: {
                    **x,
                    "context": safe_normalize(x["context"]),
                    "question": safe_normalize(x["question"]),
                },
                num_proc=data_args.preprocessing_num_workers,
                desc="Cleaning train dataset"
            )

        # datasetì—ì„œ train featureë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        train_dataset = train_dataset.map(
            prepare_train_features,
            batched=True,
            num_proc=data_args.preprocessing_num_workers,
            remove_columns=column_names,
            load_from_cache_file=not data_args.overwrite_cache,
        )

    # Validation preprocessing
    def prepare_validation_features(examples):
        # truncationê³¼ padding(lengthê°€ ì§§ì„ë•Œë§Œ)ì„ í†µí•´ toknizationì„ ì§„í–‰í•˜ë©°, strideë¥¼ ì´ìš©í•˜ì—¬ overflowë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.
        # ê° exampleë“¤ì€ ì´ì „ì˜ contextì™€ ì¡°ê¸ˆì”© ê²¹ì¹˜ê²Œë©ë‹ˆë‹¤.
        tokenized_examples = tokenizer(
            examples[question_column_name if pad_on_right else context_column_name],
            examples[context_column_name if pad_on_right else question_column_name],
            truncation="only_second" if pad_on_right else "only_first",
            max_length=max_seq_length,
            stride=data_args.doc_stride,
            return_overflowing_tokens=True,
            return_offsets_mapping=True,
            return_token_type_ids=use_token_type_ids,  # BERT: True, RoBERTa/DeBERTa/ELECTRA: False
            padding="max_length" if data_args.pad_to_max_length else False,
        )

        # ê¸¸ì´ê°€ ê¸´ contextê°€ ë“±ì¥í•  ê²½ìš° truncateë¥¼ ì§„í–‰í•´ì•¼í•˜ë¯€ë¡œ, í•´ë‹¹ ë°ì´í„°ì…‹ì„ ì°¾ì„ ìˆ˜ ìˆë„ë¡ mapping ê°€ëŠ¥í•œ ê°’ì´ í•„ìš”í•©ë‹ˆë‹¤.
        sample_mapping = tokenized_examples.pop("overflow_to_sample_mapping")

        # evaluationì„ ìœ„í•´, predictionì„ contextì˜ substringìœ¼ë¡œ ë³€í™˜í•´ì•¼í•©ë‹ˆë‹¤.
        # corresponding example_idë¥¼ ìœ ì§€í•˜ê³  offset mappingsì„ ì €ì¥í•´ì•¼í•©ë‹ˆë‹¤.
        tokenized_examples["example_id"] = []

        for i in range(len(tokenized_examples["input_ids"])):
            # sequence idë¥¼ ì„¤ì •í•©ë‹ˆë‹¤ (to know what is the context and what is the question).
            sequence_ids = tokenized_examples.sequence_ids(i)
            context_index = 1 if pad_on_right else 0

            # í•˜ë‚˜ì˜ exampleì´ ì—¬ëŸ¬ê°œì˜ spanì„ ê°€ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            sample_index = sample_mapping[i]
            tokenized_examples["example_id"].append(examples["id"][sample_index])

            # Set to None the offset_mappingì„ Noneìœ¼ë¡œ ì„¤ì •í•´ì„œ token positionì´ contextì˜ ì¼ë¶€ì¸ì§€ ì‰½ê²Œ íŒë³„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            tokenized_examples["offset_mapping"][i] = [
                (o if sequence_ids[k] == context_index else None)
                for k, o in enumerate(tokenized_examples["offset_mapping"][i])
            ]
        return tokenized_examples

    eval_dataset = datasets["validation"]
    if data_args.apply_cleaning:
        logger.info("ğŸ”§ Applying text normalization to VALIDATION dataset...")
        eval_dataset = eval_dataset.map(
            lambda x: {
                **x,
                "context": safe_normalize(x["context"]),
                "question": safe_normalize(x["question"]),
            },
            num_proc=data_args.preprocessing_num_workers,
            desc="Cleaning validation dataset"
        )
    # Validation Feature ìƒì„±
    eval_dataset = eval_dataset.map(
        prepare_validation_features,
        batched=True,
        num_proc=data_args.preprocessing_num_workers,
        remove_columns=column_names,
        load_from_cache_file=not data_args.overwrite_cache,
    )

    # Data collator
    # flagê°€ Trueì´ë©´ ì´ë¯¸ max lengthë¡œ paddingëœ ìƒíƒœì…ë‹ˆë‹¤.
    # ê·¸ë ‡ì§€ ì•Šë‹¤ë©´ data collatorì—ì„œ paddingì„ ì§„í–‰í•´ì•¼í•©ë‹ˆë‹¤.
    data_collator = DataCollatorWithPadding(
        tokenizer, pad_to_multiple_of=8 if training_args.fp16 else None
    )

    # Post-processing:
    def post_processing_function(examples, features, predictions, training_args):
        # Post-processing: start logitsê³¼ end logitsì„ original contextì˜ ì •ë‹µê³¼ matchì‹œí‚µë‹ˆë‹¤.
        predictions = postprocess_qa_predictions(
            examples=examples,
            features=features,
            predictions=predictions,
            max_answer_length=data_args.max_answer_length,
            output_dir=training_args.output_dir,
        )
        # Metricì„ êµ¬í•  ìˆ˜ ìˆë„ë¡ Formatì„ ë§ì¶°ì¤ë‹ˆë‹¤.
        formatted_predictions = [
            {"id": k, "prediction_text": v} for k, v in predictions.items()
        ]
        # í•­ìƒ EvalPrediction ë°˜í™˜
        references = [
            {"id": ex["id"], "answers": ex[answer_column_name]}
            for ex in datasets["validation"]
        ]
        return EvalPrediction(predictions=formatted_predictions, label_ids=references)

    metric = evaluate.load("squad")
    logger.info("---- metric loaded: %s ----", metric)

    def compute_metrics(p: EvalPrediction):
        return metric.compute(predictions=p.predictions, references=p.label_ids)

    # Trainer ì´ˆê¸°í™”
    trainer = QuestionAnsweringTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset if training_args.do_train else None,
        eval_dataset=eval_dataset,
        eval_examples=datasets["validation"],
        tokenizer=tokenizer,
        data_collator=data_collator,
        post_process_function=post_processing_function,
        compute_metrics=compute_metrics,
    )

    # Training (fresh run ë°©ì‹ìœ¼ë¡œ ìˆ˜ì •; í•„ìš”í•˜ë©´ YAMLì— resume_from_checkpoint ëª…ì‹œ)
    logger.info(
        "Starting training: model=%s, output_dir=%s",
        model_args.model_name_or_path,
        training_args.output_dir,
    )

    train_result = trainer.train(
        resume_from_checkpoint=getattr(training_args, "resume_from_checkpoint", None)
    )

    logger.info("Training completed.")
    logger.info("Saving model to %s", training_args.output_dir)
    logger.info(f"ìµœì¢… í›ˆë ¨ ê²°ê³¼: {train_result.metrics}")

    trainer.save_model()  # tokenizerê¹Œì§€ í•¨ê»˜ ì €ì¥
    trainer.save_state()

    metrics = train_result.metrics
    metrics["train_samples"] = len(train_dataset)

    trainer.log_metrics("train", metrics)
    trainer.save_metrics("train", metrics)

    output_train_file = os.path.join(training_args.output_dir, "train_results.txt")
    with open(output_train_file, "w") as writer:
        logger.info("***** Train results *****")
        for key, value in sorted(train_result.metrics.items()):
            logger.info(f"  {key} = {value}")
            writer.write(f"{key} = {value}\n")

    # State ì €ì¥
    trainer.state.save_to_json(
        os.path.join(training_args.output_dir, "trainer_state.json")
    )

    # Evaluation
    logger.info(
        "Running final evaluation on validation set (%d examples)",
        len(eval_dataset),
    )
    logger.info(f"Best metric: {trainer.state.best_metric}")
    logger.info(f"Best model checkpoint: {trainer.state.best_model_checkpoint}")

    metrics = trainer.evaluate()
    metrics["eval_samples"] = len(eval_dataset)
    trainer.log_metrics("eval", metrics)
    trainer.save_metrics("eval", metrics)

    # í•™ìŠµì— ì‚¬ìš©ëœ yaml config íŒŒì¼ì„ output_dirì— ë³µì‚¬
    if len(sys.argv) == 2 and sys.argv[1].endswith(".yaml"):
        os.makedirs(training_args.output_dir, exist_ok=True)
        shutil.copy2(sys.argv[1],
                     os.path.join(training_args.output_dir, "config_used.yaml"))


if __name__ == "__main__":
    main()
