import logging
import shutil
import os
import sys
import random
import numpy as np
import torch
import evaluate
from typing import NoReturn

from src.arguments import DataTrainingArguments, ModelArguments
from datasets import DatasetDict, load_from_disk
from src.trainer_qa import QuestionAnsweringTrainer
from transformers import (
    AutoConfig,
    AutoModelForQuestionAnswering,
    AutoTokenizer,
    DataCollatorWithPadding,
    EvalPrediction,
    HfArgumentParser,
    TrainingArguments,
    set_seed,
)

from src.utils import (
    check_no_error,
    postprocess_qa_predictions,
    wait_for_gpu_availability,
    get_config,
    to_serializable,
    print_section,
    get_logger,
)
from src.utils.metrics_tracker import MetricsTracker
from src.utils.evaluator import (
    FinalEvaluator,
    save_predictions,
    save_detailed_results,
)
from src.utils.analysis import save_prediction_analysis

seed = 2024
deterministic = False

random.seed(seed)  # python random seed ê³ ì •
np.random.seed(seed)  # numpy random seed ê³ ì •
torch.manual_seed(seed)  # torch random seed ê³ ì •
torch.cuda.manual_seed_all(seed)
if deterministic:  # cudnn random seed ê³ ì • - ê³ ì • ì‹œ í•™ìŠµ ì†ë„ê°€ ëŠë ¤ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

logger = get_logger(__name__)


def main():
    # ê°€ëŠ¥í•œ arguments ë“¤ì€ ./arguments.py ë‚˜ transformer package ì•ˆì˜ src/transformers/training_args.py ì—ì„œ í™•ì¸ ê°€ëŠ¥í•©ë‹ˆë‹¤.
    # --help flag ë¥¼ ì‹¤í–‰ì‹œì¼œì„œ í™•ì¸í•  ìˆ˜ ë„ ìˆìŠµë‹ˆë‹¤.

    # gpu ì‚¬ìš© ê°€ëŠ¥í•œì§€ ì²´í¬
    wait_for_gpu_availability()

    parser = HfArgumentParser(
        (ModelArguments, DataTrainingArguments, TrainingArguments)
    )

    model_args, data_args, training_args = get_config(parser)

    #
    training_args.do_train = True
    training_args.do_eval = True

    # train.pyëŠ” "í•™ìŠµ ì „ìš©" ìŠ¤í¬ë¦½íŠ¸ë¡œ ì‚¬ìš©
    if not training_args.do_train:
        raise ValueError(
            "train.pyëŠ” í•™ìŠµ ì „ìš© ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤. "
            "TrainingArguments.do_train=Trueë¡œ ì„¤ì •í•œ YAMLì„ ì‚¬ìš©í•˜ì„¸ìš”."
        )

    logger.info("model is from: %s", model_args.model_name_or_path)
    logger.info("data is from: %s", data_args.train_dataset_name)
    logger.info("output_dir is: %s", training_args.output_dir)

    # í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ argumentsë¥¼ í•œ ë²ˆì— ë¡œê·¸ë¡œ ë‚¨ê²¨ë‘ê¸°
    print_section("Model Arguments", model_args)
    print_section("Data Training Arguments", data_args)
    print("Trainging Arguments:")
    print(f"output_dir: {training_args.output_dir})")
    print(f"num_train_epochs: {training_args.num_train_epochs}")
    print(f"per_device_train_batch_size: {training_args.per_device_train_batch_size}")
    print(f"per_device_eval_batch_size: {training_args.per_device_eval_batch_size}")
    print(f"learning_rate: {training_args.learning_rate}")
    print(f"warmup_ratio: {training_args.warmup_ratio}")
    print(f"weight_decay: {training_args.weight_decay}")
    print(f"logging_steps: {training_args.logging_steps}")
    print(f"logging_first_step: {training_args.logging_first_step}")
    # attr í™˜ê²½ ì´ìŠˆë¡œ ì•„ì˜ˆ ì£¼ì„ ì²˜ë¦¬í•¨
    # print(f"evaluation_strategy: {training_args.eval_strategy}")
    # print(f"eval_strategy: {training_args.eval_strategy}")
    print(f"save_strategy: {training_args.save_strategy}")
    print(f"save_total_limit: {training_args.save_total_limit}")
    print(f"load_best_model_at_end: {training_args.load_best_model_at_end}")
    print(f"metric_for_best_model: {training_args.metric_for_best_model}")
    print(f"greater_is_better: {training_args.greater_is_better}")
    print(f"fp16: {training_args.fp16}")
    print(f"dataloader_num_workers: {training_args.dataloader_num_workers}")
    print(f"gradient_accumulation_steps: {training_args.gradient_accumulation_steps}")
    print(f"report_to: {training_args.report_to}")

    # ëª¨ë¸ì„ ì´ˆê¸°í™”í•˜ê¸° ì „ì— ë‚œìˆ˜ë¥¼ ê³ ì •í•©ë‹ˆë‹¤.
    set_seed(training_args.seed)

    datasets = load_from_disk(data_args.train_dataset_name)
    logger.info("load datasets: \n", datasets)

    # AutoConfigë¥¼ ì´ìš©í•˜ì—¬ pretrained model ê³¼ tokenizerë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
    # argumentë¡œ ì›í•˜ëŠ” ëª¨ë¸ ì´ë¦„ì„ ì„¤ì •í•˜ë©´ ì˜µì…˜ì„ ë°”ê¿€ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    config = AutoConfig.from_pretrained(
        model_args.config_name
        if model_args.config_name is not None
        else model_args.model_name_or_path,
    )
    tokenizer = AutoTokenizer.from_pretrained(
        model_args.tokenizer_name
        if model_args.tokenizer_name is not None
        else model_args.model_name_or_path,
        # 'use_fast' argumentë¥¼ Trueë¡œ ì„¤ì •í•  ê²½ìš° rustë¡œ êµ¬í˜„ëœ tokenizerë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        # Falseë¡œ ì„¤ì •í•  ê²½ìš° pythonìœ¼ë¡œ êµ¬í˜„ëœ tokenizerë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìœ¼ë©°,
        # rust versionì´ ë¹„êµì  ì†ë„ê°€ ë¹ ë¦…ë‹ˆë‹¤.
        use_fast=True,
    )
    model = AutoModelForQuestionAnswering.from_pretrained(
        model_args.model_name_or_path,
        from_tf=bool(".ckpt" in model_args.model_name_or_path),
        config=config,
    )

    logger.info(
        f"training_args type: {type(training_args)}, "
        f"model_args type: {type(model_args)}, "
        f"datasets type: {type(datasets)}, "
        f"tokenizer type: {type(tokenizer)}, "
        f"model type: {type(model)}"
    )

    run_mrc(data_args, training_args, model_args, datasets, tokenizer, model)


def run_mrc(
    data_args: DataTrainingArguments,
    training_args: TrainingArguments,
    model_args: ModelArguments,
    datasets: DatasetDict,
    tokenizer,
    model,
) -> NoReturn:
    # datasetì„ ì „ì²˜ë¦¬í•©ë‹ˆë‹¤.
    if training_args.do_train:
        column_names = datasets["train"].column_names
    else:
        column_names = datasets["validation"].column_names

    question_column_name = "question" if "question" in column_names else column_names[0]
    context_column_name = "context" if "context" in column_names else column_names[1]
    answer_column_name = "answers" if "answers" in column_names else column_names[2]

    # Paddingì— ëŒ€í•œ ì˜µì…˜ì„ ì„¤ì •í•©ë‹ˆë‹¤.
    # (question|context) í˜¹ì€ (context|question)ë¡œ ì„¸íŒ… ê°€ëŠ¥í•©ë‹ˆë‹¤.
    pad_on_right = tokenizer.padding_side == "right"

    # ëª¨ë¸ íƒ€ì…ì— ë”°ë¼ token_type_ids ì§€ì› ì—¬ë¶€ ìë™ íŒë³„
    # í•µì‹¬: tokenizerê°€ ë§Œë“¤ ìˆ˜ ìˆëŠ”ê°€ê°€ ì•„ë‹ˆë¼, ëª¨ë¸ì´ ë°›ì„ ìˆ˜ ìˆëŠ”ê°€ê°€ ì¤‘ìš”
    model_type = getattr(model.config, "model_type", "").lower()
    tokenizer_says_it_can = "token_type_ids" in getattr(
        tokenizer, "model_input_names", []
    )
    type_vocab_size = getattr(model.config, "type_vocab_size", 0)

    # RoBERTa/XLM-Rì€ type_vocab_size=1 ì´ë¼ token_type_ids ë„£ìœ¼ë©´ ì¸ë±ìŠ¤ ì—ëŸ¬ ë°œìƒ
    use_return_token_type_ids = bool(tokenizer_says_it_can and type_vocab_size > 1)

    print(
        f"model_type={model_type} | tokenizer_has_token_type_ids={tokenizer_says_it_can} "
        f"| type_vocab_size={type_vocab_size} | use_return_token_type_ids={use_return_token_type_ids}"
    )

    # ì˜¤ë¥˜ê°€ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤. (checkpointëŠ” ë¬´ì‹œ, max_seq_lengthë§Œ ì‚¬ìš©)
    _, max_seq_length = check_no_error(data_args, training_args, datasets, tokenizer)

    # Train preprocessing / ì „ì²˜ë¦¬ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.

    def prepare_train_features(examples, _use_token_type_ids=use_return_token_type_ids):
        # truncationê³¼ padding(lengthê°€ ì§§ì„ë•Œë§Œ)ì„ í†µí•´ toknizationì„ ì§„í–‰í•˜ë©°, strideë¥¼ ì´ìš©í•˜ì—¬ overflowë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.
        # ê° exampleë“¤ì€ ì´ì „ì˜ contextì™€ ì¡°ê¸ˆì”© ê²¹ì¹˜ê²Œë©ë‹ˆë‹¤.
        tokenized_examples = tokenizer(
            examples[question_column_name if pad_on_right else context_column_name],
            examples[context_column_name if pad_on_right else question_column_name],
            truncation="only_second" if pad_on_right else "only_first",
            max_length=max_seq_length,
            stride=data_args.doc_stride,
            return_overflowing_tokens=True,
            return_offsets_mapping=True,
            return_token_type_ids=_use_token_type_ids,
            padding="max_length" if data_args.pad_to_max_length else False,
        )

        # ì•ˆì „ì¥ì¹˜: í˜¹ì‹œ token_type_idsê°€ ë‚¨ì•„ìˆìœ¼ë©´ ì œê±°
        if not _use_token_type_ids and "token_type_ids" in tokenized_examples:
            tokenized_examples.pop("token_type_ids")

        # ê¸¸ì´ê°€ ê¸´ contextê°€ ë“±ì¥í•  ê²½ìš° truncateë¥¼ ì§„í–‰í•´ì•¼í•˜ë¯€ë¡œ, í•´ë‹¹ ë°ì´í„°ì…‹ì„ ì°¾ì„ ìˆ˜ ìˆë„ë¡ mapping ê°€ëŠ¥í•œ ê°’ì´ í•„ìš”í•©ë‹ˆë‹¤.
        sample_mapping = tokenized_examples.pop("overflow_to_sample_mapping")
        # tokenì˜ ìºë¦­í„° ë‹¨ìœ„ positionë¥¼ ì°¾ì„ ìˆ˜ ìˆë„ë¡ offset mappingì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
        # start_positionsê³¼ end_positionsì„ ì°¾ëŠ”ë° ë„ì›€ì„ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        offset_mapping = tokenized_examples.pop("offset_mapping")

        # ë°ì´í„°ì…‹ì— "start position", "enc position" labelì„ ë¶€ì—¬í•©ë‹ˆë‹¤.
        tokenized_examples["start_positions"] = []
        tokenized_examples["end_positions"] = []

        for i, offsets in enumerate(offset_mapping):
            input_ids = tokenized_examples["input_ids"][i]
            cls_index = input_ids.index(tokenizer.cls_token_id)  # cls index

            # sequence idë¥¼ ì„¤ì •í•©ë‹ˆë‹¤ (to know what is the context and what is the question).
            sequence_ids = tokenized_examples.sequence_ids(i)

            # í•˜ë‚˜ì˜ exampleì´ ì—¬ëŸ¬ê°œì˜ spanì„ ê°€ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            sample_index = sample_mapping[i]
            answers = examples[answer_column_name][sample_index]

            # answerê°€ ì—†ì„ ê²½ìš° cls_indexë¥¼ answerë¡œ ì„¤ì •í•©ë‹ˆë‹¤(== exampleì—ì„œ ì •ë‹µì´ ì—†ëŠ” ê²½ìš° ì¡´ì¬í•  ìˆ˜ ìˆìŒ).
            if len(answers["answer_start"]) == 0:
                tokenized_examples["start_positions"].append(cls_index)
                tokenized_examples["end_positions"].append(cls_index)
            else:
                # textì—ì„œ ì •ë‹µì˜ Start/end character index
                start_char = answers["answer_start"][0]
                end_char = start_char + len(answers["text"][0])

                # textì—ì„œ current spanì˜ Start token index
                token_start_index = 0
                while sequence_ids[token_start_index] != (1 if pad_on_right else 0):
                    token_start_index += 1

                # textì—ì„œ current spanì˜ End token index
                token_end_index = len(input_ids) - 1
                while sequence_ids[token_end_index] != (1 if pad_on_right else 0):
                    token_end_index -= 1

                # ì •ë‹µì´ spanì„ ë²—ì–´ë‚¬ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤(ì •ë‹µì´ ì—†ëŠ” ê²½ìš° CLS indexë¡œ labelë˜ì–´ìˆìŒ).
                if not (
                    offsets[token_start_index][0] <= start_char
                    and offsets[token_end_index][1] >= end_char
                ):
                    tokenized_examples["start_positions"].append(cls_index)
                    tokenized_examples["end_positions"].append(cls_index)
                else:
                    # token_start_index ë° token_end_indexë¥¼ answerì˜ ëìœ¼ë¡œ ì´ë™í•©ë‹ˆë‹¤.
                    # Note: answerê°€ ë§ˆì§€ë§‰ ë‹¨ì–´ì¸ ê²½ìš° last offsetì„ ë”°ë¼ê°ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤(edge case).
                    while (
                        token_start_index < len(offsets)
                        and offsets[token_start_index][0] <= start_char
                    ):
                        token_start_index += 1
                    tokenized_examples["start_positions"].append(token_start_index - 1)
                    while offsets[token_end_index][1] >= end_char:
                        token_end_index -= 1
                    tokenized_examples["end_positions"].append(token_end_index + 1)

        return tokenized_examples

    if training_args.do_train:
        if "train" not in datasets:
            raise ValueError("--do_train requires a train dataset")
        train_dataset = datasets["train"]
                # ğŸ”¥ Negative Sampling ì ìš© (optional)
        if getattr(data_args, "use_negative_sampling", False):
            from negative_sampling import NegativeSampler
            from src.retrieval import SparseRetrieval  # BM25 retriever
    
            print("ğŸ”„ Negative sampling enabled â€” generating augmented train dataset")
    
            # 1) ê¸°ì¡´ training corpus ë¡œë”©
            retriever = SparseRetrieval(
                tokenize_fn=lambda x: tokenizer.tokenize(x),
                data_path="./data",                      # ë””ë ‰í† ë¦¬
                context_path="wikipedia_documents.json"  # ì‹¤ì œ íŒŒì¼ëª…
            )
            retriever.get_sparse_embedding()  # BM25 ì¤€ë¹„
    
            # 2) NegativeSampler ìƒì„±
            neg_sampler = NegativeSampler(
                retriever=retriever,
                num_negative_samples=data_args.num_negative_samples,
            )
    
            # 3) train augmentation
            train_dataset = neg_sampler.augment_train_dataset(train_dataset)
    
            print(f"âœ… Negative sampling applied: Train size â†’ {len(train_dataset)}")

        # datasetì—ì„œ train featureë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        train_dataset = train_dataset.map(
            prepare_train_features,
            batched=True,
            num_proc=data_args.preprocessing_num_workers,
            remove_columns=column_names,
            load_from_cache_file=not data_args.overwrite_cache,
        )

    # Validation preprocessing
    def prepare_validation_features(
        examples, _use_token_type_ids=use_return_token_type_ids
    ):
           # ------------------ DEBUG START ------------------
        # ì‹¤ì œ ë“¤ì–´ì˜¤ëŠ” ì§ˆë¬¸/ë¬¸ë§¥ ê¸¸ì´ë¥¼ í•œ ë²ˆ ì°ì–´ë³´ê¸°
        try:
            print("\n[DEBUG] Validation batch size:", len(examples["id"]))
            print("[DEBUG] First question:", examples["question"][0][:100])
            print("[DEBUG] First context length:", len(examples["context"][0]))
        except Exception as e:
            print("[DEBUG] Failed to inspect original examples:", e)
        # ------------------ DEBUG END ------------------
        # truncationê³¼ padding(lengthê°€ ì§§ì„ë•Œë§Œ)ì„ í†µí•´ toknizationì„ ì§„í–‰í•˜ë©°, strideë¥¼ ì´ìš©í•˜ì—¬ overflowë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.
        # ê° exampleë“¤ì€ ì´ì „ì˜ contextì™€ ì¡°ê¸ˆì”© ê²¹ì¹˜ê²Œë©ë‹ˆë‹¤.
        tokenized_examples = tokenizer(
            examples[question_column_name if pad_on_right else context_column_name],
            examples[context_column_name if pad_on_right else question_column_name],
            truncation="only_second" if pad_on_right else "only_first",
            max_length=max_seq_length,
            stride=data_args.doc_stride,
            return_overflowing_tokens=True,
            return_offsets_mapping=True,
            return_token_type_ids=_use_token_type_ids,
            padding="max_length" if data_args.pad_to_max_length else False,
        )
        

        # ì•ˆì „ì¥ì¹˜: í˜¹ì‹œ token_type_idsê°€ ë‚¨ì•„ìˆìœ¼ë©´ ì œê±°
        if not _use_token_type_ids and "token_type_ids" in tokenized_examples:
            tokenized_examples.pop("token_type_ids")

        # ê¸¸ì´ê°€ ê¸´ contextê°€ ë“±ì¥í•  ê²½ìš° truncateë¥¼ ì§„í–‰í•´ì•¼í•˜ë¯€ë¡œ, í•´ë‹¹ ë°ì´í„°ì…‹ì„ ì°¾ì„ ìˆ˜ ìˆë„ë¡ mapping ê°€ëŠ¥í•œ ê°’ì´ í•„ìš”í•©ë‹ˆë‹¤.
        sample_mapping = tokenized_examples.pop("overflow_to_sample_mapping")

        # evaluationì„ ìœ„í•´, predictionì„ contextì˜ substringìœ¼ë¡œ ë³€í™˜í•´ì•¼í•©ë‹ˆë‹¤.
        # corresponding example_idë¥¼ ìœ ì§€í•˜ê³  offset mappingsì„ ì €ì¥í•´ì•¼í•©ë‹ˆë‹¤.
        tokenized_examples["example_id"] = []

        for i in range(len(tokenized_examples["input_ids"])):
            # sequence idë¥¼ ì„¤ì •í•©ë‹ˆë‹¤ (to know what is the context and what is the question).
            sequence_ids = tokenized_examples.sequence_ids(i)
            context_index = 1 if pad_on_right else 0

            # í•˜ë‚˜ì˜ exampleì´ ì—¬ëŸ¬ê°œì˜ spanì„ ê°€ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            sample_index = sample_mapping[i]
            tokenized_examples["example_id"].append(examples["id"][sample_index])

            # Set to None the offset_mappingì„ Noneìœ¼ë¡œ ì„¤ì •í•´ì„œ token positionì´ contextì˜ ì¼ë¶€ì¸ì§€ ì‰½ê²Œ íŒë³„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            tokenized_examples["offset_mapping"][i] = [
                (o if sequence_ids[k] == context_index else None)
                for k, o in enumerate(tokenized_examples["offset_mapping"][i])
            ]
        return tokenized_examples

    eval_dataset = datasets["validation"]

    # Validation Feature ìƒì„±
    eval_dataset = eval_dataset.map(
        prepare_validation_features,
        batched=True,
        num_proc=data_args.preprocessing_num_workers,
        remove_columns=column_names,
        load_from_cache_file=not data_args.overwrite_cache,
    )

    # Data collator
    # flagê°€ Trueì´ë©´ ì´ë¯¸ max lengthë¡œ paddingëœ ìƒíƒœì…ë‹ˆë‹¤.
    # ê·¸ë ‡ì§€ ì•Šë‹¤ë©´ data collatorì—ì„œ paddingì„ ì§„í–‰í•´ì•¼í•©ë‹ˆë‹¤.
    data_collator = DataCollatorWithPadding(
        tokenizer, pad_to_multiple_of=8 if training_args.fp16 else None
    )

    # Post-processing:
    def post_processing_function(examples, features, predictions, training_args):
        # Post-processing: start logitsê³¼ end logitsì„ original contextì˜ ì •ë‹µê³¼ matchì‹œí‚µë‹ˆë‹¤.
        predictions = postprocess_qa_predictions(
            examples=examples,
            features=features,
            predictions=predictions,
            max_answer_length=data_args.max_answer_length,
            output_dir=training_args.output_dir,
        )
        # Metricì„ êµ¬í•  ìˆ˜ ìˆë„ë¡ Formatì„ ë§ì¶°ì¤ë‹ˆë‹¤.
        formatted_predictions = [
            {"id": k, "prediction_text": v} for k, v in predictions.items()
        ]
        # í•­ìƒ EvalPrediction ë°˜í™˜
        references = [
            {"id": ex["id"], "answers": ex[answer_column_name]}
            for ex in datasets["validation"]
        ]
        return EvalPrediction(predictions=formatted_predictions, label_ids=references)

    metric = evaluate.load("squad")
    logger.info("---- metric loaded: %s ----", metric)

    def compute_metrics(p: EvalPrediction):
        return metric.compute(predictions=p.predictions, references=p.label_ids)

    # Metrics Tracker ì´ˆê¸°í™”
    metrics_tracker = MetricsTracker(output_dir=training_args.output_dir)

    # Trainer ì´ˆê¸°í™”
    trainer = QuestionAnsweringTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        eval_examples=datasets["validation"],
        tokenizer=tokenizer,
        data_collator=data_collator,
        post_process_function=post_processing_function,
        compute_metrics=compute_metrics,
        callbacks=[metrics_tracker],  # Metrics Tracker ì¶”ê°€
    )

    # Training (fresh run ë°©ì‹ìœ¼ë¡œ ìˆ˜ì •; í•„ìš”í•˜ë©´ YAMLì— resume_from_checkpoint ëª…ì‹œ)
    logger.info(
        "Starting training: model=%s, output_dir=%s",
        model_args.model_name_or_path,
        training_args.output_dir,
    )

    train_result = trainer.train(
        resume_from_checkpoint=getattr(training_args, "resume_from_checkpoint", None)
    )

    logger.info("Training completed.")
    logger.info("Saving model to %s", training_args.output_dir)
    logger.info(f"ìµœì¢… í›ˆë ¨ ê²°ê³¼: {train_result.metrics}")

    # ëª¨ë¸ ì €ì¥ (safetensorsëŠ” ìë™ìœ¼ë¡œ ì²˜ë¦¬ë¨)
    trainer.save_model()  # tokenizerê¹Œì§€ í•¨ê»˜ ì €ì¥
    # ğŸ’¾ ìš©ëŸ‰ ì ˆì•½: trainer.save_state() ì œê±° (optimizer.pt, scheduler.pt ì €ì¥ ì•ˆí•¨)
    # trainer.save_state()  # â† ì´ê±° í˜¸ì¶œí•˜ë©´ optimizer.pt (2.5GB) + scheduler.pt ë“±ì´ ì €ì¥ë¨

    # âœ… Best checkpoint ê²½ë¡œ ëª…ì‹œì ìœ¼ë¡œ ì €ì¥ (inferenceì—ì„œ ì‚¬ìš©)
    if trainer.state.best_model_checkpoint:
        best_checkpoint_path = os.path.join(
            training_args.output_dir, "best_checkpoint_path.txt"
        )
        with open(best_checkpoint_path, "w") as f:
            f.write(trainer.state.best_model_checkpoint)
        logger.info(f"âœ… Best checkpoint saved: {trainer.state.best_model_checkpoint}")
        logger.info(
            f"   Best metric ({training_args.metric_for_best_model}): {trainer.state.best_metric}"
        )
    else:
        logger.warning(
            "âš ï¸  No best checkpoint found (load_best_model_at_end might be False)"
        )

    metrics = train_result.metrics
    metrics["train_samples"] = len(train_dataset)

    trainer.log_metrics("train", metrics)
    trainer.save_metrics("train", metrics)

    output_train_file = os.path.join(training_args.output_dir, "train_results.txt")
    with open(output_train_file, "w") as writer:
        logger.info("***** Train results *****")
        for key, value in sorted(train_result.metrics.items()):
            logger.info(f"  {key} = {value}")
            writer.write(f"{key} = {value}\n")

    # State ì €ì¥
    trainer.state.save_to_json(
        os.path.join(training_args.output_dir, "trainer_state.json")
    )

    # Evaluation - try-exceptë¡œ ê°ì‹¸ì„œ í‰ê°€ ì‹¤íŒ¨í•´ë„ í•™ìŠµ ê²°ê³¼ëŠ” ë³´ì¡´
    try:
        logger.info(
            "Running final evaluation on validation set (%d examples)",
            len(eval_dataset),
        )
        logger.info(f"Best metric: {trainer.state.best_metric}")
        logger.info(f"Best model checkpoint: {trainer.state.best_model_checkpoint}")

        metrics = trainer.evaluate()
        metrics["eval_samples"] = len(eval_dataset)
        trainer.log_metrics("eval", metrics)
        trainer.save_metrics("eval", metrics)
    except Exception as e:
        logger.warning(f"âš ï¸  Final evaluation failed: {e}")
        logger.warning("   Best checkpoint is already saved in output directory")

    # í•™ìŠµ ìš”ì•½ ì¶œë ¥
    metrics_tracker.print_summary()

    # ìµœì¢… ì„±ëŠ¥ í‰ê°€ (train + validation)
    # ì£¼ì˜: ì´ ë¶€ë¶„ì´ ì‹¤íŒ¨í•´ë„ ìœ„ì—ì„œ ì´ë¯¸ best checkpointëŠ” ì €ì¥ë¨
    try:
        logger.info("=" * 80)
        logger.info("Running final performance evaluation on all splits...")
        logger.info("=" * 80)

        final_evaluator = FinalEvaluator(output_dir=training_args.output_dir)

        # 1. Train set í‰ê°€ (validation í˜•ì‹ìœ¼ë¡œ ë³€í™˜ í•„ìš”)
        logger.info("Evaluating on TRAIN set...")
        train_dataset_for_eval = datasets["train"].map(
            prepare_validation_features,
            batched=True,
            num_proc=data_args.preprocessing_num_workers,
            remove_columns=column_names,
            load_from_cache_file=False,  # ìºì‹œ ì¶©ëŒ ë°©ì§€: ì—¬ëŸ¬ ëª¨ë¸ ì—°ë‹¬ì•„ ì‹¤í–‰ ì‹œ í•„ìˆ˜
            desc="Preparing train features for evaluation",
        )
        train_predictions = trainer.predict(
            test_dataset=train_dataset_for_eval, test_examples=datasets["train"]
        )
        # predictionsëŠ” ë¦¬ìŠ¤íŠ¸ í˜•íƒœ [{"id": ..., "prediction_text": ...}, ...]
        train_pred_dict = {
            pred["id"]: pred["prediction_text"]
            for pred in train_predictions.predictions
        }
        train_ref_dict = {ex["id"]: ex[answer_column_name] for ex in datasets["train"]}

        # trainì—ì„œëŠ” retrieval ì‚¬ìš© ì•ˆí•¨
        final_evaluator.evaluate_split(
            predictions=train_pred_dict,
            references=train_ref_dict,
            split_name="train",
            with_retrieval=False,
        )
        save_predictions(train_pred_dict, training_args.output_dir, "train")
        # ì‚¬í›„ ë¶„ì„ì„ ìœ„í•œ confidence ì •ë³´ ì €ì¥ (detailed_resultsë³´ë‹¤ ë¨¼ì € ì‹¤í–‰)
        save_prediction_analysis(
            train_predictions,
            datasets["train"],
            training_args.output_dir,
            "train",
            answer_column_name,
        )
        save_detailed_results(
            train_pred_dict, datasets["train"], training_args.output_dir, "train"
        )

        # 2. Validation set í‰ê°€ (ì´ë¯¸ í‰ê°€ë¨, ê²°ê³¼ ì €ì¥ë§Œ)
        logger.info("Evaluating on VALIDATION set (gold context)...")
        val_predictions = trainer.predict(
            test_dataset=eval_dataset, test_examples=datasets["validation"]
        )
        # predictionsëŠ” ë¦¬ìŠ¤íŠ¸ í˜•íƒœ [{"id": ..., "prediction_text": ...}, ...]
        val_pred_dict = {
            pred["id"]: pred["prediction_text"] for pred in val_predictions.predictions
        }
        val_ref_dict = {
            ex["id"]: ex[answer_column_name] for ex in datasets["validation"]
        }

        final_evaluator.evaluate_split(
            predictions=val_pred_dict,
            references=val_ref_dict,
            split_name="validation",
            with_retrieval=False,
        )
        save_predictions(val_pred_dict, training_args.output_dir, "val")
        # ì‚¬í›„ ë¶„ì„ì„ ìœ„í•œ confidence ì •ë³´ ì €ì¥ (detailed_resultsë³´ë‹¤ ë¨¼ì € ì‹¤í–‰)
        save_prediction_analysis(
            val_predictions,
            datasets["validation"],
            training_args.output_dir,
            "val",
            answer_column_name,
        )
        save_detailed_results(
            val_pred_dict, datasets["validation"], training_args.output_dir, "val"
        )

        # eval_pred_gold.csv ì €ì¥ (gold context ì‚¬ìš©í•œ validation ì˜ˆì¸¡)
        import csv

        eval_pred_gold_path = os.path.join(
            training_args.output_dir, "eval_pred_gold.csv"
        )
        with open(eval_pred_gold_path, "w", encoding="utf-8") as f:
            writer = csv.writer(f, delimiter="\t")
            for key, value in val_pred_dict.items():
                writer.writerow([key, value])
        logger.info(
            f"âœ… Validation predictions (gold context) saved to {eval_pred_gold_path}"
        )

        # ì •ë‹µ ë ˆì´ë¸” ì €ì¥ (ìŠ¤ì½”ì–´ë§ìš©)
        import json

        eval_labels_path = os.path.join(training_args.output_dir, "eval_labels.json")
        with open(eval_labels_path, "w", encoding="utf-8") as f:
            json.dump(val_ref_dict, f, indent=2, ensure_ascii=False)
        logger.info(f"âœ… Validation labels saved to {eval_labels_path}")

        # 3. ìµœì¢… summary ì €ì¥ ë° ì¶œë ¥
        final_evaluator.save_summary()
        final_evaluator.print_summary()
        logger.info("âœ… Final performance evaluation completed successfully")

    except Exception as e:
        logger.warning(f"âš ï¸  Final evaluation failed (but model is already saved): {e}")
        logger.warning(
            "   Best checkpoint and metrics are preserved in output directory"
        )

    # Best checkpoint ê²½ë¡œë¥¼ íŒŒì¼ë¡œ ì €ì¥ (inferenceì—ì„œ ìë™ ë¡œë“œìš©) - ì´ë¯¸ ìœ„ì—ì„œ ì €ì¥í–ˆìœ¼ë¯€ë¡œ ì œê±° ê°€ëŠ¥
    # (ì¤‘ë³µ ë°©ì§€: ì´ë¯¸ trainer.save_state() ì§í›„ì— ì €ì¥ë¨)

    # í•™ìŠµì— ì‚¬ìš©ëœ yaml config íŒŒì¼ì„ output_dirì— ë³µì‚¬
    if len(sys.argv) == 2 and sys.argv[1].endswith(".yaml"):
        config_path = sys.argv[1]
        if os.path.exists(config_path):
            os.makedirs(training_args.output_dir, exist_ok=True)
            shutil.copy2(
                config_path, os.path.join(training_args.output_dir, "config_used.yaml")
            )
        else:
            logger.warning(f"âš ï¸  Config file not found: {config_path}")


if __name__ == "__main__":
    main()

