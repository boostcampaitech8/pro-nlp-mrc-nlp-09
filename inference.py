"""
Open-Domain Question Answering ì„ ìˆ˜í–‰í•˜ëŠ” inference ì½”ë“œ ì…ë‹ˆë‹¤.

ëŒ€ë¶€ë¶„ì˜ ë¡œì§ì€ train.py ì™€ ë¹„ìŠ·í•˜ë‚˜ retrieval, predict ë¶€ë¶„ì´ ì¶”ê°€ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
"""

import logging
from typing import Callable, Dict, List, NoReturn, Tuple

import evaluate
import numpy as np
from src.arguments import DataTrainingArguments, ModelArguments
from datasets import (
    Dataset,
    DatasetDict,
    Features,
    Sequence,
    Value,
)
from src.retrieval import SparseRetrieval
from src.trainer_qa import QuestionAnsweringTrainer
from transformers import (
    AutoConfig,
    AutoModelForQuestionAnswering,
    AutoTokenizer,
    DataCollatorWithPadding,
    EvalPrediction,
    HfArgumentParser,
    TrainingArguments,
    set_seed,
)

from src.utils import (
    check_no_error,
    postprocess_qa_predictions,
    wait_for_gpu_availability,
    get_config,
    get_logger,
    get_model_path,
    load_inference_dataset,
)

logger = get_logger(__name__, logging.INFO)


def main():
    # ê°€ëŠ¥í•œ arguments ë“¤ì€ ./arguments.py ë‚˜ transformer package ì•ˆì˜ src/transformers/training_args.py ì—ì„œ í™•ì¸ ê°€ëŠ¥í•©ë‹ˆë‹¤.
    # --help flag ë¥¼ ì‹¤í–‰ì‹œì¼œì„œ í™•ì¸í•  ìˆ˜ ë„ ìˆìŠµë‹ˆë‹¤.

    # gpu ì‚¬ìš© ê°€ëŠ¥í•œì§€ ì²´í¬
    wait_for_gpu_availability()

    parser = HfArgumentParser(
        (ModelArguments, DataTrainingArguments, TrainingArguments)
    )
    model_args, data_args, training_args = get_config(parser)

    # inference_splitì— ë”°ë¼ do_eval/do_predict ìë™ ì„¤ì •
    inference_split = data_args.inference_split
    if inference_split == "test":
        # test: ì •ë‹µì´ ì—†ìœ¼ë¯€ë¡œ predictë§Œ
        # testëŠ” gold contextê°€ ì—†ìœ¼ë¯€ë¡œ retrieval í•„ìˆ˜
        if not data_args.eval_retrieval:
            raise ValueError(
                "âŒ test splitì—ëŠ” gold contextê°€ ì—†ìœ¼ë¯€ë¡œ eval_retrieval=Trueê°€ í•„ìˆ˜ì…ë‹ˆë‹¤.\n"
                "ğŸ’¡ configì—ì„œ eval_retrieval: true ì„¤ì • í›„ ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”."
            )
        training_args.do_eval = False
        training_args.do_predict = True
        logger.info("ğŸ¯ Inference mode: TEST (do_predict only, retrieval required)")
    else:
        # train/validation: ì •ë‹µì´ ìˆìœ¼ë¯€ë¡œ eval + predict ëª¨ë‘ ìˆ˜í–‰
        training_args.do_eval = True
        training_args.do_predict = True
        logger.info(
            f"ğŸ¯ Inference mode: {inference_split.upper()} (do_eval + do_predict)"
        )

    # ëª¨ë¸ ê²½ë¡œ ìë™ ê²°ì • (use_trained_model=Trueì´ë©´ best checkpoint ìë™ íƒìƒ‰)
    model_path = get_model_path(model_args, training_args, for_inference=True)
    logger.info(f"ğŸ“¦ Model path: {model_path}")

    # ëª¨ë¸ì„ ì´ˆê¸°í™”í•˜ê¸° ì „ì— ë‚œìˆ˜ë¥¼ ê³ ì •í•©ë‹ˆë‹¤.
    set_seed(training_args.seed)

    # inference_splitì— ë§ëŠ” ë°ì´í„°ì…‹ ë¡œë“œ
    datasets = load_inference_dataset(data_args, inference_split)
    logger.info(f"ğŸ“Š Dataset loaded: {datasets}")

    # AutoConfigë¥¼ ì´ìš©í•˜ì—¬ pretrained model ê³¼ tokenizerë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
    config = AutoConfig.from_pretrained(
        model_args.config_name if model_args.config_name else model_path,
    )
    tokenizer = AutoTokenizer.from_pretrained(
        model_args.tokenizer_name if model_args.tokenizer_name else model_path,
        use_fast=True,
    )
    model = AutoModelForQuestionAnswering.from_pretrained(
        model_path,
        from_tf=bool(".ckpt" in model_path),
        config=config,
    )

    # Trueì¼ ê²½ìš° : run passage retrieval
    if data_args.eval_retrieval:
        datasets = run_sparse_retrieval(
            tokenizer.tokenize,
            datasets,
            training_args,
            data_args,
        )

    # eval or predict mrc model
    if training_args.do_eval or training_args.do_predict:
        run_mrc(data_args, training_args, model_args, datasets, tokenizer, model, inference_split)


def run_sparse_retrieval(
    tokenize_fn: Callable[[str], List[str]],
    datasets: DatasetDict,
    training_args: TrainingArguments,
    data_args: DataTrainingArguments,
    data_path: str = "./data",
    context_path: str = "wikipedia_documents.json",
) -> DatasetDict:
    # Queryì— ë§ëŠ” Passageë“¤ì„ Retrieval í•©ë‹ˆë‹¤.
    retriever = SparseRetrieval(
        tokenize_fn=tokenize_fn, data_path=data_path, context_path=context_path
    )
    retriever.get_sparse_embedding()

    if data_args.use_faiss:
        retriever.build_faiss(num_clusters=data_args.num_clusters)
        df = retriever.retrieve_faiss(
            datasets["validation"], topk=data_args.top_k_retrieval
        )
    else:
        df = retriever.retrieve(datasets["validation"], topk=data_args.top_k_retrieval)

    # TODO: do_predict / do_eval ë‘˜ë‹¤ ì‚¬ìš©í•˜ëŠ” ê²½ìš° ê³ ë ¤í•  ê²ƒ
    # test data ì— ëŒ€í•´ì„  ì •ë‹µì´ ì—†ìœ¼ë¯€ë¡œ id question context ë¡œë§Œ ë°ì´í„°ì…‹ì´ êµ¬ì„±ë©ë‹ˆë‹¤.
    if training_args.do_predict:
        f = Features(
            {
                "context": Value(dtype="string", id=None),
                "id": Value(dtype="string", id=None),
                "question": Value(dtype="string", id=None),
            }
        )

    # train data ì— ëŒ€í•´ì„  ì •ë‹µì´ ì¡´ì¬í•˜ë¯€ë¡œ id question context answer ë¡œ ë°ì´í„°ì…‹ì´ êµ¬ì„±ë©ë‹ˆë‹¤.
    elif training_args.do_eval:
        f = Features(
            {
                "answers": Sequence(
                    feature={
                        "text": Value(dtype="string", id=None),
                        "answer_start": Value(dtype="int32", id=None),
                    },
                    length=-1,
                    id=None,
                ),
                "context": Value(dtype="string", id=None),
                "id": Value(dtype="string", id=None),
                "question": Value(dtype="string", id=None),
            }
        )
    datasets = DatasetDict({"validation": Dataset.from_pandas(df, features=f)})
    return datasets


def run_mrc(
    data_args: DataTrainingArguments,
    training_args: TrainingArguments,
    model_args: ModelArguments,
    datasets: DatasetDict,
    tokenizer,
    model,
    inference_split: str,
) -> NoReturn:
    # eval í˜¹ì€ predictionì—ì„œë§Œ ì‚¬ìš©í•¨
    column_names = datasets["validation"].column_names

    question_column_name = "question" if "question" in column_names else column_names[0]
    context_column_name = "context" if "context" in column_names else column_names[1]
    answer_column_name = "answers" if "answers" in column_names else column_names[2]

    # Paddingì— ëŒ€í•œ ì˜µì…˜ì„ ì„¤ì •í•©ë‹ˆë‹¤.
    # (question|context) í˜¹ì€ (context|question)ë¡œ ì„¸íŒ… ê°€ëŠ¥í•©ë‹ˆë‹¤.
    pad_on_right = tokenizer.padding_side == "right"

    # ëª¨ë¸ íƒ€ì…ì— ë”°ë¼ token_type_ids ì§€ì› ì—¬ë¶€ ìë™ íŒë³„
    # í•µì‹¬: tokenizerê°€ ë§Œë“¤ ìˆ˜ ìˆëŠ”ê°€ê°€ ì•„ë‹ˆë¼, ëª¨ë¸ì´ ë°›ì„ ìˆ˜ ìˆëŠ”ê°€ê°€ ì¤‘ìš”
    model_type = getattr(model.config, "model_type", "").lower()
    tokenizer_says_it_can = "token_type_ids" in getattr(
        tokenizer, "model_input_names", []
    )
    type_vocab_size = getattr(model.config, "type_vocab_size", 0)

    # RoBERTa/XLM-Rì€ type_vocab_size=1 ì´ë¼ token_type_ids ë„£ìœ¼ë©´ ì¸ë±ìŠ¤ ì—ëŸ¬ ë°œìƒ
    use_token_type_ids = bool(tokenizer_says_it_can and type_vocab_size > 1)

    print(
        f"model_type={model_type} | tokenizer_has_token_type_ids={tokenizer_says_it_can} "
        f"| type_vocab_size={type_vocab_size} | use_token_type_ids={use_token_type_ids}"
    )

    # ì˜¤ë¥˜ê°€ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
    _, max_seq_length = check_no_error(data_args, training_args, datasets, tokenizer)

    # Validation preprocessing / ì „ì²˜ë¦¬ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.
    def prepare_validation_features(examples, _use_token_type_ids=use_token_type_ids):
        # truncationê³¼ padding(lengthê°€ ì§§ì„ë•Œë§Œ)ì„ í†µí•´ toknizationì„ ì§„í–‰í•˜ë©°, strideë¥¼ ì´ìš©í•˜ì—¬ overflowë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.
        # ê° exampleë“¤ì€ ì´ì „ì˜ contextì™€ ì¡°ê¸ˆì”© ê²¹ì¹˜ê²Œë©ë‹ˆë‹¤.
        tokenized_examples = tokenizer(
            examples[question_column_name if pad_on_right else context_column_name],
            examples[context_column_name if pad_on_right else question_column_name],
            truncation="only_second" if pad_on_right else "only_first",
            max_length=max_seq_length,
            stride=data_args.doc_stride,
            return_overflowing_tokens=True,
            return_offsets_mapping=True,
            return_token_type_ids=_use_token_type_ids,
            padding="max_length" if data_args.pad_to_max_length else False,
        )

        # ì•ˆì „ì¥ì¹˜: í˜¹ì‹œ token_type_idsê°€ ë‚¨ì•„ìˆìœ¼ë©´ ì œê±°
        if not _use_token_type_ids and "token_type_ids" in tokenized_examples:
            tokenized_examples.pop("token_type_ids")

        # ê¸¸ì´ê°€ ê¸´ contextê°€ ë“±ì¥í•  ê²½ìš° truncateë¥¼ ì§„í–‰í•´ì•¼í•˜ë¯€ë¡œ, í•´ë‹¹ ë°ì´í„°ì…‹ì„ ì°¾ì„ ìˆ˜ ìˆë„ë¡ mapping ê°€ëŠ¥í•œ ê°’ì´ í•„ìš”í•©ë‹ˆë‹¤.
        sample_mapping = tokenized_examples.pop("overflow_to_sample_mapping")

        # evaluationì„ ìœ„í•´, predictionì„ contextì˜ substringìœ¼ë¡œ ë³€í™˜í•´ì•¼í•©ë‹ˆë‹¤.
        # corresponding example_idë¥¼ ìœ ì§€í•˜ê³  offset mappingsì„ ì €ì¥í•´ì•¼í•©ë‹ˆë‹¤.
        tokenized_examples["example_id"] = []

        for i in range(len(tokenized_examples["input_ids"])):
            # sequence idë¥¼ ì„¤ì •í•©ë‹ˆë‹¤ (to know what is the context and what is the question).
            sequence_ids = tokenized_examples.sequence_ids(i)
            context_index = 1 if pad_on_right else 0

            # í•˜ë‚˜ì˜ exampleì´ ì—¬ëŸ¬ê°œì˜ spanì„ ê°€ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            sample_index = sample_mapping[i]
            tokenized_examples["example_id"].append(examples["id"][sample_index])

            # contextì˜ ì¼ë¶€ê°€ ì•„ë‹Œ offset_mappingì„ Noneìœ¼ë¡œ ì„¤ì •í•˜ì—¬ í† í° ìœ„ì¹˜ê°€ ì»¨í…ìŠ¤íŠ¸ì˜ ì¼ë¶€ì¸ì§€ ì—¬ë¶€ë¥¼ ì‰½ê²Œ íŒë³„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            tokenized_examples["offset_mapping"][i] = [
                (o if sequence_ids[k] == context_index else None)
                for k, o in enumerate(tokenized_examples["offset_mapping"][i])
            ]
        return tokenized_examples

    eval_dataset = datasets["validation"]

    # Validation Feature ìƒì„±
    eval_dataset = eval_dataset.map(
        prepare_validation_features,
        batched=True,
        num_proc=data_args.preprocessing_num_workers,
        remove_columns=column_names,
        load_from_cache_file=not data_args.overwrite_cache,
    )

    # Data collator
    # flagê°€ Trueì´ë©´ ì´ë¯¸ max lengthë¡œ paddingëœ ìƒíƒœì…ë‹ˆë‹¤.
    # ê·¸ë ‡ì§€ ì•Šë‹¤ë©´ data collatorì—ì„œ paddingì„ ì§„í–‰í•´ì•¼í•©ë‹ˆë‹¤.
    data_collator = DataCollatorWithPadding(
        tokenizer, pad_to_multiple_of=8 if training_args.fp16 else None
    )

    # Post-processing:
    def post_processing_function(
        examples,
        features,
        predictions: Tuple[np.ndarray, np.ndarray],
        training_args: TrainingArguments,
    ) -> EvalPrediction:
        # Post-processing: start logitsê³¼ end logitsì„ original contextì˜ ì •ë‹µê³¼ matchì‹œí‚µë‹ˆë‹¤.
        predictions = postprocess_qa_predictions(
            examples=examples,
            features=features,
            predictions=predictions,
            max_answer_length=data_args.max_answer_length,
            output_dir=training_args.output_dir,
            prefix="test",  # inference.pyëŠ” test ì˜ˆì¸¡ì´ë¯€ë¡œ test_pred.csv ìƒì„±
        )
        # Metricì„ êµ¬í•  ìˆ˜ ìˆë„ë¡ Formatì„ ë§ì¶°ì¤ë‹ˆë‹¤.
        formatted_predictions = [
            {"id": k, "prediction_text": v} for k, v in predictions.items()
        ]

        if training_args.do_predict:
            return formatted_predictions
        elif training_args.do_eval:
            references = [
                {"id": ex["id"], "answers": ex[answer_column_name]}
                for ex in datasets["validation"]
            ]

            return EvalPrediction(
                predictions=formatted_predictions, label_ids=references
            )

    metric = evaluate.load("squad")

    def compute_metrics(p: EvalPrediction) -> Dict:
        return metric.compute(predictions=p.predictions, references=p.label_ids)

    print("init trainer...")
    # Trainer ì´ˆê¸°í™”
    trainer = QuestionAnsweringTrainer(
        model=model,
        args=training_args,
        train_dataset=None,
        eval_dataset=eval_dataset,
        eval_examples=datasets["validation"],
        tokenizer=tokenizer,
        data_collator=data_collator,
        post_process_function=post_processing_function,
        compute_metrics=compute_metrics,
    )

    logger.info("*** Evaluate ***")

    # eval dataset & eval example - predictions.json ìƒì„±ë¨
    if training_args.do_predict:
        predictions = trainer.predict(
            test_dataset=eval_dataset, test_examples=datasets["validation"]
        )

        # predictions.json ì€ postprocess_qa_predictions() í˜¸ì¶œì‹œ ì´ë¯¸ ì €ì¥ë©ë‹ˆë‹¤.
        logger.info("=" * 80)
        logger.info("âœ… INFERENCE COMPLETED - Results saved:")
        logger.info(
            f"   ğŸ“„ predictions_test.json: {training_args.output_dir}/predictions_test.json"
        )
        logger.info(
            f"   ğŸ“„ nbest_predictions_test.json: {training_args.output_dir}/nbest_predictions_test.json"
        )
        logger.info(f"   ğŸ“Š test_pred.csv: {training_args.output_dir}/test_pred.csv")
        logger.info(f"      ğŸ‘‰ Use this CSV file for test submission!")
        logger.info("=" * 80)

        # Validation setì—ì„œ gold vs retrieval ë¹„êµ (ì˜µì…˜)
        if (
            inference_split == "validation"
            and hasattr(data_args, "compare_retrieval")
            and data_args.compare_retrieval
        ):
            logger.info("")
            logger.info("=" * 80)
            logger.info("ğŸ” RETRIEVAL COMPARISON MODE")
            logger.info("=" * 80)

            import json
            import csv
            from src.retrieval.sparse import SparseRetrieval
            from src.utils.evaluator import (
                FinalEvaluator,
                save_predictions,
                save_detailed_results,
            )

            # 1. Gold context ì˜ˆì¸¡ (ì´ë¯¸ ì™„ë£Œ)
            logger.info("1ï¸âƒ£  Gold context predictions (already done)")
            gold_pred_dict = {
                pred["id"]: pred["prediction_text"] for pred in predictions.predictions
            }

            # ì •ë‹µ ë ˆì´ë¸” ì €ì¥
            answer_column_name = (
                "answers" if "answers" in column_names else column_names[2]
            )
            val_ref_dict = {
                ex["id"]: ex[answer_column_name] for ex in datasets["validation"]
            }

            eval_labels_path = os.path.join(
                training_args.output_dir, "eval_labels.json"
            )
            with open(eval_labels_path, "w", encoding="utf-8") as f:
                json.dump(val_ref_dict, f, indent=2, ensure_ascii=False)
            logger.info(f"   âœ… Labels saved: {eval_labels_path}")

            # Gold predictions CSV ì €ì¥
            eval_pred_gold_path = os.path.join(
                training_args.output_dir, "eval_pred_gold.csv"
            )
            with open(eval_pred_gold_path, "w", encoding="utf-8") as f:
                writer = csv.writer(f, delimiter="\t")
                for key, value in gold_pred_dict.items():
                    writer.writerow([key, value])
            logger.info(f"   âœ… Gold predictions: {eval_pred_gold_path}")

            # 2. Retrieval context ì˜ˆì¸¡
            logger.info("")
            logger.info("2ï¸âƒ£  Running retrieval for validation set...")

            # Retrieval ì´ˆê¸°í™”
            retriever = SparseRetrieval(
                tokenize_fn=tokenizer.tokenize,
                data_path=data_args.data_path
                if hasattr(data_args, "data_path")
                else "./data",
                context_path=data_args.context_path
                if hasattr(data_args, "context_path")
                else "wikipedia_documents.json",
            )
            retriever.get_sparse_embedding()

            # Retrieval ìˆ˜í–‰
            val_questions = datasets["validation"]["question"]
            if data_args.use_faiss:
                retrieved_contexts = retriever.retrieve_faiss(
                    val_questions, topk=data_args.top_k_retrieval
                )
            else:
                retrieved_contexts = retriever.retrieve(
                    val_questions, topk=data_args.top_k_retrieval
                )

            # Retrieved contextë¡œ ìƒˆë¡œìš´ dataset ìƒì„±
            val_with_retrieval = datasets["validation"].map(
                lambda example, idx: {"context": retrieved_contexts[idx]},
                with_indices=True,
                desc="Adding retrieved contexts",
            )

            # Feature ìƒì„±
            val_retrieval_dataset = val_with_retrieval.map(
                prepare_validation_features,
                batched=True,
                num_proc=data_args.preprocessing_num_workers,
                remove_columns=column_names,
                load_from_cache_file=False,
                desc="Preparing validation features with retrieval",
            )

            # Retrieval ì˜ˆì¸¡
            logger.info("   Running predictions with retrieved contexts...")
            val_retrieval_predictions = trainer.predict(
                test_dataset=val_retrieval_dataset, test_examples=val_with_retrieval
            )
            val_retrieval_pred_dict = {
                pred["id"]: pred["prediction_text"]
                for pred in val_retrieval_predictions.predictions
            }

            # Retrieval predictions CSV ì €ì¥
            eval_pred_retrieval_path = os.path.join(
                training_args.output_dir, "eval_pred_retrieval.csv"
            )
            with open(eval_pred_retrieval_path, "w", encoding="utf-8") as f:
                writer = csv.writer(f, delimiter="\t")
                for key, value in val_retrieval_pred_dict.items():
                    writer.writerow([key, value])
            logger.info(f"   âœ… Retrieval predictions: {eval_pred_retrieval_path}")

            # 3. ìë™ ë¹„êµ ì‹¤í–‰
            logger.info("")
            logger.info("3ï¸âƒ£  Comparing gold vs retrieval performance...")

            import subprocess

            comparison_script = "scripts/compare_retrieval.py"
            if os.path.exists(comparison_script):
                result = subprocess.run(
                    [sys.executable, comparison_script, training_args.output_dir],
                    capture_output=False,
                )
                if result.returncode == 0:
                    logger.info("   âœ… Comparison completed successfully!")
                else:
                    logger.warning(
                        f"   âš ï¸  Comparison failed with code {result.returncode}"
                    )
            else:
                logger.warning(
                    f"   âš ï¸  Comparison script not found: {comparison_script}"
                )
                logger.info(
                    f"   ğŸ’¡ Run manually: python {comparison_script} {training_args.output_dir}"
                )

            logger.info("=" * 80)
        else:
            print(
                "No metric can be presented because there is no correct answer given. Job done!"
            )

    if training_args.do_eval:
        metrics = trainer.evaluate()
        metrics["eval_samples"] = len(eval_dataset)

        trainer.log_metrics("test", metrics)
        trainer.save_metrics("test", metrics)


if __name__ == "__main__":
    main()
