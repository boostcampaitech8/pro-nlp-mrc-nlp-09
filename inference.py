"""
Open-Domain Question Answering ì„ ìˆ˜í–‰í•˜ëŠ” inference ì½”ë“œ ì…ë‹ˆë‹¤.

ëŒ€ë¶€ë¶„ì˜ ë¡œì§ì€ train.py ì™€ ë¹„ìŠ·í•˜ë‚˜ retrieval, predict ë¶€ë¶„ì´ ì¶”ê°€ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
"""

import os
import sys
import logging
from typing import Callable, Dict, List, NoReturn, Optional, Tuple

import evaluate
import numpy as np
from src.arguments import DataTrainingArguments, ModelArguments
from datasets import (
    Dataset,
    DatasetDict,
    Features,
    Sequence,
    Value,
)
from src.retrieval import get_retriever, BaseRetrieval
from src.trainer_qa import QuestionAnsweringTrainer
from transformers import (
    AutoConfig,
    AutoModelForQuestionAnswering,
    AutoTokenizer,
    DataCollatorWithPadding,
    EvalPrediction,
    HfArgumentParser,
    TrainingArguments,
    set_seed,
)

from src.utils import (
    check_no_error,
    postprocess_qa_predictions,
    wait_for_gpu_availability,
    get_config,
    get_logger,
    get_model_path,
    load_inference_dataset,
)
from src.utils.retrieval_utils import retrieve_and_build_dataset
from src.retrieval.paths import get_path

logger = get_logger(__name__, logging.INFO)


def load_retrieval_from_cache(
    cache_path: str,
    dataset: Dataset,
    data_args: DataTrainingArguments,
    alpha: float = 0.35,
) -> Dataset:
    """
    ìºì‹œëœ retrieval ê²°ê³¼ë¥¼ ë¡œë“œí•˜ì—¬ Datasetì„ êµ¬ì„±í•©ë‹ˆë‹¤.

    Args:
        cache_path: retrieval cache JSONL ê²½ë¡œ
        dataset: ì›ë³¸ dataset (question, answers ë“± í¬í•¨)
        data_args: DataTrainingArguments
        alpha: hybrid score ê³„ì‚°ìš© BM25 ê°€ì¤‘ì¹˜

    Returns:
        contextê°€ retrieval ê²°ê³¼ë¡œ ëŒ€ì²´ëœ Dataset
    """
    import json
    import numpy as np

    # ìºì‹œ ë¡œë“œ
    cache = {}
    with open(cache_path, "r", encoding="utf-8") as f:
        for line in f:
            item = json.loads(line.strip())
            cache[item["id"]] = item

    # Passages corpus ë¡œë“œ (ìºì‹œëœ passage_idë¡œ í…ìŠ¤íŠ¸ ì¡°íšŒ)
    passages_meta_path = get_path("kure_passages_meta")
    wiki_path = get_path("wiki_corpus")

    if os.path.exists(passages_meta_path):
        passage_texts = []
        with open(passages_meta_path, "r", encoding="utf-8") as f:
            for line in f:
                meta = json.loads(line.strip())
                passage_texts.append(meta["text"])
    else:
        # Fallback: wiki corpus ì‚¬ìš©
        with open(wiki_path, "r", encoding="utf-8") as f:
            wiki = json.load(f)
        # ì¤‘ë³µ ì œê±° í›„ ìˆœì„œ ìœ ì§€
        unique_texts = {}
        for doc_id, doc_info in wiki.items():
            text = doc_info["text"]
            if text not in unique_texts:
                unique_texts[text] = text
        passage_texts = list(unique_texts.keys())

    # ê²°ê³¼ êµ¬ì„±
    result_data = {
        "id": [],
        "question": [],
        "context": [],
        "answers": [] if "answers" in dataset.column_names else None,
    }

    top_k = data_args.top_k_retrieval

    for example in dataset:
        qid = example["id"]
        cache_entry = cache.get(qid)

        if cache_entry is None:
            logger.warning(f"âš ï¸  Cache miss for question {qid}, using empty context")
            context = ""
        else:
            # Hybrid score ê³„ì‚° ë° ì •ë ¬
            candidates = cache_entry["retrieved"]

            if candidates:
                bm25_scores = np.array([c["score_bm25"] for c in candidates])
                dense_scores = np.array([c["score_dense"] for c in candidates])

                # Per-query min-max ì •ê·œí™”
                eps = 1e-9
                bm25_n = (bm25_scores - bm25_scores.min()) / (
                    bm25_scores.max() - bm25_scores.min() + eps
                )
                dense_n = (dense_scores - dense_scores.min()) / (
                    dense_scores.max() - dense_scores.min() + eps
                )

                # Hybrid score
                hybrid_scores = alpha * bm25_n + (1 - alpha) * dense_n

                # ì •ë ¬ ë° top-k ì„ íƒ
                sorted_indices = np.argsort(hybrid_scores)[::-1][:top_k]

                # Context êµ¬ì„± (top-k passage concatenation)
                contexts = []
                for idx in sorted_indices:
                    passage_id = candidates[idx]["passage_id"]
                    if passage_id < len(passage_texts):
                        contexts.append(passage_texts[passage_id])

                context = " ".join(contexts)
            else:
                context = ""

        result_data["id"].append(qid)
        result_data["question"].append(example["question"])
        result_data["context"].append(context)
        if result_data["answers"] is not None:
            result_data["answers"].append(
                example.get("answers", {"text": [], "answer_start": []})
            )

    # Dataset ìƒì„±
    if result_data["answers"] is not None:
        features = Features(
            {
                "id": Value(dtype="string"),
                "question": Value(dtype="string"),
                "context": Value(dtype="string"),
                "answers": Sequence(
                    feature={
                        "text": Value(dtype="string"),
                        "answer_start": Value(dtype="int32"),
                    }
                ),
            }
        )
    else:
        features = Features(
            {
                "id": Value(dtype="string"),
                "question": Value(dtype="string"),
                "context": Value(dtype="string"),
            }
        )
        del result_data["answers"]

    return Dataset.from_dict(result_data, features=features)


# TODO: í˜„ì¬ ì œì¶œ íŒŒì¼ ìƒì„±ê³¼ ê´€ë ¨ëœ ë²„ê·¸ ì¡´ì¬í•¨ (ì˜¤ë¥˜)
def main():
    # ê°€ëŠ¥í•œ arguments ë“¤ì€ ./arguments.py ë‚˜ transformer package ì•ˆì˜ src/transformers/training_args.py ì—ì„œ í™•ì¸ ê°€ëŠ¥í•©ë‹ˆë‹¤.
    # --help flag ë¥¼ ì‹¤í–‰ì‹œì¼œì„œ í™•ì¸í•  ìˆ˜ ë„ ìˆìŠµë‹ˆë‹¤.

    # gpu ì‚¬ìš© ê°€ëŠ¥í•œì§€ ì²´í¬
    wait_for_gpu_availability()

    parser = HfArgumentParser(
        (ModelArguments, DataTrainingArguments, TrainingArguments)
    )
    model_args, data_args, training_args = get_config(parser)

    # inference_splitì— ë”°ë¼ do_eval/do_predict ìë™ ì„¤ì •
    # ì •ì±…: validation/trainì€ do_evalë§Œ, testëŠ” do_predictë§Œ
    inference_split = data_args.inference_split
    if inference_split == "test":
        # test: ì •ë‹µì´ ì—†ìœ¼ë¯€ë¡œ predictë§Œ (ë©”íŠ¸ë¦­ ê³„ì‚° ë¶ˆê°€)
        # testëŠ” gold contextê°€ ì—†ìœ¼ë¯€ë¡œ retrieval í•„ìˆ˜
        if not data_args.eval_retrieval:
            raise ValueError(
                "âŒ test splitì—ëŠ” gold contextê°€ ì—†ìœ¼ë¯€ë¡œ eval_retrieval=Trueê°€ í•„ìˆ˜ì…ë‹ˆë‹¤.\n"
                "ğŸ’¡ configì—ì„œ eval_retrieval: true ì„¤ì • í›„ ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”."
            )
        training_args.do_eval = False
        training_args.do_predict = True
        logger.info("ğŸ¯ Inference mode: TEST (do_predict only, no metrics)")
    else:
        # train/validation: ì •ë‹µì´ ìˆìœ¼ë¯€ë¡œ do_evalë§Œ ìˆ˜í–‰ (ë©”íŠ¸ë¦­ ê³„ì‚° + predictions ì €ì¥)
        training_args.do_eval = True
        training_args.do_predict = False
        logger.info(
            f"ğŸ¯ Inference mode: {inference_split.upper()} (do_eval only, with metrics)"
        )

    # ëª¨ë¸ ê²½ë¡œ ìë™ ê²°ì • (use_trained_model=Trueì´ë©´ best checkpoint ìë™ íƒìƒ‰)
    model_path = get_model_path(model_args, training_args, for_inference=True)
    logger.info(f"ğŸ“¦ Model path: {model_path}")

    # ëª¨ë¸ì„ ì´ˆê¸°í™”í•˜ê¸° ì „ì— ë‚œìˆ˜ë¥¼ ê³ ì •í•©ë‹ˆë‹¤.
    set_seed(training_args.seed)

    # inference_splitì— ë§ëŠ” ë°ì´í„°ì…‹ ë¡œë“œ
    datasets = load_inference_dataset(data_args, inference_split)
    logger.info(f"ğŸ“Š Dataset loaded: {datasets}")

    # --- TOKENIZER SETUP (Retrieval specific) ---
    from src.utils.tokenization import get_tokenizer
    # model_args.tokenizer_name might be None, so use tokenizer (from AutoTokenizer) as fallback
    retrieval_tokenize_fn = get_tokenizer(
        data_args.retrieval_tokenizer_name, 
        model_tokenizer=AutoTokenizer.from_pretrained(model_args.model_name_or_path, use_fast=True) # Re-instantiate or assume `tokenizer` variable is available later?
        # `tokenizer` is instantiated later in this script. Let's move this block AFTER tokenizer instantiation?
        # Or just use the one we are about to create.
    )

    # Validation splitì¼ ê²½ìš° eval_labels.json ìƒì„± (ì‹¤í—˜ìš©)
    if inference_split == "validation":
        import json

        labels_path = os.path.join(training_args.output_dir, "eval_labels.json")
        if not os.path.exists(labels_path):
            logger.info("ğŸ“ Creating eval_labels.json for validation experiments...")
            labels = {}
            for ex in datasets["validation"]:
                qid = ex["id"]
                answers = ex["answers"]["text"]  # list of answers
                labels[qid] = answers
            os.makedirs(training_args.output_dir, exist_ok=True)
            with open(labels_path, "w", encoding="utf-8") as f:
                json.dump(labels, f, ensure_ascii=False, indent=2)
            logger.info(f"âœ… eval_labels.json saved: {labels_path}")

    # AutoConfigë¥¼ ì´ìš©í•˜ì—¬ pretrained model ê³¼ tokenizerë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
    config = AutoConfig.from_pretrained(
        model_args.config_name if model_args.config_name else model_path,
    )
    tokenizer = AutoTokenizer.from_pretrained(
        model_args.tokenizer_name if model_args.tokenizer_name else model_path,
        use_fast=True,
    )
    
    # Refresh retrieval tokenizer with the correct model tokenizer if needed
    if data_args.retrieval_tokenizer_name == "auto":
        retrieval_tokenize_fn = tokenizer.tokenize

    model = AutoModelForQuestionAnswering.from_pretrained(
        model_path,
        from_tf=bool(".ckpt" in model_path),
        config=config,
    )
    
    # --- RERANKER SETUP ---
    reranker = None
    if data_args.reranker_name:
        from src.retrieval.reranker import CrossEncoderReranker
        logger.info(f"ğŸš€ Initializing Reranker: {data_args.reranker_name}")
        reranker = CrossEncoderReranker(model_name=data_args.reranker_name)

    # Config ê²½ë¡œ ì¶”ì¶œ (YAML ì‚¬ìš© ì‹œ)
    config_path = (
        sys.argv[1] if len(sys.argv) == 2 and sys.argv[1].endswith(".yaml") else None
    )

    # YAMLì—ì„œ retrieval alpha ê°€ì ¸ì˜¤ê¸° (ìºì‹œ ê¸°ë°˜ retrievalìš©)
    retrieval_alpha = 0.35  # ê¸°ë³¸ê°’
    if config_path:
        try:
            import yaml

            with open(config_path, "r", encoding="utf-8") as f:
                yaml_config = yaml.safe_load(f)
            retrieval_config = yaml_config.get("retrieval", {})
            retrieval_alpha = retrieval_config.get("alpha", 0.35)
        except Exception:
            pass

    # =========================================================================
    # Test/Non-test ë¶„ê¸°: ëª…í™•í•œ ì •ì±… ë¶„ë¦¬
    # ìºì‹œê°€ ìˆìœ¼ë©´ ìºì‹œ ì‚¬ìš©, ì—†ìœ¼ë©´ ì‹¤ì‹œê°„ retrievalë¡œ fallback
    # =========================================================================
    if inference_split == "test":
        # TEST ë¶„ê¸°: retrieval í•„ìˆ˜, compare ë¶ˆê°€
        logger.info("ğŸ“ TEST branch: retrieval required, no gold context")

        # ìºì‹œ í™•ì¸
        test_cache_path = get_path("test_cache")
        if os.path.exists(test_cache_path):
            logger.info(f"ğŸ“¦ Using cached retrieval from {test_cache_path}")
            new_test_dataset = load_retrieval_from_cache(
                cache_path=test_cache_path,
                dataset=datasets["validation"],
                data_args=data_args,
                alpha=retrieval_alpha,
            )
            retriever = None
        else:
            logger.info(
                f"âš ï¸  Cache not found, running live retrieval ({data_args.retrieval_type})"
            )
            retriever = get_retriever(
                retrieval_type=data_args.retrieval_type,
                tokenize_fn=retrieval_tokenize_fn,
                config_path=config_path,
                # Pass BM25 specific args from data_args if they are not in config (but config usually has them)
                # But get_retriever reads from config_path inside if provided.
                # data_args overrides?
                # Actually BM25Retrieval reads from config_path.
                # We should pass parameters directly to ensure CLI args work if used.
                impl=data_args.bm25_impl,
                delta=data_args.bm25_delta,
            )
            retriever.build()

            # Use shared utility for retrieval
            new_test_dataset = retrieve_and_build_dataset(
                retriever=retriever,
                dataset=datasets["validation"],
                data_args=data_args,
                split_name="test",
                is_train=False,
                tokenizer=tokenizer,
                reranker=reranker, # Pass reranker
            )

        datasets = DatasetDict({"validation": new_test_dataset})

        run_mrc(
            data_args=data_args,
            training_args=training_args,
            model_args=model_args,
            datasets=datasets,
            tokenizer=tokenizer,
            model=model,
            inference_split=inference_split,
            retriever=None,
            original_datasets=None,
        )

    else:
        # VALIDATION/TRAIN ë¶„ê¸°: retrieval ì„ íƒì , compare ê°€ëŠ¥
        logger.info(
            f"ğŸ“ {inference_split.upper()} branch: retrieval optional, gold context available"
        )
        original_datasets = datasets  # compareìš© ë°±ì—… (gold context ë³´ì¡´)
        retriever = None

        if data_args.eval_retrieval:
            # ìºì‹œ ê²½ë¡œ ê²°ì • (validation/train)
            cache_path = (
                get_path("val_cache")
                if inference_split == "validation"
                else get_path("train_cache")
            )

            if os.path.exists(cache_path) and not reranker: # Skip cache if reranker is used (need raw passages)
                logger.info(f"ğŸ“¦ Using cached retrieval from {cache_path}")
                new_validation_dataset = load_retrieval_from_cache(
                    cache_path=cache_path,
                    dataset=datasets["validation"],
                    data_args=data_args,
                    alpha=retrieval_alpha,
                )
                datasets = DatasetDict({"validation": new_validation_dataset})
            else:
                if reranker:
                    logger.info("âš ï¸  Reranker enabled: Skipping cache to perform dynamic reranking.")
                else:
                    logger.info(
                        f"âš ï¸  Cache not found at {cache_path}, running live retrieval ({data_args.retrieval_type})"
                    )
                
                retriever = get_retriever(
                    retrieval_type=data_args.retrieval_type,
                    tokenize_fn=retrieval_tokenize_fn,
                    config_path=config_path,
                    impl=data_args.bm25_impl,
                    delta=data_args.bm25_delta,
                )
                retriever.build()

                # Use shared utility for retrieval
                new_validation_dataset = retrieve_and_build_dataset(
                    retriever=retriever,
                    dataset=datasets["validation"],
                    data_args=data_args,
                    split_name="validation",
                    is_train=False,
                    tokenizer=tokenizer,
                    reranker=reranker, # Pass reranker
                )
                datasets = DatasetDict({"validation": new_validation_dataset})
        else:
            logger.info("ğŸ“„ eval_retrieval=False: using gold context")

        run_mrc(
            data_args=data_args,
            training_args=training_args,
            model_args=model_args,
            datasets=datasets,
            tokenizer=tokenizer,
            model=model,
            inference_split=inference_split,
            retriever=retriever,
            original_datasets=original_datasets,
        )


def run_mrc(
    data_args: DataTrainingArguments,
    training_args: TrainingArguments,
    model_args: ModelArguments,
    datasets: DatasetDict,
    tokenizer,
    model,
    inference_split: str,
    retriever: Optional[BaseRetrieval] = None,
    original_datasets: Optional[DatasetDict] = None,
) -> NoReturn:
    # eval í˜¹ì€ predictionì—ì„œë§Œ ì‚¬ìš©í•¨
    column_names = datasets["validation"].column_names

    question_column_name = "question" if "question" in column_names else column_names[0]
    context_column_name = "context" if "context" in column_names else column_names[1]
    answer_column_name = "answers" if "answers" in column_names else column_names[2]

    # Paddingì— ëŒ€í•œ ì˜µì…˜ì„ ì„¤ì •í•©ë‹ˆë‹¤.
    # (question|context) í˜¹ì€ (context|question)ë¡œ ì„¸íŒ… ê°€ëŠ¥í•©ë‹ˆë‹¤.
    pad_on_right = tokenizer.padding_side == "right"

    # ëª¨ë¸ íƒ€ì…ì— ë”°ë¼ token_type_ids ì§€ì› ì—¬ë¶€ ìë™ íŒë³„
    # í•µì‹¬: tokenizerê°€ ë§Œë“¤ ìˆ˜ ìˆëŠ”ê°€ê°€ ì•„ë‹ˆë¼, ëª¨ë¸ì´ ë°›ì„ ìˆ˜ ìˆëŠ”ê°€ê°€ ì¤‘ìš”
    model_type = getattr(model.config, "model_type", "").lower()
    tokenizer_says_it_can = "token_type_ids" in getattr(
        tokenizer, "model_input_names", []
    )
    type_vocab_size = getattr(model.config, "type_vocab_size", 0)

    # RoBERTa/XLM-Rì€ type_vocab_size=1 ì´ë¼ token_type_ids ë„£ìœ¼ë©´ ì¸ë±ìŠ¤ ì—ëŸ¬ ë°œìƒ
    use_token_type_ids = bool(tokenizer_says_it_can and type_vocab_size > 1)

    print(
        f"model_type={model_type} | tokenizer_has_token_type_ids={tokenizer_says_it_can} "
        f"| type_vocab_size={type_vocab_size} | use_token_type_ids={use_token_type_ids}"
    )

    # ì˜¤ë¥˜ê°€ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
    _, max_seq_length = check_no_error(data_args, training_args, datasets, tokenizer)

    # Validation preprocessing / ì „ì²˜ë¦¬ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.
    def prepare_validation_features(examples, _use_token_type_ids=use_token_type_ids):
        # truncationê³¼ padding(lengthê°€ ì§§ì„ë•Œë§Œ)ì„ í†µí•´ toknizationì„ ì§„í–‰í•˜ë©°, strideë¥¼ ì´ìš©í•˜ì—¬ overflowë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.
        # ê° exampleë“¤ì€ ì´ì „ì˜ contextì™€ ì¡°ê¸ˆì”© ê²¹ì¹˜ê²Œë©ë‹ˆë‹¤.
        tokenized_examples = tokenizer(
            examples[question_column_name if pad_on_right else context_column_name],
            examples[context_column_name if pad_on_right else question_column_name],
            truncation="only_second" if pad_on_right else "only_first",
            max_length=max_seq_length,
            stride=data_args.doc_stride,
            return_overflowing_tokens=True,
            return_offsets_mapping=True,
            return_token_type_ids=_use_token_type_ids,
            padding="max_length" if data_args.pad_to_max_length else False,
        )

        # ì•ˆì „ì¥ì¹˜: í˜¹ì‹œ token_type_idsê°€ ë‚¨ì•„ìˆìœ¼ë©´ ì œê±°
        if not _use_token_type_ids and "token_type_ids" in tokenized_examples:
            tokenized_examples.pop("token_type_ids")

        # ê¸¸ì´ê°€ ê¸´ contextê°€ ë“±ì¥í•  ê²½ìš° truncateë¥¼ ì§„í–‰í•´ì•¼í•˜ë¯€ë¡œ, í•´ë‹¹ ë°ì´í„°ì…‹ì„ ì°¾ì„ ìˆ˜ ìˆë„ë¡ mapping ê°€ëŠ¥í•œ ê°’ì´ í•„ìš”í•©ë‹ˆë‹¤.
        sample_mapping = tokenized_examples.pop("overflow_to_sample_mapping")

        # evaluationì„ ìœ„í•´, predictionì„ contextì˜ substringìœ¼ë¡œ ë³€í™˜í•´ì•¼í•©ë‹ˆë‹¤.
        # corresponding example_idë¥¼ ìœ ì§€í•˜ê³  offset mappingsì„ ì €ì¥í•´ì•¼í•©ë‹ˆë‹¤.
        tokenized_examples["example_id"] = []

        for i in range(len(tokenized_examples["input_ids"])):
            # sequence idë¥¼ ì„¤ì •í•©ë‹ˆë‹¤ (to know what is the context and what is the question).
            sequence_ids = tokenized_examples.sequence_ids(i)
            context_index = 1 if pad_on_right else 0

            # í•˜ë‚˜ì˜ exampleì´ ì—¬ëŸ¬ê°œì˜ spanì„ ê°€ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            sample_index = sample_mapping[i]
            tokenized_examples["example_id"].append(examples["id"][sample_index])

            # contextì˜ ì¼ë¶€ê°€ ì•„ë‹Œ offset_mappingì„ Noneìœ¼ë¡œ ì„¤ì •í•˜ì—¬ í† í° ìœ„ì¹˜ê°€ ì»¨í…ìŠ¤íŠ¸ì˜ ì¼ë¶€ì¸ì§€ ì—¬ë¶€ë¥¼ ì‰½ê²Œ íŒë³„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            tokenized_examples["offset_mapping"][i] = [
                (o if sequence_ids[k] == context_index else None)
                for k, o in enumerate(tokenized_examples["offset_mapping"][i])
            ]
        return tokenized_examples

    eval_dataset = datasets["validation"]

    # Validation Feature ìƒì„±
    eval_dataset = eval_dataset.map(
        prepare_validation_features,
        batched=True,
        num_proc=data_args.preprocessing_num_workers,
        remove_columns=column_names,
        load_from_cache_file=not data_args.overwrite_cache,
    )

    logger.info(f"ğŸ“Š Validation examples: {len(datasets['validation'])} questions")
    logger.info(
        f"ğŸ“Š Evaluation spans after tokenization: {len(eval_dataset)} spans (with doc_stride={data_args.doc_stride})"
    )
    logger.info(
        f"ğŸ“Š Average spans per question: {len(eval_dataset) / len(datasets['validation']):.1f}"
    )

    # Data collator
    # flagê°€ Trueì´ë©´ ì´ë¯¸ max lengthë¡œ paddingëœ ìƒíƒœì…ë‹ˆë‹¤.
    # ê·¸ë ‡ì§€ ì•Šë‹¤ë©´ data collatorì—ì„œ paddingì„ ì§„í–‰í•´ì•¼í•©ë‹ˆë‹¤.
    data_collator = DataCollatorWithPadding(
        tokenizer, pad_to_multiple_of=8 if training_args.fp16 else None
    )

    # Post-processing:
    def post_processing_function(
        examples,
        features,
        predictions: Tuple[np.ndarray, np.ndarray],
        training_args: TrainingArguments,
    ) -> EvalPrediction:
        # Post-processing: start logitsê³¼ end logitsì„ original contextì˜ ì •ë‹µê³¼ matchì‹œí‚µë‹ˆë‹¤.

        # inference_splitì— ë”°ë¼ prefix ë™ì  ì„¤ì •
        prefix_map = {"train": "train", "validation": "val", "test": "test"}
        prefix = prefix_map.get(inference_split, "test")

        predictions = postprocess_qa_predictions(
            examples=examples,
            features=features,
            predictions=predictions,
            max_answer_length=data_args.max_answer_length,
            output_dir=training_args.output_dir,
            prefix=prefix,  # train/val/testì— ë”°ë¼ ë™ì  ì„¤ì •
        )
        # Metricì„ êµ¬í•  ìˆ˜ ìˆë„ë¡ Formatì„ ë§ì¶°ì¤ë‹ˆë‹¤.
        formatted_predictions = [
            {"id": k, "prediction_text": v} for k, v in predictions.items()
        ]

        # do_evalì´ Trueë©´ í•­ìƒ references í¬í•¨ (metric ê³„ì‚° ìœ„í•´)
        if training_args.do_eval:
            references = [
                {"id": ex["id"], "answers": ex[answer_column_name]}
                for ex in datasets["validation"]
            ]
            return EvalPrediction(
                predictions=formatted_predictions, label_ids=references
            )
        elif training_args.do_predict:
            return formatted_predictions

    metric = evaluate.load("squad")

    def compute_metrics(p) -> Dict:
        # post_processing_functionì´ ë°˜í™˜í•˜ëŠ” íƒ€ì…ì— ë”°ë¼ ì²˜ë¦¬
        if isinstance(p, EvalPrediction):
            # do_eval ëª¨ë“œ: EvalPrediction ê°ì²´
            predictions = p.predictions
            references = p.label_ids
            return metric.compute(predictions=predictions, references=references)
        else:
            # do_predict ëª¨ë“œ: ì´ë¯¸ formatted list (metric ê³„ì‚° ë¶ˆí•„ìš”)
            return {}

    print("init trainer...")
    # Trainer ì´ˆê¸°í™”
    trainer = QuestionAnsweringTrainer(
        model=model,
        args=training_args,
        train_dataset=None,
        eval_dataset=eval_dataset,
        eval_examples=datasets["validation"],
        tokenizer=tokenizer,
        data_collator=data_collator,
        post_process_function=post_processing_function,
        compute_metrics=compute_metrics,
    )

    logger.info("*** Evaluate ***")

    # do_evalê³¼ do_predict ì‹¤í–‰ (ìƒí˜¸ ë°°íƒ€ì )
    # - do_eval: trainer.evaluate() â†’ ë©”íŠ¸ë¦­ ê³„ì‚° + predictions ì €ì¥ (validation/trainìš©)
    # - do_predict: trainer.predict() â†’ predictionsë§Œ ì €ì¥, ë©”íŠ¸ë¦­ ì—†ìŒ (testìš©)

    if training_args.do_eval:
        # Evaluation ì‹¤í–‰ (ë©”íŠ¸ë¦­ ê³„ì‚°ë¨)
        metrics = trainer.evaluate()
        metrics["eval_samples"] = len(eval_dataset)

        # ë™ì  prefix ì‚¬ìš© (train/val/testì— ë”°ë¼ ë‹¤ë¥¸ íŒŒì¼ëª…)
        prefix_map = {"train": "train", "validation": "val", "test": "test"}
        eval_prefix = prefix_map.get(inference_split, "test")

        trainer.log_metrics(eval_prefix, metrics)
        trainer.save_metrics(eval_prefix, metrics)

        logger.info(f"ğŸ“Š Evaluation metrics saved: {eval_prefix}_results.json")
        logger.info("=" * 80)
        logger.info("âœ… EVALUATION COMPLETED - Results saved:")
        logger.info(
            f"   ğŸ“„ predictions_{eval_prefix}.json: {training_args.output_dir}/predictions_{eval_prefix}.json"
        )
        logger.info(
            f"   ğŸ“„ nbest_predictions_{eval_prefix}.json: {training_args.output_dir}/nbest_predictions_{eval_prefix}.json"
        )
        logger.info(
            f"   ğŸ“Š {eval_prefix}_pred.csv: {training_args.output_dir}/{eval_prefix}_pred.csv"
        )

        # Validation/Trainì¼ ê²½ìš° ì •ë‹µ ë¹„êµ íŒŒì¼ ìƒì„±
        if inference_split in ["validation", "train"]:
            import json
            import pandas as pd

            # predictions ë¡œë“œ
            pred_path = os.path.join(
                training_args.output_dir, f"predictions_{eval_prefix}.json"
            )
            with open(pred_path, "r", encoding="utf-8") as f:
                predictions = json.load(f)

            # ì •ë‹µê³¼ ì˜ˆì¸¡ ë¹„êµ ë°ì´í„° ìƒì„±
            comparison_data = []
            for ex in datasets["validation"]:
                qid = ex["id"]
                question = ex["question"]
                gold_answers = ex["answers"]["text"]
                pred_answer = predictions.get(qid, "")

                # EM ì²´í¬
                is_correct = pred_answer in gold_answers

                comparison_data.append(
                    {
                        "id": qid,
                        "question": question,
                        "gold_answers": " | ".join(gold_answers),  # ì—¬ëŸ¬ ì •ë‹µì€ | êµ¬ë¶„
                        "prediction": pred_answer,
                        "correct": "âœ“" if is_correct else "âœ—",
                    }
                )

            # CSV ì €ì¥
            df = pd.DataFrame(comparison_data)
            comparison_csv = os.path.join(
                training_args.output_dir, f"{eval_prefix}_comparison.csv"
            )
            df.to_csv(comparison_csv, index=False, encoding="utf-8-sig")

            logger.info(
                f"   ğŸ“Š {eval_prefix}_comparison.csv: {comparison_csv} (with gold answers)"
            )

        logger.info("=" * 80)

    elif training_args.do_predict:
        # Predictionë§Œ ì‹¤í–‰ (ë©”íŠ¸ë¦­ ê³„ì‚° ì•ˆ ë¨, testìš©)
        predictions = trainer.predict(
            test_dataset=eval_dataset, test_examples=datasets["validation"]
        )

        # predictions.jsonì€ postprocess_qa_predictions()ì—ì„œ ì´ë¯¸ ì €ì¥ë¨
        # prefixì— ë”°ë¼ íŒŒì¼ëª… ë™ì  ìƒì„±
        prefix_map = {"train": "train", "validation": "val", "test": "test"}
        prefix = prefix_map.get(inference_split, "test")

        logger.info("=" * 80)
        logger.info("âœ… INFERENCE COMPLETED - Results saved:")
        logger.info(
            f"   ğŸ“„ predictions_{prefix}.json: {training_args.output_dir}/predictions_{prefix}.json"
        )
        logger.info(
            f"   ğŸ“„ nbest_predictions_{prefix}.json: {training_args.output_dir}/nbest_predictions_{prefix}.json"
        )
        logger.info(
            f"   ğŸ“Š {prefix}_pred.csv: {training_args.output_dir}/{prefix}_pred.csv"
        )
        if inference_split == "test":
            logger.info(f"      ğŸ‘‰ Use this CSV file for test submission!")
        logger.info("=" * 80)

        # Validation setì—ì„œ gold vs retrieval ë¹„êµ (ì˜µì…˜)
        if (
            inference_split == "validation"
            and hasattr(data_args, "compare_retrieval")
            and data_args.compare_retrieval
        ):
            compare_gold_vs_retrieval(
                original_datasets=original_datasets,
                retriever=retriever,
                trainer=trainer,
                tokenizer=tokenizer,
                data_args=data_args,
                training_args=training_args,
                prepare_validation_features=prepare_validation_features,
                column_names=column_names,
                predictions=predictions,
            )
        else:
            print(
                "No metric can be presented because there is no correct answer given. Job done!"
            )


def compare_gold_vs_retrieval(
    original_datasets: DatasetDict,
    retriever: Optional[BaseRetrieval],
    trainer: QuestionAnsweringTrainer,
    tokenizer,
    data_args: DataTrainingArguments,
    training_args: TrainingArguments,
    prepare_validation_features: Callable,
    column_names: List[str],
    predictions,
) -> NoReturn:
    """
    Validation setì—ì„œ gold context vs retrieval context ì„±ëŠ¥ ë¹„êµ.

    Args:
        original_datasets: Gold contextê°€ ìˆëŠ” ì›ë³¸ ë°ì´í„°ì…‹ (retrieval ì ìš© ì „)
        retriever: Retrieval ê°ì²´ (ìˆìœ¼ë©´ ì¬ì‚¬ìš©, ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±)
        trainer: QuestionAnsweringTrainer ì¸ìŠ¤í„´ìŠ¤
        tokenizer: Tokenizer
        data_args: DataTrainingArguments
        training_args: TrainingArguments
        prepare_validation_features: Feature ì „ì²˜ë¦¬ í•¨ìˆ˜
        column_names: ë°ì´í„°ì…‹ ì»¬ëŸ¼ëª… ë¦¬ìŠ¤íŠ¸
        predictions: Gold contextë¡œ ì´ë¯¸ ìˆ˜í–‰ëœ ì˜ˆì¸¡ ê²°ê³¼
    """
    import json
    import csv
    from src.retrieval.sparse import SparseRetrieval

    logger.info("")
    logger.info("=" * 80)
    logger.info("ğŸ” RETRIEVAL COMPARISON MODE")
    logger.info("=" * 80)

    # 1. Gold context ì˜ˆì¸¡ (ì´ë¯¸ ì™„ë£Œ)
    logger.info("1ï¸âƒ£  Gold context predictions (already done)")
    gold_pred_dict = {
        pred["id"]: pred["prediction_text"] for pred in predictions.predictions
    }

    # ì •ë‹µ ë ˆì´ë¸” ì €ì¥ (original_datasets ì‚¬ìš©)
    answer_column_name = "answers" if "answers" in column_names else column_names[2]
    val_ref_dict = {
        ex["id"]: ex[answer_column_name] for ex in original_datasets["validation"]
    }

    eval_labels_path = os.path.join(training_args.output_dir, "eval_labels.json")
    with open(eval_labels_path, "w", encoding="utf-8") as f:
        json.dump(val_ref_dict, f, indent=2, ensure_ascii=False)
    logger.info(f"   âœ… Labels saved: {eval_labels_path}")

    # Gold predictions CSV ì €ì¥
    eval_pred_gold_path = os.path.join(training_args.output_dir, "eval_pred_gold.csv")
    with open(eval_pred_gold_path, "w", encoding="utf-8") as f:
        writer = csv.writer(f, delimiter="\t")
        for key, value in gold_pred_dict.items():
            writer.writerow([key, value])
    logger.info(f"   âœ… Gold predictions: {eval_pred_gold_path}")

    # 2. Retrieval context ì˜ˆì¸¡
    logger.info("")
    logger.info("2ï¸âƒ£  Running retrieval for validation set...")

    # Retrieval ê°ì²´: ì „ë‹¬ë°›ì•˜ìœ¼ë©´ ì¬ì‚¬ìš©, ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
    if retriever is None:
        logger.info("   Creating new retriever for comparison...")
        config_path = (
            sys.argv[1]
            if len(sys.argv) == 2 and sys.argv[1].endswith(".yaml")
            else None
        )
        retriever = SparseRetrieval(
            tokenize_fn=tokenizer.tokenize,
            config_path=config_path,
            use_faiss=data_args.use_faiss,
            num_clusters=data_args.num_clusters,
        )
        retriever.build()
    else:
        logger.info("   Reusing existing retriever...")

    # Retrieval ìˆ˜í–‰ (original_datasets ì‚¬ìš© - gold context ë³´ì¡´ëœ ì›ë³¸)
    val_questions = original_datasets["validation"]["question"]
    df_retrieved = retriever.retrieve(
        original_datasets["validation"], topk=data_args.top_k_retrieval
    )

    # Retrieved contextë¡œ ìƒˆë¡œìš´ dataset ìƒì„±
    features = Features(
        {
            "id": Value(dtype="string", id=None),
            "question": Value(dtype="string", id=None),
            "context": Value(dtype="string", id=None),
            "answers": Sequence(
                feature={
                    "text": Value(dtype="string", id=None),
                    "answer_start": Value(dtype="int32", id=None),
                },
                length=-1,
                id=None,
            ),
        }
    )
    val_with_retrieval = Dataset.from_pandas(
        df_retrieved[["id", "question", "context", "answers"]].reset_index(drop=True),
        features=features,
    )

    # Feature ìƒì„±
    val_retrieval_dataset = val_with_retrieval.map(
        prepare_validation_features,
        batched=True,
        num_proc=data_args.preprocessing_num_workers,
        remove_columns=["id", "question", "context", "answers"],
        load_from_cache_file=False,
        desc="Preparing validation features with retrieval",
    )

    # Retrieval ì˜ˆì¸¡
    logger.info("   Running predictions with retrieved contexts...")
    val_retrieval_predictions = trainer.predict(
        test_dataset=val_retrieval_dataset, test_examples=val_with_retrieval
    )
    val_retrieval_pred_dict = {
        pred["id"]: pred["prediction_text"]
        for pred in val_retrieval_predictions.predictions
    }

    # Retrieval predictions CSV ì €ì¥
    eval_pred_retrieval_path = os.path.join(
        training_args.output_dir, "eval_pred_retrieval.csv"
    )
    with open(eval_pred_retrieval_path, "w", encoding="utf-8") as f:
        writer = csv.writer(f, delimiter="\t")
        for key, value in val_retrieval_pred_dict.items():
            writer.writerow([key, value])
    logger.info(f"   âœ… Retrieval predictions: {eval_pred_retrieval_path}")

    # 3. ìë™ ë¹„êµ ì‹¤í–‰
    logger.info("")
    logger.info("3ï¸âƒ£  Comparing gold vs retrieval performance...")

    import subprocess

    comparison_script = "scripts/compare_retrieval.py"
    if os.path.exists(comparison_script):
        result = subprocess.run(
            [sys.executable, comparison_script, training_args.output_dir],
            capture_output=False,
        )
        if result.returncode == 0:
            logger.info("   âœ… Comparison completed successfully!")
        else:
            logger.warning(f"   âš ï¸  Comparison failed with code {result.returncode}")
    else:
        logger.warning(f"   âš ï¸  Comparison script not found: {comparison_script}")
        logger.info(
            f"   ğŸ’¡ Run manually: python {comparison_script} {training_args.output_dir}"
        )

    logger.info("=" * 80)


if __name__ == "__main__":
    main()
