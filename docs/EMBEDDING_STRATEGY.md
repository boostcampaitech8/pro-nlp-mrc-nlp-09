# ì„ë² ë”© íŒŒì¼ í˜•ì‹ ë° Answer Offset ì „ëµ

> **ì‘ì„±ì¼**: 2024-12-10  
> **ëª©ì **: ODQA íŒŒì´í”„ë¼ì¸ì—ì„œ ì„ë² ë”© íŒŒì¼ ê´€ë¦¬ ë° Answer Offset ì²˜ë¦¬ ì „ëµ ë¬¸ì„œí™”

---

## 1. ì„ë² ë”© íŒŒì¼ êµ¬ì¡°

### 1.1 íŒŒì¼ ìœ„ì¹˜ (ì¤‘ì•™ ì§‘ì¤‘ ê´€ë¦¬)

ëª¨ë“  ì„ë² ë”©/ìºì‹œ ê²½ë¡œëŠ” `src/retrieval/paths.py`ì—ì„œ ê´€ë¦¬:

```
data/
â”œâ”€â”€ embeddings/                          # Dense ì„ë² ë”©
â”‚   â”œâ”€â”€ koe5_corpus_emb.npy             # KoE5 corpus embedding (222M)
â”‚   â”œâ”€â”€ kure_corpus_emb.npy             # KURE-v1 corpus embedding (~220M)
â”‚   â””â”€â”€ kure_passages_meta.jsonl        # Passage ë©”íƒ€ë°ì´í„° (chunking ì‹œ)
â”‚
â”œâ”€â”€ indices/                             # Sparse ì¸ë±ìŠ¤
â”‚   â””â”€â”€ sparse/
â”‚       â”œâ”€â”€ bm25_model_okt.bin          # BM25 ëª¨ë¸ (Okt í† í¬ë‚˜ì´ì €)
â”‚       â”œâ”€â”€ sparse_embedding.bin        # TF-IDF embedding
â”‚       â””â”€â”€ tfidv.bin                   # TF-IDF vectorizer
â”‚
â”œâ”€â”€ cache/                               # Retrieval ìºì‹œ
â”‚   â””â”€â”€ retrieval/
â”‚       â”œâ”€â”€ train_top50.jsonl           # Train set retrieval ê²°ê³¼
â”‚       â”œâ”€â”€ val_top50.jsonl             # Validation set retrieval ê²°ê³¼
â”‚       â””â”€â”€ test_top50.jsonl            # Test set retrieval ê²°ê³¼
â”‚
â””â”€â”€ wikipedia_documents.json             # ì›ë³¸ corpus (56,737 ë¬¸ì„œ)
```

### 1.2 íŒŒì¼ í˜•ì‹ ìƒì„¸

#### Dense Embedding (`.npy`)

```python
# ì €ì¥
corpus_emb = model.encode(texts, normalize_embeddings=True)
np.save("kure_corpus_emb.npy", corpus_emb)

# ë¡œë“œ
corpus_emb = np.load("kure_corpus_emb.npy")
# shape: (num_passages, embedding_dim)
# KoE5: (56737, 768)
# KURE: (56737, 1024)
```

**ì¤‘ìš”**: ì„ë² ë”©ì€ **L2 ì •ê·œí™”**ë˜ì–´ ìˆì–´ì„œ ë‚´ì (dot product) = ì½”ì‚¬ì¸ ìœ ì‚¬ë„

#### Passage Metadata (`.jsonl`)

```jsonl
{"passage_id": 0, "doc_id": 0, "title": "ë¬¸ì„œì œëª©", "text": "...", "start_char": 0, "end_char": 500, "is_chunk": false}
{"passage_id": 1, "doc_id": 0, "title": "ë¬¸ì„œì œëª©", "text": "...", "start_char": 500, "end_char": 1000, "is_chunk": true}
...
```

| í•„ë“œ | ì„¤ëª… |
|------|------|
| `passage_id` | ìœ ì¼í•œ passage ì‹ë³„ì (0ë¶€í„° ìˆœì°¨) |
| `doc_id` | ì›ë³¸ wiki document ID |
| `title` | ë¬¸ì„œ ì œëª© |
| `text` | passage í…ìŠ¤íŠ¸ |
| `start_char` | ì›ë³¸ ë¬¸ì„œ ë‚´ ì‹œì‘ ìœ„ì¹˜ |
| `end_char` | ì›ë³¸ ë¬¸ì„œ ë‚´ ë ìœ„ì¹˜ |
| `is_chunk` | chunking ì—¬ë¶€ |

#### Retrieval Cache (`.jsonl`)

```jsonl
{
  "id": "mrc-0-000001",
  "question": "ì§ˆë¬¸ í…ìŠ¤íŠ¸",
  "retrieved": [
    {"passage_id": 123, "doc_id": 456, "score_dense": 0.85, "score_bm25": 12.5},
    {"passage_id": 789, "doc_id": 101, "score_dense": 0.82, "score_bm25": 11.2},
    ...
  ]
}
```

---

## 2. Answer Offset ì „ëµ (âš ï¸ í•µì‹¬)

### 2.1 ë¬¸ì œ ìƒí™©

Train ë°ì´í„°ì˜ `answer_start`ëŠ” **ì›ë³¸ gold context** ê¸°ì¤€ ì¸ë±ìŠ¤:

```python
example = {
    "context": "ì›ë³¸ gold context í…ìŠ¤íŠ¸...",  # ì›ë³¸ ë¬¸ì„œ
    "answers": {
        "text": ["ì •ë‹µ"],
        "answer_start": [150]  # â† ì›ë³¸ context ê¸°ì¤€!
    }
}
```

ê·¸ëŸ°ë° Retrieval passageëŠ” **chunkingëœ ë‹¤ë¥¸ í…ìŠ¤íŠ¸**:

```python
retrieval_passage = "chunked ë˜ëŠ” ë‹¤ë¥¸ ë¬¸ì„œì˜ í…ìŠ¤íŠ¸..."  # answer_start=150ì´ ë¬´ì˜ë¯¸
```

**ê²°ê³¼**: `answer_start=150`ì„ ê·¸ëŒ€ë¡œ ì“°ë©´ ì—‰ëš±í•œ ìœ„ì¹˜ê°€ labelì´ ë¨  
â†’ ëª¨ë¸ì´ ì˜ëª»ëœ gradientë¥¼ ë°›ìŒ â†’ í•™ìŠµ íš¨ê³¼ ê°ì†Œ

### 2.2 í•´ê²° ì „ëµ

#### ì˜µì…˜ A: PositiveëŠ” Gold Contextë§Œ ì‚¬ìš© (âœ… í˜„ì¬ êµ¬í˜„)

```python
def _get_train_item(self, example, qid, question):
    # Positive: í•­ìƒ ì›ë³¸ gold context ì‚¬ìš©
    selected_contexts.append(("pos", None))  # None = gold context ì‚¬ìš©
    
    # Negativeë§Œ retrieval passage ì‚¬ìš©
    for neg in hard_negatives:
        selected_contexts.append(("neg", neg))
    
    label, chosen = random.choice(selected_contexts)
    
    if label == "pos":
        # âœ… ì›ë³¸ context ì‚¬ìš© â†’ answer_startê°€ ì •í™•í•¨
        return self._tokenize_with_gold_context(example, question)
    else:
        # Negative: CLS tokenì´ answer
        return self._tokenize_without_answer(question, retrieval_passage)
```

**ì¥ì **:
- Answer offset ë¬¸ì œ 100% í•´ì†Œ
- êµ¬í˜„ ë‹¨ìˆœ
- Hard negativeëŠ” ì—¬ì „íˆ retrievalì—ì„œ ê°€ì ¸ì˜´ (í•™ìŠµ íš¨ê³¼ ìœ ì§€)

#### ì˜µì…˜ B: Retrieval Passageì—ì„œë„ Positive ì‚¬ìš© (í™•ì¥ìš©)

```python
def _get_train_item(self, example, qid, question):
    # ...
    if label == "pos" and use_retrieval_positive:
        answer_text = example["answers"]["text"][0]
        local_start = retrieval_passage.find(answer_text)
        
        if local_start == -1:
            # ì •ë‹µì´ passageì— ì—†ìœ¼ë©´ gold contextë¡œ fallback
            return self._tokenize_with_gold_context(example, question)
        
        # âœ… passage ê¸°ì¤€ìœ¼ë¡œ answer_start ì¬ê³„ì‚°
        return self._tokenize_with_answer_in_passage(
            question, retrieval_passage, answer_text, local_start
        )
```

**ì£¼ì˜ì‚¬í•­**:
- `answer_text.find()`ëŠ” **ì²« ë²ˆì§¸ ë§¤ì¹­**ë§Œ ë°˜í™˜
- ë™ì¼ í…ìŠ¤íŠ¸ê°€ ì—¬ëŸ¬ ë²ˆ ë‚˜ì˜¤ë©´ ì˜ëª»ëœ ìœ„ì¹˜ ê°€ëŠ¥
- ë”°ë¼ì„œ ì˜µì…˜ Aê°€ ë” ì•ˆì „í•¨

---

## 3. ì„ë² ë”© ìƒì„±/ë¡œë“œ íë¦„

### 3.1 Corpus Embedding ìƒì„±

```bash
# KURE corpus embedding ìƒì„±
python -m src.retrieval.embed.build_kure_corpus

# ë‚´ë¶€ ë™ì‘:
# 1. wikipedia_documents.json ë¡œë“œ
# 2. ì¤‘ë³µ ì œê±° (56,737ê°œ unique passages)
# 3. SentenceTransformer("nlpai-lab/KURE-v1") ë¡œë“œ
# 4. ë°°ì¹˜ ì¸ì½”ë”© (normalize=True)
# 5. data/embeddings/kure_corpus_emb.npy ì €ì¥
# 6. data/embeddings/kure_passages_meta.jsonl ì €ì¥
```

### 3.2 Retrieval Cache ìƒì„±

```bash
# Weighted Hybrid (BM25 + KURE) ìºì‹œ ìƒì„±
python -m src.retrieval.build_retrieval_cache \
    --split train \
    --top_k 50 \
    --alpha 0.7
```

### 3.3 Readerì—ì„œ ì‚¬ìš©

```python
# 1. ìºì‹œ ë¡œë“œ
cache = load_retrieval_cache("data/cache/retrieval/train_top50.jsonl")

# 2. Passage corpus ë¡œë“œ
passages = load_passages_corpus(
    passages_meta_path="data/embeddings/kure_passages_meta.jsonl"
)

# 3. Dataset ìƒì„±
dataset = MRCWithRetrievalDataset(
    examples=train_examples,
    retrieval_cache=cache,
    passages_corpus=passages,
    tokenizer=tokenizer,
    mode="train",
)
```

---

## 4. Chunking ì „ëµ

### 4.1 ì–¸ì œ Chunking í•˜ë‚˜?

| ìƒí™© | Chunking |
|------|----------|
| ë¬¸ì„œ ê¸¸ì´ â‰¤ max_length | í•„ìš” ì—†ìŒ |
| ë¬¸ì„œ ê¸¸ì´ > max_length | í•„ìš”í•¨ |
| Dense retrieval (KURE/KoE5) | ë³´í†µ í•„ìš” |
| BM25 retrieval | ë³´í†µ ë¶ˆí•„ìš” |

### 4.2 Chunking íŒŒë¼ë¯¸í„°

```python
# build_kure_corpus.py ê¸°ë³¸ê°’
CHUNK_SIZE = 400      # ê° chunk ìµœëŒ€ ê¸€ì ìˆ˜
CHUNK_OVERLAP = 50    # chunk ê°„ ê²¹ì¹¨
MIN_CHUNK_SIZE = 100  # ìµœì†Œ chunk í¬ê¸° (ë„ˆë¬´ ì‘ìœ¼ë©´ ë²„ë¦¼)
```

### 4.3 Chunking í›„ ë©”íƒ€ë°ì´í„° ê´€ë¦¬

```python
# ì›ë³¸ ë¬¸ì„œ â†’ ì—¬ëŸ¬ passageë¡œ ë¶„í• 
doc_id = 123
passages = [
    {"passage_id": 0, "doc_id": 123, "start_char": 0, "end_char": 400, "is_chunk": True},
    {"passage_id": 1, "doc_id": 123, "start_char": 350, "end_char": 750, "is_chunk": True},
    ...
]
```

---

## 5. ê²½ë¡œ ê´€ë¦¬ ê·œì¹™

### 5.1 ì¤‘ì•™ ì§‘ì¤‘ ê´€ë¦¬ ì›ì¹™

**ì ˆëŒ€ í•˜ë“œì½”ë”© ê¸ˆì§€**. ëª¨ë“  ê²½ë¡œëŠ” `paths.py`ì—ì„œ ê°€ì ¸ì˜¤ê¸°:

```python
# âŒ ì˜ëª»ëœ ë°©ë²•
corpus_emb_path = "./data/embeddings/kure_corpus_emb.npy"

# âœ… ì˜¬ë°”ë¥¸ ë°©ë²•
from src.retrieval.paths import get_path
corpus_emb_path = get_path("kure_corpus_emb")
```

### 5.2 ê²½ë¡œ ìƒíƒœ í™•ì¸

```bash
python -m src.retrieval.paths --status
```

ì¶œë ¥ ì˜ˆì‹œ:
```
ğŸ“‚ Dense Embeddings
--------------------------------------------------
  âœ… koe5_corpus_emb: 221.6 MB
  âŒ kure_corpus_emb: NOT FOUND
  âŒ kure_passages_meta: NOT FOUND
...
```

---

## 6. ë¬¸ì œ í•´ê²° ì²´í¬ë¦¬ìŠ¤íŠ¸

### Answerê°€ CLSë¡œë§Œ ë‚˜ì˜¤ëŠ” ê²½ìš°

1. **answer_startê°€ context ê¸°ì¤€ì¸ì§€ í™•ì¸**
   ```python
   # ë””ë²„ê¹…
   print(f"context length: {len(context)}")
   print(f"answer_start: {answer_start}")
   print(f"answer at position: {context[answer_start:answer_start+20]}")
   ```

2. **Chunking ì—¬ë¶€ í™•ì¸**
   - Chunked passageë¥¼ positiveë¡œ ì‚¬ìš©í•˜ë©´ì„œ ì›ë³¸ answer_startë¥¼ ì“°ë©´ ë¬¸ì œ

3. **í•´ê²°ì±…**
   - ì˜µì…˜ A ì ìš© (positiveëŠ” gold contextë§Œ ì‚¬ìš©)
   - ë˜ëŠ” ì˜µì…˜ B ì ìš© (answer_text.find()ë¡œ ì¬ê³„ì‚°)

### ì„ë² ë”© íŒŒì¼ ë¶ˆì¼ì¹˜

1. **passage_id ì¼ê´€ì„± í™•ì¸**
   ```python
   # corpus_emb[passage_id] â†” passage_metas[passage_id] ì¼ì¹˜í•´ì•¼ í•¨
   assert len(corpus_emb) == len(passage_metas)
   ```

2. **ìºì‹œ ì¬ìƒì„±**
   ```bash
   rm data/cache/retrieval/*.jsonl
   python -m src.retrieval.build_retrieval_cache --split train
   ```

---

## 7. ìš”ì•½

| êµ¬ë¶„ | ì„¤ëª… |
|------|------|
| **Positive context** | í•­ìƒ ì›ë³¸ gold context ì‚¬ìš© (answer_start ì •í™•) |
| **Negative context** | Retrieval passage ì‚¬ìš© (BM25/KURE hard negatives) |
| **ì„ë² ë”© í˜•ì‹** | `.npy` (L2 ì •ê·œí™”ë¨) |
| **ë©”íƒ€ë°ì´í„°** | `.jsonl` (passage_id, doc_id, text ë“±) |
| **ê²½ë¡œ ê´€ë¦¬** | `src/retrieval/paths.py` ì¤‘ì•™ ì§‘ì¤‘ |

**í•µì‹¬ ì›ì¹™**: ReaderëŠ” ì„ë² ë”© ë¡œì§ì„ ëª°ë¼ë„ ë¨. **í…ìŠ¤íŠ¸ + ì •í™•í•œ answer_startë§Œ ìˆìœ¼ë©´ OK.**
