# MRC (ODQA) í”„ë¡œì íŠ¸ ê¸°ë³¸ ì„¤ì • í…œí”Œë¦¿
# ================================================
# ëª¨ë“  ì‹¤í—˜ ì„¤ì •ì€ ì´ íŒŒì¼ì„ ê¸°ë°˜ìœ¼ë¡œ configs/active/ì— ìƒì„±í•©ë‹ˆë‹¤.
# 
# Retrieval Pipeline: KURE + BM25 Weighted Hybrid (ìºì‹œ ê¸°ë°˜)
# Training: Dynamic Hard Negative Sampling (DHN)
# 
# ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸: 2025-12-10

##################################
# --- model (ModelArguments) ---
##################################
model_name_or_path: klue/bert-base          # ì‚¬ì „í•™ìŠµ ëª¨ë¸ ë˜ëŠ” checkpoint ê²½ë¡œ
# config_name: null                         # ëª¨ë¸ê³¼ ë™ì¼í•˜ë©´ ìƒëµ ê°€ëŠ¥
# tokenizer_name: null                      # ëª¨ë¸ê³¼ ë™ì¼í•˜ë©´ ìƒëµ ê°€ëŠ¥

# [Inference ì „ìš© ì„¤ì •]
use_trained_model: true                     # inference ì‹œ output_dirì—ì„œ best checkpoint ìë™ íƒìƒ‰
                                            # falseë¡œ ì„¤ì •í•˜ë©´ model_name_or_pathì˜ pretrained model ì§ì ‘ ì‚¬ìš©


##################################
# --- data (DataTrainingArguments) ---
##################################
train_dataset_name: ./data/train_dataset    # trainì—ì„œ ì‚¬ìš©í•˜ëŠ” ë°ì´í„°ì…‹ (train + validation split í¬í•¨)
infer_dataset_name: ./data/test_dataset     # test split ë°ì´í„°ì…‹ (ì œì¶œìš©, ì •ë‹µ ì—†ìŒ)

overwrite_cache: false
preprocessing_num_workers: 4
max_seq_length: 384
pad_to_max_length: false
doc_stride: 128
max_answer_length: 30

# === Retrieval ì„¤ì • ===
# ê¸°ë³¸ê°’: KURE + BM25 Weighted Hybrid (ìµœì í™”ë¨)
eval_retrieval: true                        # inference ì‹œ retrieval ì‚¬ìš© ì—¬ë¶€
train_retrieval: false                      # trainì€ DHNìœ¼ë¡œ ëŒ€ì²´ (dynamic_hard_negative.enabled=true ì‹œ)
retrieval_type: weighted_hybrid             # sparse, dense, bm25, koe5, kure, hybrid, weighted_hybrid
top_k_retrieval: 10                         # inference ì‹œ ìƒìœ„ kê°œ ë¬¸ì„œ ì‚¬ìš©
use_faiss: false                            # FAISS ì‚¬ìš© ì—¬ë¶€ (falseë©´ exhaustive search)
num_clusters: 64                            # FAISS í´ëŸ¬ìŠ¤í„° ìˆ˜ (use_faiss=trueì¼ ë•Œë§Œ)

# [Inference ì „ìš© ì„¤ì •]
# âš ï¸ Splitë³„ ì •ì±…:
#   - test: gold context ì—†ìŒ â†’ eval_retrieval=True í•„ìˆ˜ (ìë™ ê²€ì¦)
#   - validation: gold context ìˆìŒ â†’ eval_retrieval ì„ íƒ ê°€ëŠ¥
#   - train: gold context ìˆìŒ â†’ eval_retrieval ì„ íƒ ê°€ëŠ¥
inference_split: test                       # ì¶”ë¡  ëŒ€ìƒ: train / validation / test

# ğŸ’¡ compare_retrieval: validation ì „ìš© ê¸°ëŠ¥
# gold context vs retrieval context ì„±ëŠ¥ ë¹„êµ (ë¹„ìš© í¼, í•„ìš”ì‹œë§Œ true)
compare_retrieval: false


##################################
# --- retrieval ---
##################################
# Retrieval ê°ì²´ê°€ ì§ì ‘ ì°¸ì¡°í•˜ëŠ” ìƒì„¸ ì„¤ì •
# paths.pyì˜ ì¤‘ì•™ ê´€ë¦¬ ê²½ë¡œë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©

retrieval:
  # ê¸°ë³¸ ê²½ë¡œ (paths.pyì™€ ì¼ì¹˜)
  data_path: ./data
  context_path: wikipedia_documents.json
  
  # Weighted Hybrid (BM25 + KURE) ì„¤ì •
  alpha: 0.35                               # BM25 ê°€ì¤‘ì¹˜ (ìµœì ê°’: 0.35)
                                            # hybrid_score = Î±Ã—BM25_norm + (1-Î±)Ã—Dense_norm
  
  # KURE Dense Retrieval
  dense_model: nlpai-lab/KURE-v1
  corpus_emb_path: ./data/embeddings/kure_corpus_emb.npy
  passages_meta_path: ./data/embeddings/kure_passages_meta.jsonl
  
  # BM25 Sparse Retrieval
  bm25_k1: 1.5
  bm25_b: 0.75
  
  # ìºì‹œ ê²½ë¡œ (Dynamic Hard Negative í•™ìŠµìš©)
  cache_dir: ./data/cache/retrieval
  train_cache: ./data/cache/retrieval/train_top50.jsonl
  val_cache: ./data/cache/retrieval/val_top50.jsonl
  test_cache: ./data/cache/retrieval/test_top50.jsonl


##################################
# --- dynamic_hard_negative ---
##################################
# ìºì‹œ ê¸°ë°˜ Dynamic Hard Negative Training ì„¤ì •
# ìºì‹œê°€ ìˆìœ¼ë©´ ìë™ í™œì„±í™”, ì—†ìœ¼ë©´ ê¸°ì¡´ í•™ìŠµ ë°©ì‹ìœ¼ë¡œ fallback

dynamic_hard_negative:
  enabled: true                             # DHN í•™ìŠµ ì‚¬ìš© ì—¬ë¶€
  k_ret: 20                                 # retrieval top-kì—ì„œ ìƒ˜í”Œë§
  k_read: 3                                 # í•™ìŠµì— ì‚¬ìš©í•  context ìˆ˜ (1 pos + n neg)
  alpha: 0.35                               # hybrid score ê³„ì‚°ìš© (retrieval.alphaì™€ ë™ì¼ ê¶Œì¥)
  use_title: true                           # contextì— title í¬í•¨ ì—¬ë¶€


##################################
# --- training (TrainingArguments) ---
##################################
output_dir: ./outputs/{username}/{exp_name} # {username}, {exp_name} ë¶€ë¶„ ë³€ê²½ í•„ìš”
num_train_epochs: 3
per_device_train_batch_size: 16             # 3952 / 16 = 247 steps/epoch
per_device_eval_batch_size: 32
learning_rate: 2.0e-5
warmup_ratio: 0.1                           # ~74 steps warmup (3 epochs ê¸°ì¤€)
weight_decay: 0.01

# Logging & Save ì „ëµ
logging_steps: 100
logging_first_step: true
eval_strategy: epoch                        # ì—í¬í¬ë§ˆë‹¤ í‰ê°€
save_strategy: epoch                        # ì—í¬í¬ë§ˆë‹¤ ì €ì¥
save_total_limit: 2                         # best checkpoint 2ê°œ ìœ ì§€
save_only_model: true                       # modelë§Œ ì €ì¥ (optimizer ì œì™¸)
load_best_model_at_end: true                # í•™ìŠµ ì¢…ë£Œ ì‹œ best checkpoint ë¡œë“œ
metric_for_best_model: exact_match          # EM ê¸°ì¤€ìœ¼ë¡œ best model ì„ íƒ
greater_is_better: true                     # EMì´ ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ
save_safetensors: true                      # safetensors í˜•ì‹ ì €ì¥ (ê¶Œì¥)

# ê¸°íƒ€ ì„¤ì •
fp16: true                                  # Mixed precision (V100 ìµœì í™”)
dataloader_num_workers: 2
gradient_accumulation_steps: 1
resume_from_checkpoint:                     # ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ê°œ ì‹œ ê²½ë¡œ ì§€ì •

# wandb ì„¤ì • (ì„ íƒ)
report_to:                                  # wandb ì‚¬ìš© ì‹œ ì•„ë˜ ì£¼ì„ í•´ì œ
# report_to: wandb
# run_name: mrc-experiment

