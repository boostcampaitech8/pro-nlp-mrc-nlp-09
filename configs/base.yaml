# model, dataëŠ” arguments.pyì— ì§€ì •ëœ dataclass í˜•ì‹ì— ë§ê²Œ ì‘ì„±í•˜ë©´ ë©ë‹ˆë‹¤. (ê°’ ë¯¸ê¸°ì¬ ì‹œ default ê°’ ì‚¬ìš©)
# trainingì€ TrainingArguments í˜•ì‹ì— ë§ê²Œ ì‘ì„±í•˜ë©´ ë©ë‹ˆë‹¤. (HF transformers ì œê³µ)
# base.yaml ê¸°ì¤€ìœ¼ë¡œ configs/exp/exp_name.yaml íŒŒì¼ì„ ë§Œë“¤ê³  ì‹¤í—˜ë³„ ì„¤ì •ì„ ê´€ë¦¬í•˜ì‹œë©´ ë©ë‹ˆë‹¤.

# ëª¨ë“œëŠ” ìŠ¤í¬ë¦½íŠ¸ì—ì„œ ê°•ì œ ì„¸íŒ… (train.py: do_train=True, do_eval=True)
# do_train / do_eval / do_predict ì—¬ê¸°ì„œ ì„¤ì • ë¶ˆí•„ìš”

##################################
# --- model (ModelArguments) ---
##################################
model_name_or_path: klue/bert-base  # ì‚¬ì „í•™ìŠµ ëª¨ë¸ ë˜ëŠ” checkpoint ê²½ë¡œ
# config_name: null         # ëª¨ë¸ê³¼ ë™ì¼í•˜ë©´ ìƒëµ ê°€ëŠ¥
# tokenizer_name: null      # ëª¨ë¸ê³¼ ë™ì¼í•˜ë©´ ìƒëµ ê°€ëŠ¥

# [Inference ì „ìš© ì„¤ì •]
use_trained_model: true     # inference ì‹œ output_dirì—ì„œ best checkpoint ìë™ íƒìƒ‰
#                           # falseë¡œ ì„¤ì •í•˜ë©´ model_name_or_pathì˜ pretrained model ì§ì ‘ ì‚¬ìš©


##################################
# --- data (DataTrainingArguments) ---
##################################
train_dataset_name: ./data/train_dataset     # trainì—ì„œ ì‚¬ìš©í•˜ëŠ” ë°ì´í„°ì…‹ (train + validation split í¬í•¨)
infer_dataset_name: ./data/test_dataset      # test split ë°ì´í„°ì…‹ (ì œì¶œìš©, ì •ë‹µ ì—†ìŒ)

overwrite_cache: false
preprocessing_num_workers: 4
max_seq_length: 384
pad_to_max_length: false
doc_stride: 128
max_answer_length: 30

# retrieval ê´€ë ¨
eval_retrieval: true         # inference ì‹œ retrieval ì‚¬ìš© ì—¬ë¶€
num_clusters:
top_k_retrieval: 10
use_faiss: false

# [Inference ì „ìš© ì„¤ì •]
# âš ï¸ ì¤‘ìš”: Splitë³„ ì •ì±…
# - test: gold context ì—†ìŒ â†’ eval_retrieval=True í•„ìˆ˜ (ìë™ ê²€ì¦)
# - validation: gold context ìˆìŒ â†’ eval_retrieval ì„ íƒ ê°€ëŠ¥
# - train: gold context ìˆìŒ â†’ eval_retrieval ì„ íƒ ê°€ëŠ¥
inference_split: test        # ì¶”ë¡ í•  ë°ì´í„° ì„ íƒ (train/validation/test)
                             # - train: train_datasetì˜ train split ì‚¬ìš© (do_eval + do_predict)
                             # - validation: train_datasetì˜ validation split ì‚¬ìš© (do_eval + do_predict)
                             # - test: infer_dataset_name ì‚¬ìš© (do_predictë§Œ, ì œì¶œìš©, ê¸°ë³¸ê°’)

# ğŸ’¡ compare_retrievalì€ validation ì „ìš© ê¸°ëŠ¥
# gold context vs retrieval context ì„±ëŠ¥ ë¹„êµ (ë¹„ìš© í´ ìˆ˜ë„ ìˆìŒ í•„ìš”ì‹œë§Œ true)
compare_retrieval: false     # validationì—ì„œë§Œ ì‚¬ìš©, testëŠ” ì´ë¯¸ retrieval í•„ìˆ˜

##################################
# --- training (TrainingArguments) ---
##################################
output_dir: ./outputs/{username}/{exp_name}  # {username} ë¶€ë¶„ì„ ë³¸ì¸ ì´ë¦„ìœ¼ë¡œ ë³€ê²½ ê¶Œì¥
num_train_epochs: 5
per_device_train_batch_size: 16          # 3952 / 16 = 247 steps/epoch
per_device_eval_batch_size: 32
learning_rate: 3.0e-5
warmup_ratio: 0.1                         # ~74 steps warmup
weight_decay: 0.01

# logging & save ì „ëµ (247 steps/epoch ê¸°ì¤€)
logging_steps: 50
logging_first_step: true
eval_strategy: epoch                      # ì—í¬í¬ë§ˆë‹¤ í‰ê°€
save_strategy: epoch                      # ì—í¬í¬ë§ˆë‹¤ ì €ì¥
save_total_limit: 1                       # ğŸ’¾ best checkpoint 1ê°œë§Œ ìœ ì§€ (ìš©ëŸ‰ ì ˆì•½)
save_only_model: true                     # ğŸ’¾ modelë§Œ ì €ì¥ (optimizer.pt, scheduler.pt ì €ì¥ ì•ˆí•¨)
load_best_model_at_end: true              # â­ í•™ìŠµ ì¢…ë£Œ ì‹œ best checkpoint ìë™ ë¡œë“œ
metric_for_best_model: "exact_match"      # â­ EM ê¸°ì¤€ìœ¼ë¡œ best model ì„ íƒ
greater_is_better: true                   # â­ EMì´ ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ
save_safetensors: true                    # safetensors í˜•ì‹ìœ¼ë¡œ ì €ì¥ (ê¶Œì¥, ë” ì•ˆì „í•˜ê³  ë¹ ë¦„)

# ê¸°íƒ€ ì„¤ì •
fp16: # í° ëª¨ë¸ ì–‘ìí™”
dataloader_num_workers: 2
gradient_accumulation_steps: 1
resume_from_checkpoint:

# wandb ê´€ë ¨ ì„¤ì •
report_to:
# report_to: wandb              # wandb ì‚¬ìš© ì‹œ ì£¼ì„ í•´ì œ
# run_name: mrc-bert-base       # wandb run name

