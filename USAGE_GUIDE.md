# MRC í”„ë¡œì íŠ¸ ì‹¤í—˜ ê´€ë¦¬ ê°€ì´ë“œ

## ê°œìš”

ì´ í”„ë¡œì íŠ¸ëŠ” YAML ê¸°ë°˜ ì„¤ì •ìœ¼ë¡œ trainê³¼ inferenceë¥¼ í†µí•© ê´€ë¦¬í•˜ë©°, **Confidence Score ê¸°ë°˜ ì˜ˆì¸¡ ë¶„ì„**ì„ ì§€ì›í•©ë‹ˆë‹¤.

**ğŸ¯ í•µì‹¬ ê¸°ëŠ¥:**
- âœ… **Confidence Score ê³„ì‚°**: ëª¨ë¸ í™•ì‹ ë„ ê¸°ë°˜ ì˜ˆì¸¡ í’ˆì§ˆ í‰ê°€
- âœ… **YAML ê¸°ë°˜ ì„¤ì •**: ì‹¤í—˜ ì¬í˜„ì„± ë³´ì¥
- âœ… **Makefile ìë™í™”**: ê°„í¸í•œ ëª…ë ¹ì–´ë¡œ ì‹¤í—˜ ê´€ë¦¬
- âœ… **Batch ëª¨ë“œ**: ì—¬ëŸ¬ ì‹¤í—˜ ìë™ ìˆœì°¨ ì‹¤í–‰ (ë°¤ìƒˆ GPU ê°€ë™)
- âœ… **Best Checkpoint ìë™ íƒìƒ‰**: ìˆ˜ë™ ê²½ë¡œ ì§€ì • ë¶ˆí•„ìš”
- âœ… **ìƒì„¸ ë¶„ì„ ë„êµ¬**: Epochë³„ ë©”íŠ¸ë¦­, Retrieval ì„±ëŠ¥ ë¹„êµ

## âš¡ ë¹ ë¥¸ ì‹œì‘ (Makefile)

### ê¸°ë³¸ ì›Œí¬í”Œë¡œìš°

```bash
# 1. ë„ì›€ë§ í™•ì¸
make help

# 2. ë‹¨ì¼ ì‹¤í—˜ (train â†’ inference ìë™ ì—°ê²°)
make train-pipeline CONFIG=configs/my_experiment.yaml

# 3. ì—¬ëŸ¬ ì‹¤í—˜ ë°¤ìƒˆ ëŒë¦¬ê¸° (GPU ìµœëŒ€ í™œìš©)
# Step 1: ì‹¤í—˜í•  configë“¤ì„ active í´ë”ì— ì¤€ë¹„
cp configs/exp1.yaml configs/active/
cp configs/exp2.yaml configs/active/

# Step 2: Batch ì‹¤í–‰
make batch

# 4. ê²°ê³¼ ë¶„ì„
make compare-results          # ëª¨ë“  ì‹¤í—˜ F1/EM ë¹„êµ
make show-best                # ìµœê³  ì„±ëŠ¥ ì‹¤í—˜ ì°¾ê¸°
```

### ì£¼ìš” ëª…ë ¹ì–´ ìš”ì•½

| ëª…ë ¹ì–´ | ì„¤ëª… | ì˜ˆì‹œ |
|--------|------|------|
| `make train-pipeline` | Train + Inference ìë™ ì‹¤í–‰ | `make train-pipeline CONFIG=configs/exp.yaml` |
| `make batch` | configs/active/*.yaml ìˆœì°¨ ì‹¤í–‰ | `make batch` |
| `make compare-results` | ëª¨ë“  ì‹¤í—˜ ê²°ê³¼ ë¹„êµ | `make compare-results` |
| `make gpu-status` | GPU ì‚¬ìš© í˜„í™© í™•ì¸ | `make gpu-status` |

## ğŸ“‚ ì¶œë ¥ íŒŒì¼ êµ¬ì¡°

Train/Inference ì‹¤í–‰ í›„ ìƒì„±ë˜ëŠ” íŒŒì¼ë“¤:

```
outputs/dahyeong/my_experiment/
â”œâ”€â”€ ğŸ“ checkpoint-*/                      # í•™ìŠµ ì²´í¬í¬ì¸íŠ¸
â”œâ”€â”€ ğŸ“„ best_checkpoint_path.txt          # Best checkpoint ê²½ë¡œ (ìë™ ìƒì„±)
â”œâ”€â”€ ğŸ“„ trainer_state.json                # Trainer ìƒíƒœ (HuggingFace)
â”œâ”€â”€ ğŸ“„ config_used.yaml                  # ì‹¤í—˜ì— ì‚¬ìš©ëœ ì„¤ì •
â”‚
â”œâ”€â”€ ğŸ“Š í•™ìŠµ ë©”íŠ¸ë¦­
â”‚   â”œâ”€â”€ training_metrics.json           # ì „ì²´ í•™ìŠµ ë¡œê·¸
â”‚   â”œâ”€â”€ training_metrics.png            # í•™ìŠµ ê³¡ì„  ê·¸ë˜í”„
â”‚   â”œâ”€â”€ epoch_summary.json              # ì—í¬í¬ë³„ ìš”ì•½ (JSON)
â”‚   â””â”€â”€ epoch_summary.md                # ì—í¬í¬ë³„ ìš”ì•½ (Markdown í…Œì´ë¸”)
â”‚
â”œâ”€â”€ ğŸ¯ ì˜ˆì¸¡ ê²°ê³¼
â”‚   â”œâ”€â”€ test_pred.csv                   # Test ì œì¶œ íŒŒì¼ (id, prediction)
â”‚   â”œâ”€â”€ predictions_test.json           # Test ìƒì„¸ ì˜ˆì¸¡
â”‚   â”œâ”€â”€ predictions_val.json            # Validation ì˜ˆì¸¡
â”‚   â””â”€â”€ predictions_train.json          # Train ì˜ˆì¸¡
â”‚
â”œâ”€â”€ ğŸ§  Confidence ë¶„ì„ (NEW!)
â”‚   â”œâ”€â”€ logits_test.json                # Test logits (start/end/probability)
â”‚   â”œâ”€â”€ logits_val.json                 # Validation logits
â”‚   â”œâ”€â”€ logits_train.json               # Train logits
â”‚   â”œâ”€â”€ test_confidence.csv             # Test confidence scores
â”‚   â”œâ”€â”€ val_confidence.csv              # Validation confidence scores
â”‚   â””â”€â”€ train_confidence.csv            # Train confidence scores
â”‚       # êµ¬ì¡°: id, prediction, max_prob, avg_prob, is_correct, pred_length
â”‚
â””â”€â”€ ğŸ“‹ ìƒì„¸ ë¶„ì„
    â”œâ”€â”€ val_detailed_results.json       # Validation ìƒì„¸ (question, context, prediction, confidence í¬í•¨)
    â”œâ”€â”€ train_detailed_results.json     # Train ìƒì„¸
    â”œâ”€â”€ eval_results.json               # Validation ë©”íŠ¸ë¦­ (EM, F1)
    â””â”€â”€ train_results.json              # Train ë©”íŠ¸ë¦­
```

### í•µì‹¬ íŒŒì¼ ì„¤ëª…

#### 1. **Confidence CSV** (ì˜ˆ: `val_confidence.csv`)
ëª¨ë¸ì´ ì–¼ë§ˆë‚˜ í™•ì‹ í•˜ëŠ”ì§€ ìˆ˜ì¹˜í™”:
```csv
id,prediction,max_prob,avg_prob,is_correct,pred_length
mrc-0-000000,ì„œìš¸,0.987,0.945,1,2
mrc-0-000001,1950ë…„,0.234,0.189,0,5
```
- `max_prob`: ìµœëŒ€ í™•ë¥  (ëª¨ë¸ì˜ í™•ì‹ ë„)
- `avg_prob`: í‰ê·  í™•ë¥ 
- `is_correct`: ì •ë‹µ ì—¬ë¶€ (1=ì •ë‹µ, 0=ì˜¤ë‹µ)

**í™œìš©**:
```bash
# Low confidence ì˜¤ë‹µ ì°¾ê¸° (ëª¨ë¸ë„ í—·ê°ˆë¦¬ëŠ” ì¼€ì´ìŠ¤)
awk -F, '$5==0 && $4<0.5' val_confidence.csv

# High confidence ì˜¤ë‹µ ì°¾ê¸° (ì²´ê³„ì  ì˜¤ë¥˜ - ìœ„í—˜!)
awk -F, '$5==0 && $4>0.8' val_confidence.csv
```

#### 2. **Detailed Results JSON** (ì˜ˆ: `val_detailed_results.json`)
ê° exampleì˜ ëª¨ë“  ì •ë³´:
```json
[
  {
    "id": "mrc-0-000000",
    "question": "ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ”?",
    "context": "ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ” ì„œìš¸ì´ë‹¤...",
    "prediction": "ì„œìš¸",
    "ground_truth": ["ì„œìš¸", "ì„œìš¸íŠ¹ë³„ì‹œ"],
    "em_score": 100.0,
    "f1_score": 100.0,
    "confidence_max": 0.987,  // â† NEW!
    "confidence_avg": 0.945   // â† NEW!
  }
]
```

**í™œìš©**: ì˜¤ë‹µ ë¶„ì„, ì§ˆë¬¸ ìœ í˜•ë³„ ì„±ëŠ¥ í‰ê°€

#### 3. **Epoch Summary Markdown** (`epoch_summary.md`)
ì‚¬ëŒì´ ì½ê¸° ì‰¬ìš´ í•™ìŠµ ì§„í–‰ ìƒí™©:
```markdown
| Epoch | EM Score | F1 Score | Eval Loss | Step |
|-------|----------|----------|-----------|------|
| 1.00  | 68.50    | 75.20    | 0.8234    | 499  |
| 2.00  | 70.30    | 76.80    | 0.7123    | 998  |
| 3.00  | 72.10    | 78.30    | 0.6891    | 1497 |

**Best Exact Match:** 72.10% (Epoch 3.00)
**Best F1 Score:** 78.30% (Epoch 3.00)
```



## ğŸ”§ ì£¼ìš” ê¸°ëŠ¥

### 1. Train + Inference Pipeline

**í•œ ì¤„ ëª…ë ¹ìœ¼ë¡œ í•™ìŠµë¶€í„° ì¶”ë¡ ê¹Œì§€**:
```bash
make train-pipeline CONFIG=configs/my_experiment.yaml
```

ì‹¤í–‰ ê³¼ì •:
1. âœ… GPU ë©”ëª¨ë¦¬ í™•ì¸ (10GB ë¯¸ë§Œ ëŒ€ê¸°)
2. ğŸš€ Training ì‹œì‘ (3 epochs)
3. ğŸ’¾ Best checkpoint ìë™ ì €ì¥
4. ğŸ” Validation set inference (EM/F1 ê³„ì‚°)
5. ğŸ“Š Confidence score ê³„ì‚°
6. ğŸ“ Test set inference (ì œì¶œìš© test_pred.csv ìƒì„±)

ìƒì„± íŒŒì¼:
- `test_pred.csv` (ì œì¶œìš©)
- `val_confidence.csv` (ë¶„ì„ìš©)
- `val_detailed_results.json` (ìƒì„¸ ë¶„ì„ìš©)
- `epoch_summary.md` (í•™ìŠµ ì§„í–‰ ìš”ì•½)

---

### 2. Batch ì‹¤í—˜ (ë°¤ìƒˆ GPU ëŒë¦¬ê¸°)

**ì—¬ëŸ¬ ì‹¤í—˜ì„ ìˆœì°¨ì ìœ¼ë¡œ ìë™ ì‹¤í–‰**:

```bash
# Step 1: ì‹¤í—˜í•  configë“¤ì„ active í´ë”ì— ì¤€ë¹„
ls configs/active/
# exp1_bert_lr3e5.yaml
# exp2_bert_lr5e5.yaml
# exp3_electra_lr3e5.yaml

# Step 2: Batch ì‹¤í–‰
make batch

# ì‹¤í–‰ ê²°ê³¼:
# ğŸ“¦ [1/3] Processing: exp1_bert_lr3e5.yaml
#   âœ… Train completed (57.5min)
#   âœ… Inference completed
# ğŸ“¦ [2/3] Processing: exp2_bert_lr5e5.yaml
#   âœ… Train completed (58.5min)
#   âœ… Inference completed
# ğŸ“¦ [3/3] Processing: exp3_electra_lr3e5.yaml
#   âœ… Train completed (61.6min)
#   âœ… Inference completed
#
# ğŸ‰ ALL EXPERIMENTS COMPLETED! (Total: 3h 2min)
```

**ì£¼ìš” íŠ¹ì§•**:
- âœ… ì‹¤íŒ¨í•´ë„ ë‹¤ìŒ ì‹¤í—˜ ê³„ì† ì§„í–‰
- âœ… ê° ì‹¤í—˜ ì†Œìš” ì‹œê°„ ìë™ ì¶”ì 
- âœ… ìµœì¢… Summary ë¦¬í¬íŠ¸ ìƒì„±
- âœ… GPU ê³µë°± ì‹œê°„ ìµœì†Œí™”

---

### 3. Confidence ê¸°ë°˜ ì˜ˆì¸¡ ë¶„ì„

**ëª¨ë¸ì˜ í™•ì‹ ë„ë¥¼ ìˆ˜ì¹˜í™”í•˜ì—¬ í’ˆì§ˆ í‰ê°€**:

```bash
# 1. í•™ìŠµ ì™„ë£Œ í›„ confidence.csv ìƒì„±ë¨
cat outputs/dahyeong/my_exp/val_confidence.csv
# id,prediction,max_prob,avg_prob,is_correct,pred_length
# mrc-0-000000,ì„œìš¸,0.987,0.945,1,2
# mrc-0-000001,1950ë…„,0.234,0.189,0,5

# 2. Low confidence ì˜¤ë‹µ ì°¾ê¸° (ëª¨ë¸ë„ í—·ê°ˆë¦¬ëŠ” ì¼€ì´ìŠ¤)
awk -F, '$5==0 && $4<0.5 {print $1,$2,$4}' val_confidence.csv | head -10
# mrc-0-000123 ê¹€ì˜ì‚¼ 0.234
# mrc-0-000456 1980ë…„ 0.312

# 3. High confidence ì˜¤ë‹µ ì°¾ê¸° (ì²´ê³„ì  ì˜¤ë¥˜ - ìœ„í—˜!)
awk -F, '$5==0 && $4>0.8 {print $1,$2,$4}' val_confidence.csv
# mrc-0-000789 ì„œìš¸íŠ¹ë³„ì‹œ 0.892  # "ì„œìš¸"ì´ ì •ë‹µì¸ë° "ì„œìš¸íŠ¹ë³„ì‹œ"ë¡œ ì˜ˆì¸¡

# 4. Pythonìœ¼ë¡œ ìƒì„¸ ë¶„ì„
python << EOF
import pandas as pd
df = pd.read_csv('outputs/dahyeong/my_exp/val_confidence.csv')

# ì •í™•ë„
accuracy = df['is_correct'].mean()
print(f"Accuracy: {accuracy:.2%}")

# Confidence ë¶„í¬
errors = df[df['is_correct'] == 0]
print(f"\nError confidence distribution:")
print(errors['avg_prob'].describe())
EOF
```

**Confidence Score í™œìš©**:
1. **Low confidence ì˜¤ë‹µ**: ë°ì´í„° í’ˆì§ˆ ë¬¸ì œ ë˜ëŠ” ì–´ë ¤ìš´ ì§ˆë¬¸
2. **High confidence ì˜¤ë‹µ**: ëª¨ë¸ì˜ ì²´ê³„ì  ì˜¤ë¥˜ (ì¬í•™ìŠµ í•„ìš”)
3. **Low confidence ì •ë‹µ**: ìš´ìœ¼ë¡œ ë§ì¶˜ ì¼€ì´ìŠ¤ (ë¶ˆì•ˆì •)
4. **High confidence ì •ë‹µ**: ëª¨ë¸ì´ ì˜ í•™ìŠµí•œ ì¼€ì´ìŠ¤

---

### 4. ê²°ê³¼ ë¹„êµ ë° ë¶„ì„

```bash
# 1. ëª¨ë“  ì‹¤í—˜ ê²°ê³¼ ë¹„êµ (F1, EM)
make compare-results

# ì¶œë ¥ ì˜ˆì‹œ:
# ğŸ“Š Comparing experiment results:
#   exp1_bert_lr3e5       F1: 68.45    EM: 55.32
#   exp2_bert_lr5e5       F1: 71.23    EM: 59.87
#   exp3_electra_lr3e5    F1: 69.87    EM: 57.45

# 2. ìµœê³  ì„±ëŠ¥ ì‹¤í—˜ ì°¾ê¸°
make show-best

# ì¶œë ¥ ì˜ˆì‹œ:
# ğŸ† Best experiment (by F1 score):
#   Experiment: exp2_bert_lr5e5
#   F1 Score: 71.23
#   Path: ./outputs/dahyeong/exp2_bert_lr5e5/

# 3. GPU ìƒíƒœ í™•ì¸
make gpu-status

# ì¶œë ¥ ì˜ˆì‹œ:
# ğŸ–¥ï¸  GPU Status:
#   GPU 0: NVIDIA A100 | Util: 85% | Mem: 25631/40960 MB
```

---

### 5. Active Config ê´€ë¦¬

`configs/active/` í´ë”ë¡œ ì‹¤í—˜ ëŒ€ìƒ ê´€ë¦¬:

```bash
# 1. ëª¨ë“  config ëª©ë¡ ë³´ê¸°
make list-active

# 2. Config ìœ íš¨ì„± ê²€ì¦
make check-config CONFIG=configs/my_experiment.yaml

# 3. Active í´ë” ë¹„ìš°ê¸° (ë°°ì¹˜ ì‹¤í–‰ ì „ ì •ë¦¬)
make clean-active
```



## ğŸ“‹ Makefile ëª…ë ¹ì–´ ì „ì²´ ëª©ë¡

### ì‹¤í—˜ ì‹¤í–‰

| ëª…ë ¹ì–´ | ì„¤ëª… | ìš©ë„ |
|--------|------|------|
| `make train-pipeline CONFIG=...` | Train + Test inference | **ê°€ì¥ ë§ì´ ì‚¬ìš©** |
| `make train CONFIG=...` | Trainë§Œ ì‹¤í–‰ | í•™ìŠµë§Œ |
| `make inference CONFIG=...` | Inferenceë§Œ ì‹¤í–‰ | ì´ë¯¸ í•™ìŠµëœ ëª¨ë¸ |
| `make eval-val CONFIG=...` | Validation ë¶„ì„ (gold vs retrieval) | Retrieval ì„±ëŠ¥ ë¹„êµ |
| `make eval-test CONFIG=...` | Test inference | ì œì¶œìš© |

### Batch ì‹¤í—˜

| ëª…ë ¹ì–´ | ì„¤ëª… |
|--------|------|
| `make batch` | configs/active/*.yaml ìˆœì°¨ ì‹¤í–‰ |
| `make list-active` | Active config ëª©ë¡ ë³´ê¸° |
| `make check-active` | Active config ìœ íš¨ì„± ê²€ì¦ |

### ê²°ê³¼ ë¶„ì„

| ëª…ë ¹ì–´ | ì„¤ëª… |
|--------|------|
| `make compare-results` | ëª¨ë“  ì‹¤í—˜ F1/EM ë¹„êµ |
| `make show-best` | ìµœê³  F1 ì‹¤í—˜ ì°¾ê¸° |

### ìœ í‹¸ë¦¬í‹°

| ëª…ë ¹ì–´ | ì„¤ëª… |
|--------|------|
| `make gpu-status` | GPU ì‚¬ìš© í˜„í™© |
| `make clean-checkpoints` | checkpoint í´ë”ë§Œ ì‚­ì œ |
| `make check-config CONFIG=...` | YAML ìœ íš¨ì„± ê²€ì¦ |
| `make help` | ë„ì›€ë§ |



### ëª¨ë¸ ì„¤ì • (ModelArguments)

```yaml
##################################
# --- model (ModelArguments) ---
##################################
model_name_or_path: klue/bert-base  # í•™ìŠµ ì‹œì‘ ëª¨ë¸ (pretrained ë˜ëŠ” ê²½ë¡œ)
# config_name: null                 # ëª¨ë¸ê³¼ ë™ì¼í•˜ë©´ ìƒëµ
# tokenizer_name: null              # ëª¨ë¸ê³¼ ë™ì¼í•˜ë©´ ìƒëµ

# [Inference ì „ìš©]
use_trained_model: true             # true: output_dirì—ì„œ best checkpoint ìë™ íƒìƒ‰
                                    # false: model_name_or_path ì§ì ‘ ì‚¬ìš©
```

### ë°ì´í„° ì„¤ì • (DataTrainingArguments)

```yaml
##################################
# --- data (DataTrainingArguments) ---
##################################
train_dataset_name: ./data/train_dataset     # í•™ìŠµìš© ë°ì´í„° (train/validation split í¬í•¨)
infer_dataset_name: ./data/test_dataset      # í…ŒìŠ¤íŠ¸ ë°ì´í„° (ì •ë‹µ ì—†ìŒ)

# [Inference ì „ìš©]
inference_split: validation                   # ì¶”ë¡ í•  ë°ì´í„° ì„ íƒ
                                              # - train: train_datasetì˜ train split
                                              # - validation: train_datasetì˜ validation split (ê¸°ë³¸)
                                              # - test: infer_dataset_name ì‚¬ìš© (ì œì¶œìš©)
```

### Inference ë™ì‘ ë°©ì‹

| inference_split | ë°ì´í„°ì…‹ | do_eval | do_predict | ìš©ë„ |
|----------------|---------|---------|------------|------|
| train | train_dataset/train | âœ… | âœ… | train set ì„±ëŠ¥ ë¶„ì„ |
| validation | train_dataset/validation | âœ… | âœ… | validation set ì„±ëŠ¥ í™•ì¸ (ê¸°ë³¸) |
| test | infer_dataset_name | âŒ | âœ… | ì œì¶œìš© predictions.json ìƒì„± |

## Best Checkpoint ìë™ íƒìƒ‰ ë¡œì§

`use_trained_model=true`ë¡œ ì„¤ì •í•˜ë©´ ë‹¤ìŒ ìˆœì„œë¡œ checkpointë¥¼ íƒìƒ‰í•©ë‹ˆë‹¤:

1. **`best_checkpoint_path.txt`** (train.pyê°€ ìë™ ìƒì„±)
   - train ì™„ë£Œ ì‹œ best checkpoint ê²½ë¡œê°€ ì €ì¥ë¨
   - ìµœìš°ì„  ì°¸ì¡°

2. **`trainer_state.json`ì˜ `best_model_checkpoint`**
   - HuggingFace Trainerê°€ ìë™ ìƒì„±
   - 2ìˆœìœ„ ì°¸ì¡°

3. **`checkpoint-*` í´ë” ì¤‘ ê°€ì¥ í° ìˆ«ì**
   - ìœ„ íŒŒì¼ë“¤ì´ ì—†ì„ ë•Œ fallback
   - ì˜ˆ: checkpoint-1234 > checkpoint-123

## ğŸ¯ ì‹¤ì „ ì›Œí¬í”Œë¡œìš° ì˜ˆì‹œ

### Case 1: ì²« ì‹¤í—˜ (ì „ì²´ íŒŒì´í”„ë¼ì¸)

```bash
# 1. Base config ë³µì‚¬
cp configs/base.yaml configs/my_first_exp.yaml

# 2. ì„¤ì • ìˆ˜ì •
vim configs/my_first_exp.yaml
# - model_name_or_path: monologg/koelectra-small-v3-discriminator
# - output_dir: ./outputs/dahyeong/koelectra_baseline
# - num_train_epochs: 3

# 3. YAML ìœ íš¨ì„± ê²€ì¦
make check-config CONFIG=configs/my_first_exp.yaml

# 4. Train + Inference ì‹¤í–‰
make train-pipeline CONFIG=configs/my_first_exp.yaml

# 5. ê²°ê³¼ í™•ì¸
ls outputs/dahyeong/koelectra_baseline/
# test_pred.csv              â† ì œì¶œìš©!
# val_confidence.csv         â† ë¶„ì„ìš©
# val_detailed_results.json  â† ì˜¤ë‹µ ë¶„ì„ìš©
# epoch_summary.md           â† í•™ìŠµ ì§„í–‰ í™•ì¸
```

---

### Case 2: ì—¬ëŸ¬ ì‹¤í—˜ ë¹„êµ (Hyperparameter Tuning)

```bash
# 1. ì—¬ëŸ¬ config ì¤€ë¹„
configs/
  â”œâ”€â”€ exp1_lr2e5.yaml   # learning_rate: 2e-5
  â”œâ”€â”€ exp2_lr3e5.yaml   # learning_rate: 3e-5
  â””â”€â”€ exp3_lr5e5.yaml   # learning_rate: 5e-5

# 2. Active í´ë”ë¡œ ë³µì‚¬
cp configs/exp*.yaml configs/active/

# 3. Batch ì‹¤í–‰ (ë°¤ìƒˆ ëŒë¦¬ê¸°)
make batch

# ë‹¤ìŒë‚  ì•„ì¹¨ í™•ì¸:
make compare-results
# ğŸ“Š Comparing experiment results:
#   exp1_lr2e5    F1: 68.45    EM: 55.32
#   exp2_lr3e5    F1: 71.23    EM: 59.87  â† Best!
#   exp3_lr5e5    F1: 69.12    EM: 56.78

make show-best
# ğŸ† Best experiment (by F1 score):
#   Experiment: exp2_lr3e5
#   F1 Score: 71.23

# 4. Active í´ë” ì •ë¦¬
rm configs/active/*.yaml
```

---

### Case 3: Confidence ê¸°ë°˜ ì˜¤ë‹µ ë¶„ì„

```bash
# 1. Validation confidence í™•ì¸
cat outputs/dahyeong/my_exp/val_confidence.csv | head

# 2. ì˜¤ë‹µë§Œ í•„í„°ë§
awk -F, '$5==0' outputs/dahyeong/my_exp/val_confidence.csv > errors.csv

# 3. Confidence ë¶„í¬ í™•ì¸ (Python)
python << EOF
import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv('errors.csv', names=['id','pred','max_prob','avg_prob','is_correct','length'])

# Confidence íˆìŠ¤í† ê·¸ë¨
df['avg_prob'].hist(bins=20)
plt.xlabel('Confidence Score')
plt.ylabel('Error Count')
plt.title('Error Confidence Distribution')
plt.savefig('error_confidence.png')

# Low vs High confidence ì˜¤ë‹µ ë¹„ìœ¨
low_conf = len(df[df['avg_prob'] < 0.5])
high_conf = len(df[df['avg_prob'] > 0.8])
print(f"Low confidence errors: {low_conf} ({low_conf/len(df)*100:.1f}%)")
print(f"High confidence errors: {high_conf} ({high_conf/len(df)*100:.1f}%)")
EOF

# 4. High confidence ì˜¤ë‹µ ìƒì„¸ ë¶„ì„ (ì²´ê³„ì  ì˜¤ë¥˜)
python << EOF
import pandas as pd
import json

# Confidence ë¡œë“œ
conf_df = pd.read_csv('outputs/dahyeong/my_exp/val_confidence.csv')

# Detailed results ë¡œë“œ
with open('outputs/dahyeong/my_exp/val_detailed_results.json') as f:
    details = {item['id']: item for item in json.load(f)}

# High confidence ì˜¤ë‹µ ì°¾ê¸°
high_conf_errors = conf_df[(conf_df['is_correct'] == 0) & (conf_df['avg_prob'] > 0.8)]

print(f"Found {len(high_conf_errors)} high confidence errors (systematic errors)\n")

# ìƒìœ„ 10ê°œ ì¶œë ¥
for idx, row in high_conf_errors.head(10).iterrows():
    detail = details[row['id']]
    print(f"ID: {row['id']}")
    print(f"Question: {detail['question']}")
    print(f"Prediction: {detail['prediction']} (confidence: {row['avg_prob']:.3f})")
    print(f"Ground Truth: {detail['ground_truth']}")
    print(f"---")
EOF
```

---

### Case 4: Best Modelë¡œ Test ì œì¶œ

```bash
# 1. ìµœê³  ì„±ëŠ¥ ì‹¤í—˜ ì°¾ê¸°
make show-best
# ğŸ† Best experiment: exp2_lr3e5
#   Path: ./outputs/dahyeong/exp2_lr3e5/

# 2. í•´ë‹¹ ì‹¤í—˜ì˜ test_pred.csv í™•ì¸
head outputs/dahyeong/exp2_lr3e5/test_pred.csv
# mrc-0-000000	ì„œìš¸
# mrc-0-000001	1950ë…„

# 3. ì œì¶œ
cp outputs/dahyeong/exp2_lr3e5/test_pred.csv submission.csv
# Kaggle/Competition ì‚¬ì´íŠ¸ì— ì—…ë¡œë“œ
```

---

### Case 5: Retrieval ì„±ëŠ¥ ë¹„êµ

```bash
# 1. Validation setì— ëŒ€í•´ gold context vs retrieval context ë¹„êµ
make eval-val CONFIG=configs/my_exp.yaml

# ì‹¤í–‰ ê³¼ì •:
# Step 1: Gold contextë¡œ inference (ìƒí•œì„  ì¸¡ì •)
# Step 2: Retrieval contextë¡œ inference (ì‹¤ì œ ì„±ëŠ¥)
# Step 3: ë‘ ê²°ê³¼ ë¹„êµ

# 2. ë¹„êµ ê²°ê³¼ í™•ì¸
cat outputs/dahyeong/my_exp/retrieval_comparison.json

# ì˜ˆì‹œ ì¶œë ¥:
# {
#   "gold_metrics": {"exact_match": 72.08, "f1": 80.23},
#   "retrieval_metrics": {"exact_match": 65.34, "f1": 74.56},
#   "performance_gap": {"em_drop": 6.74, "f1_drop": 5.67},
#   "rates": {
#     "retrieval_success_rate": 90.63,
#     "retrieval_failure_rate": 6.74
#   }
# }

# 3. Retrieval ì‹¤íŒ¨ ì¼€ì´ìŠ¤ ìƒì„¸ ë¶„ì„
cat outputs/dahyeong/my_exp/retrieval_failures.json | head -20
# Gold contextë¡œëŠ” ë§ì•˜ì§€ë§Œ Retrievalë¡œ í‹€ë¦° ì¼€ì´ìŠ¤ë“¤
# â†’ Retrieval ê°œì„  í•„ìš”!
```



**Makefile ì‚¬ìš©:**
```bash
# 1. YAML ì„¤ì • ì‘ì„±
cp configs/base.yaml configs/bert_lr3e5.yaml
vim configs/bert_lr3e5.yaml  # ì„¤ì • ìˆ˜ì •

# 2. Pipeline ì‹¤í–‰ (train + inference)
make pipeline CONFIG=configs/bert_lr3e5.yaml
```

**Python ì§ì ‘ ì‹¤í–‰:**
```bash
python run.py --mode pipeline --config configs/bert_lr3e5.yaml
```

**ê²°ê³¼ë¬¼:**
- `./outputs/{username}/{exp_name}/` í´ë”ì— ëª¨ë¸, ì²´í¬í¬ì¸íŠ¸, ë©”íŠ¸ë¦­ ì €ì¥
- `best_checkpoint_path.txt`: best checkpoint ê²½ë¡œ
- `predictions.json`: validation set ì˜ˆì¸¡ ê²°ê³¼ (do_eval + do_predict)
- `eval_results.json`: validation ì„±ëŠ¥ ë©”íŠ¸ë¦­

### Case 2: ğŸ”¥ ì—¬ëŸ¬ ì‹¤í—˜ ë°¤ìƒˆ ëŒë¦¬ê¸° (GPU ìµœëŒ€ í™œìš©)

**Makefile ì‚¬ìš© (ê°€ì¥ ê°„ë‹¨):**
```bash
# 1. ì—¬ëŸ¬ ì‹¤í—˜ YAMLì„ active í´ë”ì— ì¤€ë¹„
make activate-config CONFIG=configs/exp1_bert_lr3e5.yaml
make activate-config CONFIG=configs/exp2_bert_lr5e5.yaml
make activate-config CONFIG=configs/exp3_electra_lr3e5.yaml
make activate-config CONFIG=configs/exp4_roberta_lr3e5.yaml

# 2. tmuxì—ì„œ ë°¤ìƒˆ ëŒë¦¬ê¸° (SSH ëŠê²¨ë„ ì•ˆì „)
make tmux-start

# ë˜ëŠ” tmux ì—†ì´ ë°”ë¡œ ì‹¤í–‰
make batch
```

**Python ì§ì ‘ ì‹¤í–‰:**
```bash
# 1. ì—¬ëŸ¬ ì‹¤í—˜ YAML ì¤€ë¹„
configs/
  â”œâ”€â”€ exp1_bert_lr3e5.yaml
  â”œâ”€â”€ exp2_bert_lr5e5.yaml
  â”œâ”€â”€ exp3_electra_lr3e5.yaml
  â””â”€â”€ exp4_roberta_lr3e5.yaml

# 2. Batch ì‹¤í–‰
python run.py --mode batch --batch-mode pipeline \
    --configs configs/exp*.yaml

# ë˜ëŠ” tmuxì™€ í•¨ê»˜ ì‚¬ìš©
tmux new -s experiments
python run.py --mode batch --batch-mode pipeline --configs configs/exp*.yaml
# Ctrl+B, Dë¡œ detach
# ë‚˜ì¤‘ì— tmux attach -t experimentsë¡œ ì¬ì ‘ì†
```

**ì‹¤í–‰ ê²°ê³¼:**
```
ğŸš€ BATCH MODE STARTED
================================================================================
ğŸ“‹ Experiments to run: 4
ğŸ¯ Mode: pipeline
âš™ï¸  Continue on error: True
ğŸ• Start time: 2025-12-07 22:00:00

Experiment list:
  1. exp1_bert_lr3e5
  2. exp2_bert_lr5e5
  3. exp3_electra_lr3e5
  4. exp4_roberta_lr3e5
================================================================================

... (ê° ì‹¤í—˜ ì‹¤í–‰) ...

ğŸ“ˆ BATCH RUN SUMMARY
================================================================================

ğŸ“Š Overall Statistics:
   Total experiments: 4
   âœ… Succeeded: 4
   âŒ Failed: 0
   â±ï¸  Total time: 14523.2s (242.1min / 4.0h)
   ğŸ“Š Avg time per experiment: 3630.8s (60.5min)

ğŸ“ Detailed Results:
No.   Status     Config                                             Duration       
--------------------------------------------------------------------------------
1     âœ… success   exp1_bert_lr3e5                                    3450.2s (57.5min)
2     âœ… success   exp2_bert_lr5e5                                    3512.1s (58.5min)
3     âœ… success   exp3_electra_lr3e5                                 3698.5s (61.6min)
4     âœ… success   exp4_roberta_lr3e5                                 3862.4s (64.4min)
================================================================================

ğŸ‰ ALL EXPERIMENTS COMPLETED SUCCESSFULLY! ğŸ‰
================================================================================
```

### Case 3: ì´ë¯¸ í•™ìŠµëœ ëª¨ë¸ë¡œ ë‹¤ë¥¸ split ì¶”ë¡ 

**Makefile ì‚¬ìš©:**
```bash
# validation setìœ¼ë¡œ ì¶”ë¡  (ê¸°ë³¸)
make inference CONFIG=configs/bert_lr3e5.yaml

# train setìœ¼ë¡œ ì¶”ë¡ í•˜ë ¤ë©´ YAML ìˆ˜ì • í•„ìš”
vim configs/bert_lr3e5.yaml
# inference_split: train ìœ¼ë¡œ ë³€ê²½

make inference CONFIG=configs/bert_lr3e5.yaml
```

**Python ì§ì ‘ ì‹¤í–‰:**
```bash
python run.py --mode inference --config configs/bert_lr3e5.yaml
```

### Case 4: Test set ì œì¶œìš© predictions.json ìƒì„±

```yaml
# YAMLì—ì„œ inference_split ë³€ê²½
inference_split: test
```

**Makefile ì‚¬ìš©:**
```bash
make inference CONFIG=configs/bert_lr3e5.yaml
```

**Python ì§ì ‘ ì‹¤í–‰:**
```bash
python run.py --mode inference --config configs/bert_lr3e5.yaml
```

**ê²°ê³¼:** `./outputs/{username}/{exp_name}/predictions.json` (ì œì¶œìš©)

### Case 5: íŒ€ì›ì˜ ëª¨ë¸ë¡œ inference

```yaml
# íŒ€ì›ì˜ output_dir ì§€ì •
output_dir: ./outputs/seunghwan/bert_experiment

# use_trained_modelì´ trueë©´ í•´ë‹¹ í´ë”ì˜ best checkpoint ìë™ ì‚¬ìš©
use_trained_model: true
inference_split: validation
```

**Makefile ì‚¬ìš©:**
```bash
make inference CONFIG=configs/use_teammate_model.yaml
```

**Python ì§ì ‘ ì‹¤í–‰:**
```bash
python run.py --mode inference --config configs/use_teammate_model.yaml
```

### Case 6: ì‹¤íŒ¨í•œ ì‹¤í—˜ë§Œ ì¬ì‹¤í–‰

Batch ì‹¤í–‰ í›„ ì¼ë¶€ ì‹¤í—˜ì´ ì‹¤íŒ¨í–ˆë‹¤ë©´:

**Makefile ì‚¬ìš©:**
```bash
# ì‹¤íŒ¨í•œ ì‹¤í—˜ë§Œ ê°œë³„ ì‹¤í–‰
make pipeline CONFIG=configs/exp2_bert_lr5e5.yaml
make pipeline CONFIG=configs/exp4_roberta_lr3e5.yaml

# ë˜ëŠ” active í´ë” ì´ìš©
make activate-config CONFIG=configs/exp2_bert_lr5e5.yaml
make activate-config CONFIG=configs/exp4_roberta_lr3e5.yaml
make batch
```

**Python ì§ì ‘ ì‹¤í–‰:**
```bash
# ì‹¤íŒ¨í•œ ì‹¤í—˜ë§Œ ê³¨ë¼ì„œ ì¬ì‹¤í–‰
python run.py --mode pipeline --config configs/exp2_bert_lr5e5.yaml
python run.py --mode pipeline --config configs/exp4_roberta_lr3e5.yaml

# ë˜ëŠ” batchë¡œ ì‹¤íŒ¨í•œ ê²ƒë“¤ë§Œ ëª¨ì•„ì„œ
python run.py --mode batch --batch-mode pipeline \
    --configs configs/exp2_bert_lr5e5.yaml configs/exp4_roberta_lr3e5.yaml
```

### Case 7: GPU ìƒíƒœ í™•ì¸ ë° ì‹¤í—˜ ê²°ê³¼ ë¹„êµ

**Makefile ì‚¬ìš©:**
```bash
# GPU ìƒíƒœ í™•ì¸
make gpu-status

# GPU ìƒíƒœ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ (2ì´ˆë§ˆë‹¤ ê°±ì‹ )
make watch-gpu

# ëª¨ë“  ì‹¤í—˜ ê²°ê³¼ ë¹„êµ (F1, EM ì ìˆ˜)
make compare-results

# ê°€ì¥ ë†’ì€ F1 ì ìˆ˜ë¥¼ ê°€ì§„ ì‹¤í—˜ ì°¾ê¸°
make show-best
```

ì¶œë ¥ ì˜ˆì‹œ:
```
ğŸ† Best experiment (by F1 score):
  Experiment: exp3_electra_lr3e5
  F1 Score: 71.23
  Path: ./outputs/dahyeong/exp3_electra_lr3e5/
```

## ğŸ› ï¸ ë””ë²„ê¹… ê°€ì´ë“œ

### ë¬¸ì œ 1: "No checkpoint found" ì—ëŸ¬

```
FileNotFoundError: No checkpoint found in ./outputs/dahyeong/exp1
```

**ì›ì¸**: í•™ìŠµì´ ì™„ë£Œë˜ì§€ ì•Šì•˜ê±°ë‚˜ checkpoint ì €ì¥ ì‹¤íŒ¨

**í•´ê²°**:
```bash
# 1. ì²´í¬í¬ì¸íŠ¸ ì¡´ì¬ í™•ì¸
ls outputs/dahyeong/exp1/
# best_checkpoint_path.txtê°€ ìˆëŠ”ì§€ í™•ì¸

# 2. ë¡œê·¸ í™•ì¸
tail -n 100 outputs/dahyeong/exp1/*.log

# 3. ì—†ìœ¼ë©´ ì¬í•™ìŠµ
make train-pipeline CONFIG=configs/exp1.yaml
```

---

### ë¬¸ì œ 2: Confidence scoreê°€ ëª¨ë‘ -1.0

**ì›ì¸**: `save_logits=False`ë¡œ ì„¤ì •ë˜ì—ˆê±°ë‚˜ postprocessing ì‹¤íŒ¨

**í•´ê²°**:
```bash
# 1. logits_{split}.json íŒŒì¼ ì¡´ì¬ í™•ì¸
ls outputs/dahyeong/exp1/logits_*.json

# 2. ì—†ìœ¼ë©´ inference ì¬ì‹¤í–‰ (logits ë‹¤ì‹œ ìƒì„±)
make inference CONFIG=configs/exp1.yaml
```

---

### ë¬¸ì œ 3: GPU Out of Memory

```
RuntimeError: CUDA out of memory
```

**í•´ê²°**:
```yaml
# Config YAML ìˆ˜ì •
per_device_train_batch_size: 4   # 8 â†’ 4ë¡œ ê°ì†Œ
gradient_accumulation_steps: 8   # 4 â†’ 8ë¡œ ì¦ê°€
fp16: true                        # Mixed precision í™œì„±í™”
```

ë˜ëŠ”:
```bash
# GPU ë©”ëª¨ë¦¬ ì •ë¦¬
make gpu-status
# ì¢€ë¹„ í”„ë¡œì„¸ìŠ¤ í™•ì¸ í›„ kill
kill -9 <PID>
```

---

### ë¬¸ì œ 4: Batch ì‹¤í–‰ ì¤‘ ì¼ë¶€ë§Œ ì„±ê³µ

**ìƒí™©**: 3ê°œ ì‹¤í—˜ ì¤‘ 2ë²ˆì§¸ë§Œ ì‹¤íŒ¨

**í•´ê²°**:
```bash
# 1. ì‹¤íŒ¨í•œ ì‹¤í—˜ë§Œ ì¬ì‹¤í–‰
make train-pipeline CONFIG=configs/exp2.yaml

# 2. ë˜ëŠ” activeì— ì‹¤íŒ¨í•œ ê²ƒë§Œ ì¶”ê°€
cp configs/exp2.yaml configs/active/
make batch
```

---

### ë¬¸ì œ 5: YAML íŒŒì‹± ì—ëŸ¬

```
yaml.scanner.ScannerError: mapping values are not allowed here
```

**í•´ê²°**:
```bash
# YAML ìœ íš¨ì„± ê²€ì¦
make check-config CONFIG=configs/my_exp.yaml

# ì¼ë°˜ì ì¸ ì˜¤ë¥˜:
# âŒ model_name_or_path:klue/bert-base  (ì½œë¡  ë’¤ ê³µë°± í•„ìˆ˜)
# âœ… model_name_or_path: klue/bert-base

# âŒ ë“¤ì—¬ì“°ê¸° ì˜¤ë¥˜
# âœ… ì¼ê´€ëœ ë“¤ì—¬ì“°ê¸° (space 2ì¹¸ ë˜ëŠ” 4ì¹¸)
```



## í´ë” êµ¬ì¡°

```
MRC/
â”œâ”€â”€ run.py                          # í†µí•© ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ (NEW)
â”œâ”€â”€ train.py                        # í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ inference.py                    # ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸ (ê°œì„ ë¨)
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ base.yaml                   # ê¸°ë³¸ ì„¤ì • í…œí”Œë¦¿ (ì—…ë°ì´íŠ¸ë¨)
â”‚   â””â”€â”€ my_experiment.yaml          # ì‚¬ìš©ì ì‹¤í—˜ ì„¤ì •
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ arguments.py                # Arguments ì •ì˜ (ì—…ë°ì´íŠ¸ë¨)
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ model_loader.py         # ëª¨ë¸ ê²½ë¡œ ìë™ íƒìƒ‰ (NEW)
â”‚       â””â”€â”€ ...
â””â”€â”€ outputs/
    â””â”€â”€ {username}/
        â””â”€â”€ {exp_name}/
            â”œâ”€â”€ checkpoint-123/
            â”œâ”€â”€ checkpoint-247/
            â”œâ”€â”€ best_checkpoint_path.txt   # Best checkpoint ê²½ë¡œ (NEW)
            â”œâ”€â”€ trainer_state.json
            â”œâ”€â”€ config_used.yaml
            â”œâ”€â”€ predictions.json
            â””â”€â”€ eval_results.json
```

## ë§ˆì´ê·¸ë ˆì´ì…˜ ê°€ì´ë“œ (ê¸°ì¡´ ì½”ë“œ â†’ ìƒˆ êµ¬ì¡°)

### ê¸°ì¡´ ë°©ì‹
```bash
# Train
python train.py --output_dir ./outputs/dahyeong/exp1 \
                --model_name_or_path klue/bert-base \
                --do_train

# Inference (ìˆ˜ë™ìœ¼ë¡œ checkpoint ê²½ë¡œ ì§€ì •)
python inference.py --output_dir ./outputs/dahyeong/exp1_infer \
                    --model_name_or_path ./outputs/dahyeong/exp1/checkpoint-247 \
                    --do_predict
```

### ìƒˆ ë°©ì‹
```yaml
# configs/exp1.yaml
model_name_or_path: klue/bert-base
output_dir: ./outputs/dahyeong/exp1
use_trained_model: true
inference_split: validation
```

```bash
# Pipeline (train + inference ìë™)
python run.py --mode pipeline --config configs/exp1.yaml
```

## ì£¼ìš” ë³€ê²½ì‚¬í•­ ìš”ì•½

1. âœ… **í†µí•© YAML ì„¤ì •**: í•˜ë‚˜ì˜ YAMLë¡œ trainê³¼ inference ëª¨ë‘ ê´€ë¦¬
2. âœ… **ìë™ checkpoint íƒìƒ‰**: `use_trained_model=true`ë¡œ best model ìë™ ë¡œë“œ
3. âœ… **ìœ ì—°í•œ ë°ì´í„°ì…‹ ì„ íƒ**: `inference_split`ìœ¼ë¡œ train/validation/test ê°„í¸ ì „í™˜
4. âœ… **Pipeline ëª¨ë“œ**: train â†’ inference í•œ ë²ˆì— ì‹¤í–‰
5. âœ… **Batch ëª¨ë“œ**: ì—¬ëŸ¬ ì‹¤í—˜ ìˆœì°¨ ì‹¤í–‰ìœ¼ë¡œ GPU í™œìš© ê·¹ëŒ€í™”
6. âœ… **do_eval/do_predict ìë™ ì„¤ì •**: splitì— ë”°ë¼ ìë™ ê²°ì •
7. âœ… **ê¸°ì¡´ ë°©ì‹ í˜¸í™˜**: ê¸°ì¡´ CLI ë°©ì‹ë„ ê·¸ëŒ€ë¡œ ì‚¬ìš© ê°€ëŠ¥

---

## Makefile ëª…ë ¹ì–´ ì „ì²´ ëª©ë¡

### ë„ì›€ë§
```bash
make help              # ëª¨ë“  ëª…ë ¹ì–´ì™€ ì„¤ëª… ì¶œë ¥
```

### ë‹¨ì¼ ì‹¤í—˜ ì‹¤í–‰
```bash
make train CONFIG=configs/my_exp.yaml         # Train ëª¨ë“œ
make inference CONFIG=configs/my_exp.yaml     # Inference ëª¨ë“œ
make pipeline CONFIG=configs/my_exp.yaml      # Pipeline ëª¨ë“œ (train â†’ inference)
```

### ë°°ì¹˜ ì‹¤í—˜ ì‹¤í–‰
```bash
make batch                                    # configs/active/*.yaml ëª¨ë‘ ì‹¤í–‰
make batch-custom CONFIGS='file1 file2'       # ì§€ì •í•œ íŒŒì¼ë“¤ë§Œ ì‹¤í–‰
make batch-train                              # Trainë§Œ batch ì‹¤í–‰
make batch-infer                              # Inferenceë§Œ batch ì‹¤í–‰
make batch-stop-on-error                      # ì‹¤íŒ¨ ì‹œ ì¤‘ë‹¨
```

### tmux ì„¸ì…˜ ê´€ë¦¬
```bash
make tmux-start        # tmux ì„¸ì…˜ ì‹œì‘ + batch ì‹¤í–‰
make tmux-attach       # ì‹¤í–‰ ì¤‘ì¸ tmux ì„¸ì…˜ ì¬ì ‘ì†
make tmux-kill         # tmux ì„¸ì…˜ ì¢…ë£Œ
```

### Config ê´€ë¦¬
```bash
make check-config CONFIG=configs/my_exp.yaml  # YAML ìœ íš¨ì„± ê²€ì¦
make list-configs                             # ì‚¬ìš© ê°€ëŠ¥í•œ config ëª©ë¡
make activate-config CONFIG=configs/my.yaml   # Active í´ë”ë¡œ ë³µì‚¬
make deactivate-config NAME=my.yaml           # Activeì—ì„œ ì œê±°
make clear-active                             # Active í´ë” ë¹„ìš°ê¸°
```

### ìœ í‹¸ë¦¬í‹°
```bash
make gpu-status        # GPU ì‚¬ìš© í˜„í™© í™•ì¸
make watch-gpu         # GPU ìƒíƒœ 2ì´ˆë§ˆë‹¤ ìë™ ê°±ì‹ 
make compare-results   # ëª¨ë“  ì‹¤í—˜ ê²°ê³¼ ë¹„êµ (F1, EM)
make show-best         # ê°€ì¥ ë†’ì€ F1 ì ìˆ˜ ì‹¤í—˜ ì¶œë ¥
make clean-outputs     # outputs í´ë” ì „ì²´ ì‚­ì œ
make clean-checkpoints # checkpoint í´ë”ë§Œ ì‚­ì œ
```

### ê°œë°œ ë„êµ¬
```bash
make install           # íŒ¨í‚¤ì§€ ì„¤ì¹˜ (requirements.txt)
make format            # ì½”ë“œ ìë™ í¬ë§·íŒ… (black, isort)
make lint              # ì½”ë“œ í¬ë§· ê²€ì‚¬
make test-config       # í…ŒìŠ¤íŠ¸ìš© config ë¹ ë¥¸ ì‹¤í–‰
```

### ë””ë²„ê¹…
```bash
make debug-train CONFIG=configs/my.yaml       # Train ë””ë²„ê·¸ ëª¨ë“œ
make debug-inference CONFIG=configs/my.yaml   # Inference ë””ë²„ê·¸ ëª¨ë“œ
make tail-log OUTPUT_DIR_PATH=outputs/user/exp1  # ìµœê·¼ ë¡œê·¸ í™•ì¸
```

## ë””ë²„ê¹… & ë¬¸ì œ í•´ê²°

### ëª¨ë¸ ë¡œë”© ë¬¸ì œ

**ì¦ìƒ**: `Model not found` ë˜ëŠ” `best_checkpoint_path.txt not found` ì—ëŸ¬
```
FileNotFoundError: [Errno 2] No such file or directory: './outputs/username/exp1/best_checkpoint_path.txt'
```

**ì›ì¸ & í•´ê²°**:
1. **í•™ìŠµì´ ì™„ë£Œë˜ì§€ ì•ŠìŒ**: `train.py`ê°€ ëê¹Œì§€ ì‹¤í–‰ëëŠ”ì§€ í™•ì¸
   ```bash
   ls -la ./outputs/username/exp1/
   # best_checkpoint_path.txtê°€ ìˆëŠ”ì§€ í™•ì¸
   ```

2. **í•™ìŠµ ì¤‘ ì—ëŸ¬ ë°œìƒ**: ë¡œê·¸ í™•ì¸
   ```bash
   make tail-log OUTPUT_DIR_PATH=./outputs/username/exp1
   # ë˜ëŠ”
   tail -n 50 ./outputs/username/exp1/log.txt
   ```

3. **ìˆ˜ë™ìœ¼ë¡œ checkpoint ì§€ì •**: YAMLì—ì„œ ì§ì ‘ ê²½ë¡œ ëª…ì‹œ
   ```yaml
   use_trained_model: true
   model_name_or_path: ./outputs/username/exp1/checkpoint-247
   ```

### Batch ëª¨ë“œ ë¬¸ì œ

**ì¦ìƒ**: ì¼ë¶€ ì‹¤í—˜ë§Œ ì‹¤í–‰ë˜ê³  ì¤‘ë‹¨ë¨
```
Experiment 3/5: configs/exp3.yaml
Error: CUDA out of memory
```

**í•´ê²°**:
1. **ì‹¤íŒ¨ ì‹œì ë¶€í„° ì¬ì‹¤í–‰**:
   ```bash
   # Makefile ì‚¬ìš©
   make activate-config CONFIG=configs/exp3.yaml
   make activate-config CONFIG=configs/exp4.yaml
   make batch
   
   # ë˜ëŠ” ì§ì ‘ ì‹¤í–‰
   python run.py --mode batch --batch-mode pipeline \
       --configs configs/exp3.yaml configs/exp4.yaml
   ```

2. **ë©”ëª¨ë¦¬ ì„¤ì • ì¡°ì •**:
   ```yaml
   # ì‹¤íŒ¨í•œ ì‹¤í—˜ì˜ YAMLì—ì„œ
   per_device_train_batch_size: 8  # 16 â†’ 8ë¡œ ê°ì†Œ
   gradient_accumulation_steps: 4   # 2 â†’ 4ë¡œ ì¦ê°€ (ë™ì¼í•œ effective batch size ìœ ì§€)
   ```

3. **ì‹¤í—˜ ì‚¬ì´ GPU ë©”ëª¨ë¦¬ ì •ë¦¬**:
   ```bash
   make gpu-status
   # ì¢€ë¹„ í”„ë¡œì„¸ìŠ¤ í™•ì¸ í›„
   kill -9 <PID>
   ```

**ì¦ìƒ**: Batch ì‹¤í–‰ ì¤‘ SSH ì—°ê²° ëŠê¹€ìœ¼ë¡œ ì¤‘ë‹¨
```
client_loop: send disconnect: Broken pipe
```

**í•´ê²°**: tmux ì‚¬ìš©
```bash
# Makefile ì‚¬ìš© (ê¶Œì¥)
make tmux-start      # ìë™ìœ¼ë¡œ batch ì‹œì‘ + tmux ì„¸ì…˜ ìƒì„±

# ë‹¤ì‹œ ì ‘ì† í›„
make tmux-attach

# ë˜ëŠ” ì§ì ‘ tmux ì‚¬ìš©
tmux new -s mrc_batch
make batch
# Ctrl+B, Dë¡œ detach

# ì¬ì ‘ì†
tmux attach -t mrc_batch
```

### ë°ì´í„°ì…‹ ê´€ë ¨ ë¬¸ì œ

**ì¦ìƒ**: `inference_split` ì„¤ì •í–ˆëŠ”ë° ì—‰ëš±í•œ ë°ì´í„°ì…‹ìœ¼ë¡œ ì¶”ë¡ ë¨

**í™•ì¸ì‚¬í•­**:
```yaml
# YAMLì—ì„œ í™•ì¸
inference_split: test  # "test" ë§ëŠ”ì§€ í™•ì¸ (ì˜¤íƒ€ ì£¼ì˜: "tset", "Test" ë“±)
use_trained_model: true  # trueë¡œ ì„¤ì •ëëŠ”ì§€

# ë¡œê·¸ì—ì„œ ì–´ë–¤ ë°ì´í„°ì…‹ì„ ë¡œë“œí–ˆëŠ”ì§€ í™•ì¸
tail -n 100 ./outputs/username/exp1/log.txt | grep "Loading dataset"
```

### YAML ì„¤ì • ë¬¸ì œ

**ì¦ìƒ**: YAML íŒŒì‹± ì—ëŸ¬
```
yaml.scanner.ScannerError: mapping values are not allowed here
```

**í•´ê²°**: YAML ë¬¸ë²• í™•ì¸
```bash
# Makefileë¡œ ìœ íš¨ì„± ê²€ì¦
make check-config CONFIG=configs/my_experiment.yaml
```

```yaml
# âŒ ì˜ëª»ëœ ì˜ˆ
model_name_or_path:klue/bert-base  # ì½œë¡  ë’¤ ê³µë°± í•„ìˆ˜

# âœ… ì˜¬ë°”ë¥¸ ì˜ˆ
model_name_or_path: klue/bert-base

# âŒ ë“¤ì—¬ì“°ê¸° ì˜¤ë¥˜
model_name_or_path: klue/bert-base
 output_dir: ./outputs/exp1  # ì•ì— ë¶ˆí•„ìš”í•œ ê³µë°±

# âœ… ì¼ê´€ëœ ë“¤ì—¬ì“°ê¸°
model_name_or_path: klue/bert-base
output_dir: ./outputs/exp1
```

---

## ë¶€ë¡: ì¶”ê°€ ê¸°ëŠ¥

### Config ìœ íš¨ì„± ê²€ì¦

ì‹¤í—˜ ì‹¤í–‰ ì „ YAML ì„¤ì • ê²€ì¦:
```bash
# Makefile ì‚¬ìš© (ê¶Œì¥)
make check-config CONFIG=configs/my_exp.yaml

# ë˜ëŠ” ì§ì ‘ Python ì‹¤í–‰
python -c "
from transformers import HfArgumentParser
from src.arguments import ModelArguments, DataTrainingArguments, TrainingArguments

parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))
model_args, data_args, training_args = parser.parse_yaml_file('configs/my_exp.yaml')
print('âœ… YAML ì„¤ì • ìœ íš¨')
"
```

### Best Checkpoint ê²½ë¡œ í™•ì¸

```bash
# í•™ìŠµ ì™„ë£Œ í›„ ì–´ë–¤ checkpointê°€ bestì¸ì§€ í™•ì¸
cat ./outputs/username/exp1/best_checkpoint_path.txt
# ì¶œë ¥: ./outputs/username/exp1/checkpoint-247

# ë˜ëŠ” trainer_state.jsonì—ì„œ ì§ì ‘ í™•ì¸
python -c "
import json
with open('./outputs/username/exp1/trainer_state.json') as f:
    state = json.load(f)
    print(state['best_model_checkpoint'])
"
```

### ì‹¤í—˜ ê²°ê³¼ ë¹„êµ

ì—¬ëŸ¬ ì‹¤í—˜ì˜ ê²°ê³¼ í•œëˆˆì— ë¹„êµ:
```bash
# Makefile ì‚¬ìš© (ê¶Œì¥)
make compare-results

# ê°€ì¥ ë†’ì€ F1 ì ìˆ˜ ì‹¤í—˜ ì°¾ê¸°
make show-best
```

ì¶œë ¥ ì˜ˆì‹œ:
```
ğŸ“Š Comparing experiment results:

  exp1_bert                                F1: 68.45    EM: 55.32   
  exp2_roberta                             F1: 71.23    EM: 59.87   
  exp3_electra                             F1: 69.87    EM: 57.45   

ğŸ† Best experiment (by F1 score):
  Experiment: exp2_roberta
  F1 Score: 71.23
  Path: ./outputs/dahyeong/exp2_roberta/
```

---

## â“ FAQ

**Q1: ê°€ì¥ ë¨¼ì € ì‹¤í–‰í•´ì•¼ í•  ëª…ë ¹ì–´ëŠ”?**  
A: `make train-pipeline CONFIG=configs/base.yaml`ë¡œ ê¸°ë³¸ ì‹¤í—˜ë¶€í„° ì‹œì‘í•˜ì„¸ìš”.

**Q2: Confidence scoreê°€ ë‚®ì€ ì˜ˆì¸¡ì€ ì‹ ë¢°í•  ìˆ˜ ì—†ë‚˜ìš”?**  
A: 
- **Low confidence + ì •ë‹µ**: ìš´ìœ¼ë¡œ ë§ì¶˜ ì¼€ì´ìŠ¤ (ë¶ˆì•ˆì •)
- **Low confidence + ì˜¤ë‹µ**: ëª¨ë¸ë„ í—·ê°ˆë¦¬ëŠ” ì–´ë ¤ìš´ ë¬¸ì œ
- **High confidence + ì˜¤ë‹µ**: **ìœ„í—˜!** ì²´ê³„ì  ì˜¤ë¥˜ (ì¬í•™ìŠµ í•„ìš”)

**Q3: Batch ì‹¤í–‰ ì¤‘ SSH ì—°ê²°ì´ ëŠê¸°ë©´?**  
A: ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ ì‹¤í—˜ì€ ê³„ì† ì§„í–‰ë˜ì§€ë§Œ, ë‹¤ìŒ ì‹¤í—˜ì€ ì‹¤í–‰ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì¥ì‹œê°„ ì‹¤í—˜ì€ tmux/screen ì‚¬ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤.

**Q4: GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ì—ëŸ¬ê°€ ë°œìƒí•˜ë©´?**  
```yaml
# Config YAMLì—ì„œ batch size ì¡°ì •
per_device_train_batch_size: 8  # 16 â†’ 8ë¡œ ê°ì†Œ
gradient_accumulation_steps: 4   # 2 â†’ 4ë¡œ ì¦ê°€ (effective batch size ìœ ì§€)
```

**Q5: configs/active/ í´ë”ëŠ” ì™œ í•„ìš”í•œê°€ìš”?**  
A: "ì§€ê¸ˆ ì‹¤í–‰í•˜ê³  ì‹¶ì€ ì‹¤í—˜ë“¤"ì„ ëª¨ì•„ë‘ëŠ” ê³³ì…ë‹ˆë‹¤. `make batch`ëŠ” ì´ í´ë”ì˜ YAMLë§Œ ì‹¤í–‰í•©ë‹ˆë‹¤.

**Q6: Best checkpointëŠ” ì–´ë–»ê²Œ ìë™ íƒìƒ‰ë˜ë‚˜ìš”?**  
A: 
1. `best_checkpoint_path.txt` (train.pyê°€ ìƒì„±)
2. `trainer_state.json`ì˜ `best_model_checkpoint`
3. `checkpoint-*` í´ë” ì¤‘ ê°€ì¥ í° ìˆ«ì

**Q7: ì—¬ëŸ¬ ì‹¤í—˜ ì¤‘ í•˜ë‚˜ë§Œ ì‹¤íŒ¨í•˜ë©´ ì „ì²´ê°€ ì¤‘ë‹¨ë˜ë‚˜ìš”?**  
A: ì•„ë‹™ë‹ˆë‹¤. ê¸°ë³¸ê°’ì€ `continue_on_error=True`ë¡œ ì‹¤íŒ¨í•´ë„ ë‹¤ìŒ ì‹¤í—˜ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.

**Q8: Confidence scoreëŠ” ì–´ë–»ê²Œ ê³„ì‚°ë˜ë‚˜ìš”?**  
A: 
```python
# Start/End logitì˜ softmax í™•ë¥ ì„ í‰ê· 
start_prob = softmax(start_logits)
end_prob = softmax(end_logits)
avg_prob = (max(start_prob) + max(end_prob)) / 2
```

**Q9: íŒ€ì›ì˜ ì‹¤í—˜ ê²°ê³¼ë¥¼ ë‚´ ì„¤ì •ìœ¼ë¡œ ì¬í˜„í•˜ë ¤ë©´?**  
```bash
# íŒ€ì›ì˜ config_used.yaml ë³µì‚¬
cp outputs/teammate/exp5/config_used.yaml configs/reproduce_exp5.yaml

# output_dirë§Œ ë³€ê²½ í›„ ì‹¤í–‰
vim configs/reproduce_exp5.yaml  # output_dir: ./outputs/myname/reproduce_exp5
make train-pipeline CONFIG=configs/reproduce_exp5.yaml
```

**Q10: YAML ì„¤ì •ì´ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í•˜ë ¤ë©´?**  
```bash
make check-config CONFIG=configs/my_exp.yaml
```



---

## ë§ˆì´ê·¸ë ˆì´ì…˜ ê°€ì´ë“œ (ê¸°ì¡´ ì½”ë“œ â†’ ìƒˆ êµ¬ì¡°)

### ê¸°ì¡´ ë°©ì‹
```bash
# Train
python train.py --output_dir ./outputs/dahyeong/exp1 \
                --model_name_or_path klue/bert-base \
                --do_train

# Inference (ìˆ˜ë™ìœ¼ë¡œ checkpoint ê²½ë¡œ ì§€ì •)
python inference.py --output_dir ./outputs/dahyeong/exp1_infer \
                    --model_name_or_path ./outputs/dahyeong/exp1/checkpoint-247 \
                    --do_predict
```

### ìƒˆ ë°©ì‹ (Makefile)
```yaml
# configs/exp1.yaml
model_name_or_path: klue/bert-base
output_dir: ./outputs/dahyeong/exp1
use_trained_model: true
inference_split: validation
```

```bash
# Pipeline (train + inference ìë™)
make pipeline CONFIG=configs/exp1.yaml
```

## ì£¼ìš” ë³€ê²½ì‚¬í•­ ìš”ì•½

1. âœ… **Makefile ì¶”ê°€**: ëª…ë ¹ì–´ ê°„í¸í™” ë° ìœ í‹¸ë¦¬í‹° ì œê³µ
2. âœ… **YAML ê¸°ë°˜ ì„¤ì •**: í•˜ë‚˜ì˜ YAMLë¡œ trainê³¼ inference ëª¨ë‘ ê´€ë¦¬
3. âœ… **ìë™ checkpoint íƒìƒ‰**: `use_trained_model=true`ë¡œ best model ìë™ ë¡œë“œ
4. âœ… **ìœ ì—°í•œ ë°ì´í„°ì…‹ ì„ íƒ**: `inference_split`ìœ¼ë¡œ train/validation/test ê°„í¸ ì „í™˜
5. âœ… **Pipeline ëª¨ë“œ**: train â†’ inference í•œ ë²ˆì— ì‹¤í–‰
6. âœ… **Batch ëª¨ë“œ**: ì—¬ëŸ¬ ì‹¤í—˜ ìˆœì°¨ ì‹¤í–‰ìœ¼ë¡œ GPU í™œìš© ê·¹ëŒ€í™”
7. âœ… **tmux í†µí•©**: ì¥ì‹œê°„ ì‹¤í—˜ì„ ì•ˆì „í•˜ê²Œ ì‹¤í–‰
8. âœ… **ê²°ê³¼ ë¹„êµ ë„êµ¬**: F1, EM ì ìˆ˜ ìë™ ë¹„êµ
9. âœ… **do_eval/do_predict ìë™ ì„¤ì •**: splitì— ë”°ë¼ ìë™ ê²°ì •
10. âœ… **ê¸°ì¡´ ë°©ì‹ í˜¸í™˜**: ê¸°ì¡´ CLI ë°©ì‹ë„ ê·¸ëŒ€ë¡œ ì‚¬ìš© ê°€ëŠ¥

---

## ğŸ“š ì¶”ê°€ ìë£Œ

### Confidence Score í™œìš© ì „ëµ

#### 1. ì˜ˆì¸¡ í•„í„°ë§ ì „ëµ
```python
import pandas as pd

df = pd.read_csv('outputs/dahyeong/my_exp/val_confidence.csv')

# ì „ëµ 1: High confidenceë§Œ ì œì¶œ (ì •ë°€ë„ ìš°ì„ )
high_conf = df[df['avg_prob'] > 0.9]
print(f"High confidence: {len(high_conf)} examples ({len(high_conf)/len(df)*100:.1f}%)")

# ì „ëµ 2: Low confidence ì¬ê²€í† 
low_conf = df[df['avg_prob'] < 0.3]
print(f"Need review: {len(low_conf)} examples")
```

#### 2. ëª¨ë¸ ì•™ìƒë¸” ì‹œ ê°€ì¤‘ì¹˜
```python
# Confidenceë¥¼ ê°€ì¤‘ì¹˜ë¡œ ì‚¬ìš©
ensemble_pred = (
    model1_pred * model1_confidence +
    model2_pred * model2_confidence
) / (model1_confidence + model2_confidence)
```

#### 3. ëŠ¥ë™ í•™ìŠµ (Active Learning)
```python
# Low confidence ì¼€ì´ìŠ¤ë§Œ ì¶”ê°€ ë¼ë²¨ë§
unlabeled_df = df[df['avg_prob'] < 0.5]
print(f"Need labeling: {len(unlabeled_df)} examples")
# â†’ Human annotation â†’ Retrain
```

---

### ì„±ëŠ¥ ê°œì„  ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] **Hyperparameter Tuning**
  - Learning rate: [2e-5, 3e-5, 5e-5]
  - Batch size ì¡°ì •
  - Epochs: 2-5 ë²”ìœ„

- [ ] **Data Augmentation**
  - Back-translation
  - Synonym replacement
  - Context paraphrasing

- [ ] **Model Selection**
  - KoELECTRA vs RoBERTa vs BERT
  - Large vs Base ë¹„êµ

- [ ] **Retrieval ê°œì„ **
  - BM25 â†’ Dense retrieval (DPR, ColBERT)
  - Top-K ì¡°ì • (30 vs 50 vs 100)
  - Reranking ì¶”ê°€

- [ ] **Post-processing**
  - ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
  - íŠ¹ìˆ˜ë¬¸ì ì •ê·œí™”
  - Entity ë³´ì •

- [ ] **Confidence ê¸°ë°˜ í•„í„°ë§**
  - High confidence ì˜¤ë‹µ ì¬í•™ìŠµ
  - Low confidence ë°ì´í„° ì¶”ê°€ ë¼ë²¨ë§

---

### ìœ ìš©í•œ ìŠ¤í¬ë¦½íŠ¸ ëª¨ìŒ

#### ì˜¤ë‹µ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸
```bash
# scripts/analyze_errors.py ìƒì„±
cat > scripts/analyze_errors.py << 'EOF'
import pandas as pd
import json

def analyze_errors(output_dir):
    # Load confidence
    conf_df = pd.read_csv(f'{output_dir}/val_confidence.csv')
    
    # Load detailed results
    with open(f'{output_dir}/val_detailed_results.json') as f:
        details = {item['id']: item for item in json.load(f)}
    
    errors = conf_df[conf_df['is_correct'] == 0]
    
    print(f"Total errors: {len(errors)}")
    print(f"\nError breakdown:")
    print(f"  Low confidence (<0.5): {len(errors[errors['avg_prob'] < 0.5])}")
    print(f"  Medium confidence (0.5-0.8): {len(errors[(errors['avg_prob'] >= 0.5) & (errors['avg_prob'] < 0.8)])}")
    print(f"  High confidence (>0.8): {len(errors[errors['avg_prob'] >= 0.8])}")
    
    # High confidence errors (systematic errors)
    high_conf_errors = errors[errors['avg_prob'] > 0.8]
    print(f"\nâš ï¸  {len(high_conf_errors)} high confidence errors (need investigation):")
    
    for idx, row in high_conf_errors.head(5).iterrows():
        detail = details[row['id']]
        print(f"\nID: {row['id']}")
        print(f"Question: {detail['question']}")
        print(f"Prediction: {detail['prediction']} (conf: {row['avg_prob']:.3f})")
        print(f"Ground Truth: {detail['ground_truth']}")

if __name__ == '__main__':
    import sys
    analyze_errors(sys.argv[1])
EOF

# ì‹¤í–‰
python scripts/analyze_errors.py outputs/dahyeong/my_exp
```

#### ì‹¤í—˜ ë¹„êµ ìŠ¤í¬ë¦½íŠ¸
```bash
# scripts/compare_experiments.py
cat > scripts/compare_experiments.py << 'EOF'
import json
import os
from pathlib import Path

def compare_experiments(base_dir='outputs/dahyeong'):
    results = []
    
    for exp_dir in Path(base_dir).iterdir():
        if not exp_dir.is_dir():
            continue
        
        eval_file = exp_dir / 'eval_results.json'
        if not eval_file.exists():
            continue
        
        with open(eval_file) as f:
            metrics = json.load(f)
        
        results.append({
            'name': exp_dir.name,
            'em': metrics.get('eval_exact_match', 0),
            'f1': metrics.get('eval_f1', 0)
        })
    
    # Sort by F1
    results.sort(key=lambda x: x['f1'], reverse=True)
    
    print("Experiment Comparison (sorted by F1):")
    print(f"{'Rank':<5} {'Experiment':<50} {'EM':<8} {'F1':<8}")
    print("-" * 75)
    
    for rank, res in enumerate(results, 1):
        print(f"{rank:<5} {res['name']:<50} {res['em']:<8.2f} {res['f1']:<8.2f}")

if __name__ == '__main__':
    compare_experiments()
EOF

# ì‹¤í–‰
python scripts/compare_experiments.py
```

---

### ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬

```bash
# ì˜¤ë˜ëœ ì²´í¬í¬ì¸íŠ¸ ì‚­ì œ (bestë§Œ ìœ ì§€)
make clean-checkpoints

# ë””ìŠ¤í¬ ìš©ëŸ‰ í™•ì¸
du -sh outputs/dahyeong/*/

# íŠ¹ì • ì‹¤í—˜ë§Œ ì‚­ì œ
rm -rf outputs/dahyeong/failed_exp/
```

---

## ğŸ“ Best Practices

1. **ì‹¤í—˜ ëª…ëª… ê·œì¹™**
   ```
   {model}_{hyper}_{date}
   ì˜ˆ: koelectra_lr3e5_1208
       roberta_bs16_1208
   ```

2. **Config ë²„ì „ ê´€ë¦¬**
   - `configs/` í´ë”ëŠ” Git ì»¤ë°‹
   - `configs/active/` í´ë”ëŠ” .gitignore (ì„ì‹œ ì‘ì—… ê³µê°„)

3. **ê²°ê³¼ ë°±ì—…**
   ```bash
   # ì¤‘ìš”í•œ ì‹¤í—˜ ê²°ê³¼ ë°±ì—…
   tar -czf experiments_backup_1208.tar.gz outputs/dahyeong/
   ```

4. **GPU ì—í‹°ì¼“**
   - í•™ìŠµ ì „ `make gpu-status` í™•ì¸
   - ì¥ì‹œê°„ ì‹¤í—˜ì€ ë°¤ì— ì‹¤í–‰
   - ì™„ë£Œ í›„ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ í™•ì¸

5. **ë¬¸ì„œí™”**
   - ê° ì‹¤í—˜ í›„ `epoch_summary.md` í™•ì¸
   - ì¤‘ìš”í•œ ë°œê²¬ì‚¬í•­ì€ ë…¸íŠ¸ ê¸°ë¡
   - Best modelì˜ `config_used.yaml` ë°±ì—…

---

## ğŸ“ ë¬¸ì œ í•´ê²°

### ë„ì›€ì´ í•„ìš”í•  ë•Œ

1. **ë¡œê·¸ í™•ì¸**
   ```bash
   tail -n 100 outputs/dahyeong/my_exp/*.log
   ```

2. **GPU ìƒíƒœ í™•ì¸**
   ```bash
   make gpu-status
   nvidia-smi
   ```

3. **Config ê²€ì¦**
   ```bash
   make check-config CONFIG=configs/my_exp.yaml
   ```

4. **Issue ìƒì„±**
   - ì—ëŸ¬ ë©”ì‹œì§€ ì „ì²´ ë³µì‚¬
   - ì‚¬ìš©í•œ ëª…ë ¹ì–´ ê¸°ë¡
   - í™˜ê²½ ì •ë³´ (GPU, Python ë²„ì „)

---

**ì´ ê°€ì´ë“œê°€ ë„ì›€ì´ ë˜ì…¨ë‚˜ìš”? ì¶”ê°€ ì§ˆë¬¸ì€ ì–¸ì œë“  í™˜ì˜í•©ë‹ˆë‹¤! ğŸ‰**
