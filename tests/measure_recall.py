#!/usr/bin/env python
# -*- coding: utf-8 -*-
# TODO: gold context ìœ„ì¹˜ ìˆœìœ„, ê° ìƒ˜í”Œ ë½‘ìœ¼ë©´ì„œ context bm25, dense ì ìˆ˜ í‘œê¸°, context ì ìˆ˜ ìˆœ ë‚˜ì—´ (retrievalê°€ ê°€ì ¸ê°€ëŠ” ìˆœì„œ ë“±)
"""
Retrieval Recall@k ì¸¡ì • ìŠ¤í¬ë¦½íŠ¸

Validation setì„ ê¸°ì¤€ìœ¼ë¡œ ë‹¤ì–‘í•œ alpha ê°’ì—ì„œ Recall@kë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤.

Usage:
    python -m tests.measure_recall
    python -m tests.measure_recall --alphas 0.3 0.5 0.7
    python -m tests.measure_recall --save_results logs/recall_results.json

Output:
    - Recall@1, @5, @10, @20, @50 for each alpha
    - Best alpha recommendation
    - Detailed per-query analysis (optional)
"""

import argparse
import json
import os
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import numpy as np
from datasets import load_from_disk

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
ROOT_DIR = Path(__file__).parent.parent
sys.path.insert(0, str(ROOT_DIR))

from src.retrieval.paths import get_path


def load_cache(cache_path: str) -> Dict[str, Dict]:
    """ìºì‹œ íŒŒì¼ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë¡œë“œ."""
    cache = {}
    with open(cache_path, "r", encoding="utf-8") as f:
        for line in f:
            item = json.loads(line.strip())
            cache[item["id"]] = item
    return cache


def build_text_to_doc_id_mapping(wiki_path: str) -> Dict[str, int]:
    """
    ë¬¸ì„œ í…ìŠ¤íŠ¸ â†’ doc_id ë§¤í•‘ ìƒì„±.

    âš ï¸ ì¤‘ìš”: ë™ì¼ í…ìŠ¤íŠ¸ê°€ ì—¬ëŸ¬ doc_idì— ì¡´ìž¬í•  ìˆ˜ ìžˆìŒ.
    KURE ìž„ë² ë”©ì€ "ì²« ë²ˆì§¸" doc_idë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ, ì—¬ê¸°ì„œë„ ì²« ë²ˆì§¸ë¥¼ ì‚¬ìš©.
    """
    with open(wiki_path, "r", encoding="utf-8") as f:
        wiki = json.load(f)

    text_to_doc_id = {}
    for doc_id, doc in wiki.items():
        text = doc["text"]
        # ì²« ë²ˆì§¸ ë“±ìž¥í•œ doc_idë§Œ ì €ìž¥ (KURE ìž„ë² ë”©ê³¼ ì¼ê´€ì„± ìœ ì§€)
        if text not in text_to_doc_id:
            text_to_doc_id[text] = int(doc_id)

    return text_to_doc_id


def compute_hybrid_scores(
    candidates: List[Dict], alpha: float, eps: float = 1e-9
) -> List[Dict]:
    """
    Raw scoreë¡œë¶€í„° hybrid score ê³„ì‚° ë° ì •ë ¬.

    Args:
        candidates: retrieval í›„ë³´ ë¦¬ìŠ¤íŠ¸ (score_bm25, score_dense í¬í•¨)
        alpha: BM25 ê°€ì¤‘ì¹˜ (0-1, 1ì´ë©´ BM25ë§Œ)
        eps: 0 ë‚˜ëˆ„ê¸° ë°©ì§€

    Returns:
        hybrid_scoreê°€ ì¶”ê°€ë˜ê³  ì •ë ¬ëœ í›„ë³´ ë¦¬ìŠ¤íŠ¸
    """
    if not candidates:
        return candidates

    bm25_scores = np.array([c["score_bm25"] for c in candidates])
    dense_scores = np.array([c["score_dense"] for c in candidates])

    # Per-query min-max normalization
    bm25_n = (bm25_scores - bm25_scores.min()) / (
        bm25_scores.max() - bm25_scores.min() + eps
    )
    dense_n = (dense_scores - dense_scores.min()) / (
        dense_scores.max() - dense_scores.min() + eps
    )

    # Weighted combination
    hybrid_scores = alpha * bm25_n + (1 - alpha) * dense_n

    # ì •ë ¬ ì¸ë±ìŠ¤
    sorted_indices = np.argsort(-hybrid_scores)

    sorted_candidates = []
    for idx in sorted_indices:
        cand = candidates[idx].copy()
        cand["hybrid_score"] = float(hybrid_scores[idx])
        sorted_candidates.append(cand)

    return sorted_candidates


def compute_recall_at_k(
    val_data,
    cache: Dict[str, Dict],
    text_to_doc_id: Dict[str, int],
    alpha: float,
    k_list: List[int] = [1, 5, 10, 20, 50],
) -> Tuple[Dict[int, float], int, List[Dict]]:
    """
    Recall@k ê³„ì‚°.

    Args:
        val_data: validation dataset
        cache: retrieval ìºì‹œ {id -> {question, retrieved}}
        text_to_doc_id: context -> doc_id ë§¤í•‘
        alpha: BM25 ê°€ì¤‘ì¹˜
        k_list: ì¸¡ì •í•  k ê°’ ëª©ë¡

    Returns:
        (recall_dict, total_count, per_query_results)
    """
    hits = {k: 0 for k in k_list}
    total = 0
    per_query_results = []

    for example in val_data:
        qid = example["id"]
        gold_context = example["context"]
        question = example["question"]

        # Gold doc_id ì°¾ê¸°
        gold_doc_id = text_to_doc_id.get(gold_context)
        if gold_doc_id is None:
            continue

        if qid not in cache:
            continue

        candidates = cache[qid]["retrieved"]
        sorted_candidates = compute_hybrid_scores(candidates, alpha)

        total += 1

        # ê° kì—ì„œ hit í™•ì¸
        query_result = {
            "id": qid,
            "question": question[:100],
            "gold_doc_id": gold_doc_id,
            "hits": {},
            "top_candidates": [],
        }

        for k in k_list:
            top_k_doc_ids = [c["doc_id"] for c in sorted_candidates[:k]]
            if gold_doc_id in top_k_doc_ids:
                hits[k] += 1
                query_result["hits"][k] = True
            else:
                query_result["hits"][k] = False

        # Top-5 í›„ë³´ ì €ìž¥
        for c in sorted_candidates[:5]:
            query_result["top_candidates"].append(
                {
                    "doc_id": c["doc_id"],
                    "passage_id": c["passage_id"],
                    "hybrid_score": c.get("hybrid_score", 0),
                    "is_gold": c["doc_id"] == gold_doc_id,
                }
            )

        per_query_results.append(query_result)

    recall = {k: hits[k] / total * 100 if total > 0 else 0 for k in k_list}

    return recall, total, per_query_results


def run_recall_measurement(
    alphas: List[float] = None,
    dataset_path: str = None,
    cache_path: str = None,
    wiki_path: str = None,
    k_list: List[int] = None,
    verbose: bool = True,
    save_per_query: bool = False,
) -> Dict:
    """
    ì—¬ëŸ¬ alphaì— ëŒ€í•´ Recall@k ì¸¡ì •.

    Returns:
        ì¢…í•© ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    if alphas is None:
        alphas = [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]

    if k_list is None:
        k_list = [1, 5, 10, 20, 50]

    # ê²½ë¡œ ì„¤ì •
    if dataset_path is None:
        dataset_path = get_path("train_dataset")
    if cache_path is None:
        cache_path = os.path.join(get_path("retrieval_cache_dir"), "val_top50.jsonl")
    if wiki_path is None:
        wiki_path = get_path("wiki_corpus")

    results = {
        "timestamp": datetime.now().isoformat(),
        "config": {
            "dataset_path": dataset_path,
            "cache_path": cache_path,
            "wiki_path": wiki_path,
            "alphas": alphas,
            "k_list": k_list,
        },
        "results": {},
        "best_alpha": None,
        "total_samples": 0,
    }

    if verbose:
        print("=" * 70)
        print("ðŸ“Š Retrieval Recall@k Measurement")
        print("=" * 70)
        print(f"Dataset: {dataset_path}")
        print(f"Cache: {cache_path}")
        print(f"Alphas: {alphas}")
        print(f"K values: {k_list}")

    # ë°ì´í„° ë¡œë“œ
    if verbose:
        print("\n[1/3] Loading data...")

    dataset = load_from_disk(dataset_path)
    val_data = dataset["validation"]

    cache = load_cache(cache_path)
    text_to_doc_id = build_text_to_doc_id_mapping(wiki_path)

    if verbose:
        print(f"      Validation samples: {len(val_data)}")
        print(f"      Cache entries: {len(cache)}")

    # ê° alphaì— ëŒ€í•´ ì¸¡ì •
    if verbose:
        print("\n[2/3] Measuring Recall@k...")
        print("-" * 70)
        header = "Alpha  |  " + "  ".join([f"R@{k:2d}" for k in k_list])
        print(header)
        print("-" * 70)

    best_recall_10 = -1
    best_alpha = None

    for alpha in alphas:
        recall, total, per_query = compute_recall_at_k(
            val_data, cache, text_to_doc_id, alpha, k_list
        )

        results["results"][alpha] = {
            "recall": recall,
            "total_samples": total,
        }

        if save_per_query:
            results["results"][alpha]["per_query"] = per_query

        results["total_samples"] = total

        # Best alpha ì¶”ì  (R@10 ê¸°ì¤€)
        if recall.get(10, 0) > best_recall_10:
            best_recall_10 = recall.get(10, 0)
            best_alpha = alpha

        if verbose:
            recall_str = "  ".join([f"{recall[k]:5.1f}%" for k in k_list])
            print(f" {alpha:.2f}  |  {recall_str}")

    results["best_alpha"] = {
        "alpha": best_alpha,
        "recall_at_10": best_recall_10,
    }

    if verbose:
        print("-" * 70)
        print(f"\n[3/3] Best Alpha: {best_alpha} (R@10 = {best_recall_10:.1f}%)")
        print("\n" + "=" * 70)
        print("âœ… Recall Measurement Complete")
        print("=" * 70)

    return results


def main():
    parser = argparse.ArgumentParser(description="Measure Retrieval Recall@k")
    parser.add_argument(
        "--alphas",
        nargs="+",
        type=float,
        default=None,
        help="Alpha values to test (default: 0.3 to 1.0)",
    )
    parser.add_argument(
        "--k_list",
        nargs="+",
        type=int,
        default=None,
        help="K values for Recall@k (default: 1 5 10 20 50)",
    )
    parser.add_argument(
        "--dataset_path",
        type=str,
        default=None,
        help="Dataset path",
    )
    parser.add_argument(
        "--cache_path",
        type=str,
        default=None,
        help="Cache file path",
    )
    parser.add_argument(
        "--wiki_path",
        type=str,
        default=None,
        help="Wikipedia JSON path",
    )
    parser.add_argument(
        "--save_results",
        type=str,
        default=None,
        help="Save results to JSON file",
    )
    parser.add_argument(
        "--save_per_query",
        action="store_true",
        help="Include per-query results in output",
    )
    parser.add_argument(
        "--quiet",
        action="store_true",
        help="Suppress verbose output",
    )

    args = parser.parse_args()

    results = run_recall_measurement(
        alphas=args.alphas,
        dataset_path=args.dataset_path,
        cache_path=args.cache_path,
        wiki_path=args.wiki_path,
        k_list=args.k_list,
        verbose=not args.quiet,
        save_per_query=args.save_per_query,
    )

    if args.save_results:
        os.makedirs(os.path.dirname(args.save_results) or ".", exist_ok=True)
        with open(args.save_results, "w", encoding="utf-8") as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        print(f"\nResults saved to {args.save_results}")

    return 0


if __name__ == "__main__":
    sys.exit(main())
