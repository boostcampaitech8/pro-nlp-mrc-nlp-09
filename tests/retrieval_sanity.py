# tests/test_retrieval_sanity.py

"""
Retrieval ëª¨ë“ˆ(Sparse / Dense)ì— ëŒ€í•œ sanity check & ìµœì†Œ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸.

- SparseRetrieval:
  - build()ë¡œ TF-IDF embedding ë¡œë“œ/ìƒì„±
  - train+validì—ì„œ num_samplesê°œ ìƒ˜í”Œ ë½‘ì•„ì„œ retrieve()
  - original_contextê°€ top-k ì•ˆì— ë“¤ì–´ìˆëŠ” ë¹„ìœ¨(hit@k) ì¶œë ¥

- DenseRetrieval:
  - build()ë¡œ corpus dense embedding ë¡œë“œ/ìƒì„±
  - ê°™ì€ ë°©ì‹ìœ¼ë¡œ retrieve() + hit@k ì¶œë ¥
"""

import argparse
import os
import sys
from typing import List, Tuple

import numpy as np
import pandas as pd
from datasets import Dataset, concatenate_datasets, load_from_disk

# í”„ë¡œì íŠ¸ ë£¨íŠ¸(src ìƒìœ„)ë¥¼ PYTHONPATHì— ì¶”ê°€
ROOT_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if ROOT_DIR not in sys.path:
    sys.path.append(ROOT_DIR)

from src.retrieval import SparseRetrieval, DenseRetrieval  # noqa
from transformers import AutoTokenizer  # noqa


def compute_hit_at_k_by_index(
    dataset: Dataset,
    retriever,
    doc_indices: List[List[int]],
) -> float:
    """
    ì›ë³¸ contextê°€ retrieved top-k ë¬¸ì„œ ì¸ë±ìŠ¤ ì•ˆì— ìˆëŠ”ì§€ í™•ì¸.
    ë¬¸ì„œ ë‹¨ìœ„ ì •í™•í•œ ë§¤ì¹­ìœ¼ë¡œ hit@k ê³„ì‚°.

    Args:
        dataset: original contextë¥¼ í¬í•¨í•œ ë°ì´í„°ì…‹
        retriever: contexts ë¦¬ìŠ¤íŠ¸ë¥¼ ê°€ì§„ retriever ì¸ìŠ¤í„´ìŠ¤
        doc_indices: ê° queryë³„ top-k ë¬¸ì„œ ì¸ë±ìŠ¤ [[idx1, idx2, ...], ...]

    Returns:
        hit@k ë¹„ìœ¨ (0.0 ~ 1.0)
    """
    if "context" not in dataset.column_names:
        print("âš ï¸  context ì»¬ëŸ¼ì´ ì—†ì–´ hit@kë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return float("nan")

    # context -> index ë§¤í•‘ (contextsëŠ” ìœ ë‹ˆí¬í•˜ë‹¤ëŠ” ì „ì œ)
    ctx2idx = {ctx: i for i, ctx in enumerate(retriever.contexts)}

    hits = 0
    total = len(dataset)

    for i, ex in enumerate(dataset):
        orig_ctx = ex["context"]
        orig_idx = ctx2idx.get(orig_ctx, None)

        if orig_idx is None:
            # ì›ë³¸ contextê°€ corpusì— ì—†ëŠ” ê²½ìš° (missë¡œ ì²˜ë¦¬)
            continue

        if orig_idx in doc_indices[i]:
            hits += 1

    return hits / total if total > 0 else float("nan")


def compute_recall_precision_at_k(
    dataset: Dataset,
    retriever,
    doc_indices: List[List[int]],
    verbose: bool = False,
) -> dict:
    """
    Document ID ê¸°ë°˜ìœ¼ë¡œ Recall@kì™€ Precision@k ê³„ì‚°.

    ëª©ì : Questionì„ ë˜ì¡Œì„ ë•Œ, ì •ë‹µì´ ìˆëŠ” Documentë¥¼ Top-K ì•ˆì—ì„œ ì°¾ì•˜ëŠ”ì§€ í™•ì¸

    Args:
        dataset: document_id(ì •ë‹µ ë¬¸ì„œ ID)ë¥¼ í¬í•¨í•œ ë°ì´í„°ì…‹
        retriever: contextsì™€ ids(ê° ë¬¸ì„œì˜ document_id)ë¥¼ ê°€ì§„ retriever
        doc_indices: ê° queryë³„ top-k ë¬¸ì„œ ì¸ë±ìŠ¤ [[idx1, idx2, ...], ...]
        verbose: ë””ë²„ê¹… ì •ë³´ ì¶œë ¥ ì—¬ë¶€

    Returns:
        {
            'recall_at_k': ì •ë‹µ ë¬¸ì„œë¥¼ ì°¾ì€ ë¹„ìœ¨ (0.0~1.0),
            'exact_match_count': ì •ë‹µ ë¬¸ì„œë¥¼ ì°¾ì€ ê°œìˆ˜,
            'total_samples': ì „ì²´ ìƒ˜í”Œ ìˆ˜
        }
    """
    if "document_id" not in dataset.column_names:
        print("âš ï¸  document_id ì»¬ëŸ¼ì´ ì—†ì–´ recall/precisionì„ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return {
            "recall_at_k": float("nan"),
            "exact_match_count": 0,
            "total_samples": len(dataset),
        }

    total = len(dataset)
    exact_matches = 0

    # ë””ë²„ê¹…ìš©: ì²« 3ê°œ ìƒ˜í”Œë§Œ ìƒì„¸ ì¶œë ¥
    if verbose and total > 0:
        print("\n[ê²€ìƒ‰ ê²°ê³¼ ì˜ˆì‹œ] ìƒìœ„ 3ê°œ ìƒ˜í”Œ")
        print("=" * 100)

    for i, ex in enumerate(dataset):
        gold_doc_id = ex["document_id"]
        gold_title = ex.get("title", "")

        # retrieved top-k ë¬¸ì„œë“¤ì˜ document_id ì¶”ì¶œ
        retrieved_doc_ids = [retriever.ids[idx] for idx in doc_indices[i]]

        # ì •ë‹µ document_idê°€ retrieved ì•ˆì— ìˆëŠ”ì§€ í™•ì¸
        is_match = gold_doc_id in retrieved_doc_ids
        if is_match:
            exact_matches += 1

        # ë””ë²„ê¹… ì¶œë ¥ - titleê³¼ context preview í¬í•¨
        if verbose and i < 3:
            print(f"\nìƒ˜í”Œ #{i + 1} {'âœ… HIT' if is_match else 'âŒ MISS'}")
            print(f"Question: {ex['question'][:80]}...")
            print(f"ì •ë‹µ ë¬¸ì„œ: [ID:{gold_doc_id}] {gold_title}")

            # Top-3 ê²€ìƒ‰ ê²°ê³¼ ì¶œë ¥
            print(f"ê²€ìƒ‰ ê²°ê³¼ (Top-{min(3, len(doc_indices[i]))}):")
            for rank, idx in enumerate(doc_indices[i][:3], 1):
                doc_id = retriever.ids[idx]
                title = retriever.titles[idx] if hasattr(retriever, "titles") else ""
                # Context ì›ë³¸ ê·¸ëŒ€ë¡œ ì¶œë ¥ (ë…¸ì´ì¦ˆ í™•ì¸ ê°€ëŠ¥)
                context_preview = retriever.contexts[idx][:200].replace("\n", " ")
                match_mark = "â­" if doc_id == gold_doc_id else "  "
                print(f"  {match_mark} {rank}. [ID:{doc_id}] {title}")
                print(f"      {context_preview}...")

    if verbose and total > 0:
        print("=" * 100)

    recall_at_k = exact_matches / total if total > 0 else 0.0

    return {
        "recall_at_k": recall_at_k,
        "exact_match_count": exact_matches,
        "total_samples": total,
    }


def load_small_eval_dataset(dataset_path: str, num_samples: int) -> Dataset:
    """
    train + validation í•©ì³ì„œ num_samples ê°œ ìƒ˜í”Œë§Œ shuffleí•´ì„œ ì‚¬ìš©.
    """
    ds = load_from_disk(dataset_path)
    full = concatenate_datasets(
        [
            ds["train"].flatten_indices(),
            ds["validation"].flatten_indices(),
        ]
    )
    full = full.shuffle(seed=2024)
    num_samples = min(num_samples, len(full))
    small = full.select(range(num_samples))
    return small


def test_sparse(
    dataset_path: str,
    data_path: str,
    context_path: str,
    num_samples: int,
    topk: int,
    use_faiss: bool,
    show_examples: int = 3,
) -> None:
    print("\n" + "=" * 80)
    print("ğŸ” SPARSE RETRIEVAL SANITY CHECK")
    print("=" * 80)

    small_ds = load_small_eval_dataset(dataset_path, num_samples)
    print(f"ğŸ“Š í‰ê°€ ìƒ˜í”Œ: {len(small_ds)}ê°œ (train+validationì—ì„œ ëœë¤ ì¶”ì¶œ)")

    # tokenizerëŠ” ê·¸ëƒ¥ klue/bert-base ê¸°ì¤€ìœ¼ë¡œ
    tokenizer = AutoTokenizer.from_pretrained("klue/bert-base")

    retriever = SparseRetrieval(
        tokenize_fn=tokenizer.tokenize,
        data_path=data_path,
        context_path=context_path,
    )

    # Base ìŠ¤íƒ€ì¼ build() (ë‚´ë¶€ì—ì„œ get_sparse_embedding í˜¸ì¶œ)
    print("ğŸ“¥ Retriever ì´ˆê¸°í™” ì¤‘...")
    retriever.build()
    print(f"âœ… Sparse p_embedding shape: {retriever.p_embedding.shape}")

    # Dataset ë‹¨ìœ„ retrieve
    df = retriever.retrieve(small_ds, topk=topk)
    print(f"âœ… Retrieve ì™„ë£Œ: {len(df)}ê°œ ìƒ˜í”Œ")

    # hit@k ê³„ì‚° (Context í…ìŠ¤íŠ¸ ë§¤ì¹­ ê¸°ë°˜)
    _, doc_indices = retriever.get_relevant_doc_bulk(small_ds["question"], k=topk)
    hit = compute_hit_at_k_by_index(small_ds, retriever, doc_indices)

    # Recall@k ê³„ì‚° (Document ID ë§¤ì¹­ ê¸°ë°˜) - show_examples ê°œìˆ˜ë§Œí¼ ì¶œë ¥
    metrics = compute_recall_precision_at_k(
        small_ds, retriever, doc_indices, verbose=(show_examples > 0)
    )

    print("\n" + "=" * 80)
    print("ğŸ“Š RETRIEVAL ì„±ëŠ¥ ë©”íŠ¸ë¦­")
    print("=" * 80)
    print(f"í‰ê°€ ìƒ˜í”Œ ìˆ˜: {metrics['total_samples']}ê°œ")
    print(f"Top-K: {topk}")
    print()
    print(f"Context í…ìŠ¤íŠ¸ ë§¤ì¹­:")
    print(
        f"  Hit@{topk}: {hit:.1%}  ({int(hit * metrics['total_samples'])}/{metrics['total_samples']})"
    )
    print(f"  â†’ ì •ë‹µ contextê°€ ê²€ìƒ‰ëœ ë¬¸ì„œ í…ìŠ¤íŠ¸ì— í¬í•¨ëœ ë¹„ìœ¨")
    print()
    print(f"Document ID ë§¤ì¹­:")
    print(
        f"  Recall@{topk}: {metrics['recall_at_k']:.1%}  ({metrics['exact_match_count']}/{metrics['total_samples']})"
    )
    print(f"  â†’ ì •ë‹µ document_idê°€ Top-{topk} ì•ˆì— ìˆëŠ” ë¹„ìœ¨")
    print("=" * 80)


def test_dense(
    dataset_path: str,
    data_path: str,
    context_path: str,
    num_samples: int,
    topk: int,
    dense_model: str,
    dense_embedding_path: str,
) -> None:
    print("\n" + "=" * 80)
    print("ğŸ” DENSE RETRIEVAL SANITY CHECK")
    print("=" * 80)

    small_ds = load_small_eval_dataset(dataset_path, num_samples)

    retriever = DenseRetrieval(
        model_name_or_path=dense_model,
        data_path=data_path,
        context_path=context_path,
        embedding_path=dense_embedding_path,
        max_length=256,
        batch_size=16,
    )

    # ì „ì²´ corpus ê¸°ì¤€ build() (embedding_pathê°€ ìˆìœ¼ë©´ ë¡œë“œ, ì—†ìœ¼ë©´ ê³„ì‚° í›„ ì €ì¥)
    retriever.build()
    print(f"âœ… Dense p_embedding shape: {retriever.p_embedding.shape}")

    # Dataset ë‹¨ìœ„ retrieve
    df = retriever.retrieve(small_ds, topk=topk)
    print("âœ… Dense retrieve() ê²°ê³¼ DataFrame columns:", df.columns.tolist())
    print(df.head(3))

    # hit@k ê³„ì‚° (ë¬¸ì„œ ì¸ë±ìŠ¤ ê¸°ë°˜)
    _, doc_indices = retriever.get_relevant_doc_bulk(small_ds["question"], k=topk)
    hit = compute_hit_at_k_by_index(small_ds, retriever, doc_indices)
    print(f"âœ… Dense hit@{topk} on {len(df)} samples: {hit:.4f}")

    # ë‹¨ì¼ query í…ŒìŠ¤íŠ¸
    q = small_ds[0]["question"]
    print("\n[ì˜ˆì‹œ ì¿¼ë¦¬(Dense)]", q)
    result = retriever.retrieve(q, topk=3)
    scores, contexts = result
    print(f"Top-1 score: {scores[0]:.4f}")
    print("Top-1 passage (ì• 200ì):")
    print(contexts[0][:200].replace("\n", " ") + "...")
    print("=" * 80)


def analyze_full_dataset(
    dataset_path: str,
    data_path: str,
    context_path: str,
    topk_list: List[int] = [1, 5, 10, 20, 50],
    save_log: bool = False,
) -> None:
    """
    ì „ì²´ train+validation ë°ì´í„°ì…‹ìœ¼ë¡œ Sparse Retrievalì˜ recall@k ë¶„ì„.

    ì„±ëŠ¥ ìµœì í™”: k=max(topk_list) í•œ ë²ˆë§Œ ê³„ì‚° í›„ ìŠ¬ë¼ì´ì‹±
    """
    print("\n" + "=" * 80)
    print("ğŸ“Š FULL DATASET ANALYSIS (Train + Validation)")
    print("=" * 80)

    # ì „ì²´ ë°ì´í„°ì…‹ ë¡œë“œ
    ds = load_from_disk(dataset_path)
    train_ds = ds["train"].flatten_indices()
    valid_ds = ds["validation"].flatten_indices()

    print(f"ğŸ“ Train samples: {len(train_ds)}")
    print(f"ğŸ“ Valid samples: {len(valid_ds)}")

    # Tokenizer & Retriever ì´ˆê¸°í™”
    tokenizer = AutoTokenizer.from_pretrained("klue/bert-base")
    retriever = SparseRetrieval(
        tokenize_fn=tokenizer.tokenize,
        data_path=data_path,
        context_path=context_path,
    )
    retriever.build()

    max_k = max(topk_list)

    # ë¡œê·¸ ì €ì¥ìš©
    log_lines = []
    log_lines.append("=" * 80)
    log_lines.append("SPARSE RETRIEVAL ANALYSIS REPORT")
    log_lines.append("=" * 80)
    log_lines.append(f"Date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}")
    log_lines.append(f"Max K: {max_k}")
    log_lines.append("")

    # Train ë¶„ì„ (í•œ ë²ˆë§Œ ê³„ì‚°)
    print(f"\nâ³ Train ë°ì´í„°ì…‹ ë¶„ì„ ì¤‘... (Top-{max_k} ê³„ì‚°)")
    _, train_doc_indices = retriever.get_relevant_doc_bulk(
        train_ds["question"], k=max_k
    )

    print("\n" + "=" * 80)
    print("ğŸ“ˆ Train Dataset (n={})".format(len(train_ds)))
    print("=" * 80)
    print(f"{'Top-K':<8} {'Recall@K':<12} {'Match/Total':<15}")
    print("-" * 80)

    log_lines.append(f"Train Dataset (n={len(train_ds)})")
    log_lines.append("-" * 80)

    for k in topk_list:
        # ì´ë¯¸ ê³„ì‚°ëœ ê²°ê³¼ì—ì„œ ìŠ¬ë¼ì´ì‹±
        sliced_indices = [indices[:k] for indices in train_doc_indices]
        metrics = compute_recall_precision_at_k(train_ds, retriever, sliced_indices)

        result_line = f"{k:<8} {metrics['recall_at_k']:<12.1%} {metrics['exact_match_count']}/{metrics['total_samples']}"
        print(result_line)
        log_lines.append(
            f"  Recall@{k:3d}: {metrics['recall_at_k']:.4f} ({metrics['exact_match_count']}/{metrics['total_samples']})"
        )

    log_lines.append("")

    # Validation ë¶„ì„ (í•œ ë²ˆë§Œ ê³„ì‚°)
    print(f"\nâ³ Validation ë°ì´í„°ì…‹ ë¶„ì„ ì¤‘... (Top-{max_k} ê³„ì‚°)")
    _, valid_doc_indices = retriever.get_relevant_doc_bulk(
        valid_ds["question"], k=max_k
    )

    print("\n" + "=" * 80)
    print("ğŸ“ˆ Validation Dataset (n={})".format(len(valid_ds)))
    print("=" * 80)
    print(f"{'Top-K':<8} {'Recall@K':<12} {'Match/Total':<15}")
    print("-" * 80)

    log_lines.append(f"Validation Dataset (n={len(valid_ds)})")
    log_lines.append("-" * 80)

    for k in topk_list:
        sliced_indices = [indices[:k] for indices in valid_doc_indices]
        metrics = compute_recall_precision_at_k(valid_ds, retriever, sliced_indices)

        result_line = f"{k:<8} {metrics['recall_at_k']:<12.1%} {metrics['exact_match_count']}/{metrics['total_samples']}"
        print(result_line)
        log_lines.append(
            f"  Recall@{k:3d}: {metrics['recall_at_k']:.4f} ({metrics['exact_match_count']}/{metrics['total_samples']})"
        )

    log_lines.append("")

    # ì „ì²´ ë°ì´í„° ë¶„ì„ (í•œ ë²ˆë§Œ ê³„ì‚°)
    full_ds = concatenate_datasets([train_ds, valid_ds])
    print(f"\nâ³ Full ë°ì´í„°ì…‹ ë¶„ì„ ì¤‘... (Top-{max_k} ê³„ì‚°)")
    full_doc_indices = train_doc_indices + valid_doc_indices

    print("\n" + "=" * 80)
    print("ğŸ“ˆ Full Dataset - Train + Valid (n={})".format(len(full_ds)))
    print("=" * 80)
    print(f"{'Top-K':<8} {'Recall@K':<12} {'Match/Total':<15}")
    print("-" * 80)

    log_lines.append(f"Full Dataset (n={len(full_ds)})")
    log_lines.append("-" * 80)

    for k in topk_list:
        sliced_indices = [indices[:k] for indices in full_doc_indices]
        metrics = compute_recall_precision_at_k(full_ds, retriever, sliced_indices)

        result_line = f"{k:<8} {metrics['recall_at_k']:<12.1%} {metrics['exact_match_count']}/{metrics['total_samples']}"
        print(result_line)
        log_lines.append(
            f"  Recall@{k:3d}: {metrics['recall_at_k']:.4f} ({metrics['exact_match_count']}/{metrics['total_samples']})"
        )

    print("=" * 80)
    print("\nğŸ’¡ í•´ì„:")
    print("  - Recall@K: Question ë˜ì¡Œì„ ë•Œ ì •ë‹µ documentê°€ Top-K ì•ˆì— ìˆëŠ” ë¹„ìœ¨")
    print("  - ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ (Retrieverê°€ ì •ë‹µ ë¬¸ì„œë¥¼ ì˜ ì°¾ìŒ)")
    print("  - Kê°€ í´ìˆ˜ë¡ Recallì€ ì¦ê°€ (ë” ë§ì€ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ë¯€ë¡œ)")

    # ë¡œê·¸ ì €ì¥
    if save_log:
        log_lines.append("")
        log_lines.append("=" * 80)
        log_file = f"logs/retrieval_analysis_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.txt"
        os.makedirs("logs", exist_ok=True)
        with open(log_file, "w", encoding="utf-8") as f:
            f.write("\n".join(log_lines))
        print(f"\nğŸ’¾ ë¡œê·¸ ì €ì¥: {log_file}")


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Retrieval sanity check (Sparse / Dense)"
    )

    parser.add_argument(
        "--dataset_path",
        type=str,
        default="./data/train_dataset",
        help="HuggingFace load_from_diskë¡œ ì €ì¥ëœ train_dataset ê²½ë¡œ",
    )
    parser.add_argument(
        "--data_path",
        type=str,
        default="./data",
        help="wikipedia_documents.jsonì´ ìˆëŠ” ê²½ë¡œ",
    )
    parser.add_argument(
        "--context_path",
        type=str,
        default="wikipedia_documents.json",
        help="ìœ„í‚¤ ì½”í¼ìŠ¤ íŒŒì¼ëª…",
    )
    parser.add_argument(
        "--num_samples",
        type=int,
        default=32,
        help="sanity checkìš©ìœ¼ë¡œ ì‚¬ìš©í•  ìƒ˜í”Œ ìˆ˜",
    )
    parser.add_argument(
        "--topk",
        type=int,
        default=5,
        help="retrieval ì‹œ top-k passage ê°œìˆ˜",
    )

    # Sparse / Dense on/off
    parser.add_argument(
        "--sparse",
        type=lambda x: str(x).lower() == "true",
        default=True,
        help="Sparse retrieval í…ŒìŠ¤íŠ¸ ì—¬ë¶€ (default: True)",
    )
    parser.add_argument(
        "--use_faiss",
        action="store_true",
        help="Sparseì—ì„œ faiss indexer ì‚¬ìš© ì—¬ë¶€",
    )
    parser.add_argument(
        "--dense",
        type=lambda x: str(x).lower() == "true",
        default=False,
        help="Dense retrieval í…ŒìŠ¤íŠ¸ ì—¬ë¶€ (default: False)",
    )
    parser.add_argument(
        "--analyze_full",
        action="store_true",
        help="ì „ì²´ train+valid ë°ì´í„°ì…‹ìœ¼ë¡œ recall@k ë¶„ì„ ìˆ˜í–‰",
    )
    parser.add_argument(
        "--save_log",
        action="store_true",
        help="ë¶„ì„ ê²°ê³¼ë¥¼ ë¡œê·¸ íŒŒì¼ë¡œ ì €ì¥",
    )
    parser.add_argument(
        "--show_examples",
        type=int,
        default=0,
        help="Sanity check ì‹œ ì¶œë ¥í•  ì˜ˆì‹œ ê°œìˆ˜ (ê¸°ë³¸: 0)",
    )

    # Dense ì„¤ì •
    parser.add_argument(
        "--dense_model",
        type=str,
        default="upskyy/gte-base-korean",
        help="DenseRetrievalì— ì‚¬ìš©í•  HF embedding ëª¨ë¸ ì´ë¦„",
    )
    parser.add_argument(
        "--dense_embedding_path",
        type=str,
        default=None,
        help="corpus dense embeddingì„ ì €ì¥/ë¡œë”©í•  npy ê²½ë¡œ (Noneì´ë©´ ìë™ ìƒì„±)",
    )

    args = parser.parse_args()

    # sparse/dense ë‘˜ ë‹¤ Falseë©´ ê°•ì œë¡œ sparseë§Œ ì¼œê¸°
    if not args.sparse and not args.dense:
        print("âš ï¸ sparse/dense ëª¨ë‘ Falseë¼ì„œ, sparseë§Œ Trueë¡œ ì„¤ì •í•©ë‹ˆë‹¤.")
        args.sparse = True

    return args


def main() -> None:
    args = parse_args()

    # Dense embedding path ìë™ ìƒì„± ë¡œì§
    if args.dense and args.dense_embedding_path is None:
        model_slug = args.dense_model.replace("/", "_")
        args.dense_embedding_path = f"{args.data_path}/dense_embedding_{model_slug}.npy"
        print(f"ğŸ“Œ Auto-generated dense_embedding_path: {args.dense_embedding_path}")

    print("=== Retrieval Sanity Check Config ===")
    print(f"dataset_path        = {args.dataset_path}")
    print(f"data_path           = {args.data_path}")
    print(f"context_path        = {args.context_path}")
    print(f"num_samples         = {args.num_samples}")
    print(f"topk                = {args.topk}")
    print(f"sparse              = {args.sparse}")
    print(f"use_faiss           = {args.use_faiss}")
    print(f"dense               = {args.dense}")
    print(f"dense_model         = {args.dense_model}")
    print(f"dense_embedding_path= {args.dense_embedding_path}")
    print(f"analyze_full        = {args.analyze_full}")
    print(f"save_log            = {args.save_log}")
    print(f"show_examples       = {args.show_examples}")
    print("=====================================")

    # ì „ì²´ ë°ì´í„°ì…‹ ë¶„ì„ ëª¨ë“œ
    if args.analyze_full:
        analyze_full_dataset(
            dataset_path=args.dataset_path,
            data_path=args.data_path,
            context_path=args.context_path,
            topk_list=[1, 5, 10, 20, 50, 100],
            save_log=args.save_log,
        )
        return
    # Sanity check ëª¨ë“œ
    if args.sparse:
        test_sparse(
            dataset_path=args.dataset_path,
            data_path=args.data_path,
            context_path=args.context_path,
            num_samples=args.num_samples,
            topk=args.topk,
            use_faiss=args.use_faiss,
            show_examples=args.show_examples,
        )

    if args.dense:
        test_dense(
            dataset_path=args.dataset_path,
            data_path=args.data_path,
            context_path=args.context_path,
            num_samples=args.num_samples,
            topk=args.topk,
            dense_model=args.dense_model,
            dense_embedding_path=args.dense_embedding_path,
        )


if __name__ == "__main__":
    main()
