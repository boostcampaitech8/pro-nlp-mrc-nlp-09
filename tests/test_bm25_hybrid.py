"""
BM25 ë° Hybrid Retrieval ë¹ ë¥¸ í…ŒìŠ¤íŠ¸

ì‚¬ìš©ë²•:
    # BM25ë§Œ í…ŒìŠ¤íŠ¸
    python tests/test_bm25_hybrid.py --method bm25

    # Hybrid (BM25 + KoE5) í…ŒìŠ¤íŠ¸
    python tests/test_bm25_hybrid.py --method hybrid --alpha 0.5

    # ì „ì²´ ë¹„êµ (TF-IDF, BM25, KoE5, Hybrid)
    python tests/test_bm25_hybrid.py --method all
"""

import argparse
import os
import sys
import time

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

from datasets import load_from_disk
from transformers import AutoTokenizer

from src.retrieval import BM25Retrieval, KoE5Retrieval, HybridRetrieval, SparseRetrieval


def calculate_recall(dataset, retriever, k=10):
    """Recall@K ê³„ì‚°"""
    queries = dataset["question"]
    gold_contexts = [ex["context"] for ex in dataset]

    print(f"   Retrieving top-{k} documents...")
    _, doc_indices = retriever.get_relevant_doc_bulk(queries, k=k)

    hits = 0
    for gold_ctx, indices in zip(gold_contexts, doc_indices):
        retrieved_contexts = [retriever.contexts[idx] for idx in indices]
        if gold_ctx in retrieved_contexts:
            hits += 1

    recall = hits / len(dataset)
    return recall, hits, len(dataset)


def test_retriever(name, retriever, dataset, topk_list=[1, 5, 10, 20, 50]):
    """íŠ¹ì • retriever í…ŒìŠ¤íŠ¸"""
    print("\n" + "=" * 80)
    print(f"ğŸ” {name}")
    print("=" * 80)

    start_time = time.time()
    retriever.build()
    build_time = time.time() - start_time

    print(f"Build time: {build_time:.1f}s")
    print()
    print(f"{'Top-K':<8} {'Recall@K':<12} {'Match/Total':<15}")
    print("-" * 80)

    # ëª¨ë“  k ê°’ì— ëŒ€í•´ í•œ ë²ˆì— ê³„ì‚° (progress bar í•œ ë²ˆë§Œ)
    max_k = max(topk_list)
    queries = dataset["question"]
    gold_contexts = [ex["context"] for ex in dataset]

    print(f"Retrieving top-{max_k}...", end=" ", flush=True)
    _, doc_indices = retriever.get_relevant_doc_bulk(queries, k=max_k)
    print("âœ“")

    # ê° kì— ëŒ€í•´ recall ê³„ì‚° (slicingìœ¼ë¡œ ë¹ ë¥´ê²Œ)
    for k in topk_list:
        hits = 0
        for gold_ctx, indices in zip(gold_contexts, doc_indices):
            topk_contexts = [retriever.contexts[idx] for idx in indices[:k]]
            if gold_ctx in topk_contexts:
                hits += 1

        recall = hits / len(dataset)
        print(f"{k:<8} {recall:<12.1%} {hits}/{len(dataset)}")

    print("=" * 80)


def main():
    parser = argparse.ArgumentParser(description="BM25 ë° Hybrid Retrieval í…ŒìŠ¤íŠ¸")
    parser.add_argument(
        "--method",
        type=str,
        default="bm25",
        choices=["bm25", "hybrid", "all"],
        help="í…ŒìŠ¤íŠ¸í•  ë°©ë²•",
    )
    parser.add_argument(
        "--alpha",
        type=float,
        default=0.5,
        help="Hybrid alpha ê°’ (BM25 ê°€ì¤‘ì¹˜, DenseëŠ” 1-alpha)",
    )
    parser.add_argument(
        "--fusion",
        type=str,
        default="rrf",
        choices=["rrf", "score"],
        help="Hybrid fusion ë°©ë²•",
    )
    parser.add_argument(
        "--dataset", type=str, default="./data/train_dataset", help="ë°ì´í„°ì…‹ ê²½ë¡œ"
    )
    parser.add_argument(
        "--split",
        type=str,
        default="validation",
        choices=["train", "validation"],
        help="ë°ì´í„°ì…‹ split",
    )

    args = parser.parse_args()

    print("=" * 80)
    print("ğŸš€ BM25 & Hybrid Retrieval Test")
    print("=" * 80)
    print(f"Method: {args.method}")
    print(f"Dataset: {args.dataset} ({args.split} split)")
    if args.method == "hybrid":
        print(f"Alpha: {args.alpha} (BM25:{args.alpha}, Dense:{1 - args.alpha})")
        print(f"Fusion: {args.fusion}")
    print("=" * 80)

    # ë°ì´í„°ì…‹ ë¡œë“œ
    datasets = load_from_disk(args.dataset)
    eval_dataset = datasets[args.split]

    # Tokenizer
    tokenizer = AutoTokenizer.from_pretrained("klue/bert-base")

    # Top-K ë¦¬ìŠ¤íŠ¸
    topk_list = [1, 5, 10, 20, 50]

    if args.method == "bm25":
        retriever = BM25Retrieval(
            tokenize_fn=tokenizer.tokenize,
            data_path="./data",
            context_path="wikipedia_documents.json",
        )
        test_retriever("BM25", retriever, eval_dataset, topk_list)

    elif args.method == "hybrid":
        retriever = HybridRetrieval(
            tokenize_fn=tokenizer.tokenize,
            data_path="./data",
            context_path="wikipedia_documents.json",
            corpus_emb_path="./data/koe5_corpus_emb.npy",
            alpha=args.alpha,
            fusion_method=args.fusion,
        )
        test_retriever(
            f"Hybrid (alpha={args.alpha}, {args.fusion})",
            retriever,
            eval_dataset,
            topk_list,
        )

    else:  # all
        # TF-IDF
        tfidf_retriever = SparseRetrieval(
            tokenize_fn=tokenizer.tokenize,
            data_path="./data",
            context_path="wikipedia_documents.json",
        )
        test_retriever("TF-IDF", tfidf_retriever, eval_dataset, topk_list)

        # BM25
        bm25_retriever = BM25Retrieval(
            tokenize_fn=tokenizer.tokenize,
            data_path="./data",
            context_path="wikipedia_documents.json",
        )
        test_retriever("BM25", bm25_retriever, eval_dataset, topk_list)

        # KoE5
        koe5_retriever = KoE5Retrieval(
            data_path="./data",
            context_path="wikipedia_documents.json",
            corpus_emb_path="./data/koe5_corpus_emb.npy",
        )
        test_retriever("KoE5", koe5_retriever, eval_dataset, topk_list)

        # Hybrid
        hybrid_retriever = HybridRetrieval(
            tokenize_fn=tokenizer.tokenize,
            data_path="./data",
            context_path="wikipedia_documents.json",
            corpus_emb_path="./data/koe5_corpus_emb.npy",
            alpha=0.5,
            fusion_method="rrf",
        )
        test_retriever(
            "Hybrid (alpha=0.5, RRF)", hybrid_retriever, eval_dataset, topk_list
        )

    print("\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")


if __name__ == "__main__":
    main()
