"""
Answer Realignment End-to-End Sanity Check

ì‹¤ì œ ë°ì´í„°ì…‹ê³¼ Retrieval ê²°ê³¼ë¥¼ ì‚¬ìš©í•˜ì—¬ answer realignmentì´
ì˜¬ë°”ë¥´ê²Œ ë™ì‘í•˜ëŠ”ì§€ ê²€ì¦í•©ë‹ˆë‹¤.

ê²€ì¦ í•­ëª©:
1. realign í›„ answer_startê°€ ì‹¤ì œ contextì—ì„œ ì •ë‹µì„ ê°€ë¦¬í‚¤ëŠ”ì§€
2. Tokenization í›„ start_positions/end_positionsê°€ ì •í™•í•œì§€
3. ë””ì½”ë”©ëœ ì •ë‹µì´ ì›ë³¸ ì •ë‹µê³¼ ì¼ì¹˜í•˜ëŠ”ì§€

ì‹¤í–‰:
    python -m tests.sanity_answer_realignment
    python -m tests.sanity_answer_realignment --verbose
    python -m tests.sanity_answer_realignment --num_samples 50
"""

import argparse
import json
import os
import sys
from pathlib import Path
from typing import Dict, List, Tuple, Optional

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from datasets import load_from_disk
from transformers import AutoTokenizer


def print_header(title: str):
    print("\n" + "=" * 70)
    print(f"  {title}")
    print("=" * 70)


def print_result(name: str, passed: bool, details: str = ""):
    status = "âœ… PASS" if passed else "âŒ FAIL"
    print(f"  {status}: {name}")
    if details:
        print(f"         {details}")


class AnswerRealignmentSanityChecker:
    """Answer Realignment ì¢…í•© ê²€ì¦ í´ë˜ìŠ¤"""

    def __init__(
        self,
        tokenizer_name: str = "klue/roberta-large",
        max_seq_length: int = 384,
        doc_stride: int = 128,
        verbose: bool = False,
    ):
        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
        self.max_seq_length = max_seq_length
        self.doc_stride = doc_stride
        self.verbose = verbose

        # ê²°ê³¼ ì €ì¥
        self.results = {
            "total": 0,
            "char_level_correct": 0,
            "token_level_correct": 0,
            "decode_correct": 0,
            "failures": [],
        }

    def check_char_level_alignment(
        self, context: str, answer_text: str, answer_start: int
    ) -> Tuple[bool, str]:
        """
        1ë‹¨ê³„: Character levelì—ì„œ answer_startê°€ ì •í™•í•œì§€ í™•ì¸

        context[answer_start:answer_start+len(answer_text)] == answer_text
        """
        if answer_start < 0:
            return False, f"Invalid answer_start: {answer_start}"

        if answer_start + len(answer_text) > len(context):
            return (
                False,
                f"answer_start({answer_start}) + len({len(answer_text)}) > context_len({len(context)})",
            )

        extracted = context[answer_start : answer_start + len(answer_text)]
        if extracted == answer_text:
            return True, ""
        else:
            return False, f"Mismatch: expected '{answer_text}', got '{extracted}'"

    def check_token_level_alignment(
        self,
        question: str,
        context: str,
        answer_text: str,
        answer_start: int,
    ) -> Tuple[bool, str, Optional[Dict]]:
        """
        2ë‹¨ê³„: Token levelì—ì„œ start_positions/end_positionsê°€ ì •í™•í•œì§€ í™•ì¸

        Tokenization í›„ offset_mappingì„ ì‚¬ìš©í•´ ì •ë‹µ í† í° ìœ„ì¹˜ ê³„ì‚°
        """
        # Tokenization (question + context)
        tokenized = self.tokenizer(
            question,
            context,
            truncation="only_second",
            max_length=self.max_seq_length,
            stride=self.doc_stride,
            return_overflowing_tokens=True,
            return_offsets_mapping=True,
            padding="max_length",
        )

        # ì²« ë²ˆì§¸ spanë§Œ ê²€ì‚¬ (ê°„ë‹¨í™”)
        input_ids = tokenized["input_ids"][0]
        offset_mapping = tokenized["offset_mapping"][0]

        # Answerì˜ character ë²”ìœ„
        answer_end = answer_start + len(answer_text)

        # sequence_idsë¡œ context ì˜ì—­ ì°¾ê¸°
        sequence_ids = tokenized.sequence_ids(0)

        # Context ì‹œì‘/ë í† í° ì¸ë±ìŠ¤ ì°¾ê¸°
        context_start_token = None
        context_end_token = None
        for idx, seq_id in enumerate(sequence_ids):
            if seq_id == 1:  # context
                if context_start_token is None:
                    context_start_token = idx
                context_end_token = idx

        if context_start_token is None:
            return False, "Context not found in tokenization", None

        # ì •ë‹µì´ ì´ spanì— í¬í•¨ë˜ëŠ”ì§€ í™•ì¸
        span_start_char = offset_mapping[context_start_token][0]
        span_end_char = offset_mapping[context_end_token][1]

        if answer_start < span_start_char or answer_end > span_end_char:
            return (
                False,
                f"Answer not in span: answer[{answer_start}:{answer_end}], span[{span_start_char}:{span_end_char}]",
                None,
            )

        # ì •ë‹µ í† í° ìœ„ì¹˜ ì°¾ê¸°
        start_token = None
        end_token = None

        for idx in range(context_start_token, context_end_token + 1):
            token_start, token_end = offset_mapping[idx]
            if token_start is None:
                continue

            # start_token: ì •ë‹µ ì‹œì‘ì„ í¬í•¨í•˜ëŠ” í† í°
            if start_token is None and token_start <= answer_start < token_end:
                start_token = idx
            # ë˜ëŠ” ì •ë‹µ ì‹œì‘ì´ í† í° ì‹œì‘ê³¼ ì •í™•íˆ ì¼ì¹˜
            if start_token is None and token_start == answer_start:
                start_token = idx

            # end_token: ì •ë‹µ ëì„ í¬í•¨í•˜ëŠ” í† í°
            if token_start < answer_end <= token_end:
                end_token = idx

        # ë” ê´€ëŒ€í•œ ë§¤ì¹­
        if start_token is None:
            for idx in range(context_start_token, context_end_token + 1):
                token_start, token_end = offset_mapping[idx]
                if token_start is None:
                    continue
                if token_start <= answer_start and answer_start < token_end:
                    start_token = idx
                    break

        if end_token is None:
            for idx in range(context_start_token, context_end_token + 1):
                token_start, token_end = offset_mapping[idx]
                if token_start is None:
                    continue
                if token_start < answer_end and answer_end <= token_end:
                    end_token = idx

        if start_token is None or end_token is None:
            return (
                False,
                f"Could not find answer tokens: start={start_token}, end={end_token}",
                None,
            )

        debug_info = {
            "start_token": start_token,
            "end_token": end_token,
            "input_ids": input_ids,
            "offset_mapping": offset_mapping,
        }

        return True, "", debug_info

    def check_decode_correctness(
        self,
        input_ids: List[int],
        start_token: int,
        end_token: int,
        expected_answer: str,
    ) -> Tuple[bool, str]:
        """
        3ë‹¨ê³„: ë””ì½”ë”©ëœ ì •ë‹µì´ ì›ë³¸ê³¼ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
        """
        decoded = self.tokenizer.decode(input_ids[start_token : end_token + 1])
        decoded_clean = decoded.strip()

        # ì •í™•íˆ ì¼ì¹˜í•˜ê±°ë‚˜, ê³µë°±/íŠ¹ìˆ˜ë¬¸ì ì œê±° í›„ ì¼ì¹˜
        if decoded_clean == expected_answer:
            return True, ""

        # ê³µë°± ì •ê·œí™” í›„ ë¹„êµ
        decoded_normalized = " ".join(decoded_clean.split())
        expected_normalized = " ".join(expected_answer.split())

        if decoded_normalized == expected_normalized:
            return True, f"(normalized match)"

        # ë¶€ë¶„ ì¼ì¹˜ í—ˆìš© (í† í°í™” ê²½ê³„ ë¬¸ì œ)
        if expected_answer in decoded_clean or decoded_clean in expected_answer:
            return True, f"(partial match: decoded='{decoded_clean}')"

        return (
            False,
            f"Mismatch: expected '{expected_answer}', decoded '{decoded_clean}'",
        )

    def check_single_example(
        self,
        question: str,
        context: str,
        answer_text: str,
        answer_start: int,
        example_id: str = "",
    ) -> Dict:
        """ë‹¨ì¼ ì˜ˆì‹œ ê²€ì¦"""
        result = {
            "id": example_id,
            "char_level": False,
            "token_level": False,
            "decode": False,
            "error": None,
        }

        # 1. Character level ê²€ì¦
        char_ok, char_error = self.check_char_level_alignment(
            context, answer_text, answer_start
        )
        result["char_level"] = char_ok

        if not char_ok:
            result["error"] = f"Char level: {char_error}"
            return result

        # 2. Token level ê²€ì¦
        token_ok, token_error, debug_info = self.check_token_level_alignment(
            question, context, answer_text, answer_start
        )
        result["token_level"] = token_ok

        if not token_ok:
            result["error"] = f"Token level: {token_error}"
            return result

        # 3. Decode ê²€ì¦
        decode_ok, decode_error = self.check_decode_correctness(
            debug_info["input_ids"],
            debug_info["start_token"],
            debug_info["end_token"],
            answer_text,
        )
        result["decode"] = decode_ok

        if not decode_ok:
            result["error"] = f"Decode: {decode_error}"

        return result

    def run_on_dataset(
        self,
        examples: List[Dict],
        use_title: bool = False,
        sep_token: Optional[str] = None,
    ) -> Dict:
        """ë°ì´í„°ì…‹ ì „ì²´ì— ëŒ€í•´ ê²€ì¦ ì‹¤í–‰"""
        from src.utils.retrieval_utils import realign_answers_in_retrieved_context

        print_header("Answer Realignment Sanity Check")
        print(f"  Tokenizer: {self.tokenizer.name_or_path}")
        print(f"  use_title: {use_title}")
        print(f"  sep_token: {sep_token}")
        print(f"  Total examples: {len(examples)}")

        self.results = {
            "total": len(examples),
            "char_level_correct": 0,
            "token_level_correct": 0,
            "decode_correct": 0,
            "failures": [],
        }

        for i, example in enumerate(examples):
            # Realignment ì ìš©
            realigned = realign_answers_in_retrieved_context(
                example.copy(),
                sep_token=sep_token,
                use_title=use_title,
            )

            # ì •ë‹µì´ ì—†ìœ¼ë©´ ìŠ¤í‚µ
            if len(realigned["answers"]["text"]) == 0:
                continue

            answer_text = realigned["answers"]["text"][0]
            answer_start = realigned["answers"]["answer_start"][0]
            context = realigned["context"]
            question = example.get("question", "ì§ˆë¬¸")
            example_id = example.get("id", f"ex_{i}")

            # ê²€ì¦
            result = self.check_single_example(
                question=question,
                context=context,
                answer_text=answer_text,
                answer_start=answer_start,
                example_id=example_id,
            )

            if result["char_level"]:
                self.results["char_level_correct"] += 1
            if result["token_level"]:
                self.results["token_level_correct"] += 1
            if result["decode"]:
                self.results["decode_correct"] += 1

            if result["error"]:
                self.results["failures"].append(
                    {
                        "id": example_id,
                        "error": result["error"],
                        "answer": answer_text,
                        "answer_start": answer_start,
                        "context_snippet": context[:100] + "...",
                    }
                )

                if self.verbose:
                    print(f"\n  âš ï¸ Example {example_id}:")
                    print(f"     Error: {result['error']}")
                    print(f"     Answer: '{answer_text}' at {answer_start}")

        return self.results

    def print_summary(self):
        """ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print_header("Results Summary")

        total = self.results["total"]
        if total == 0:
            print("  No examples to check!")
            return

        char_pct = self.results["char_level_correct"] / total * 100
        token_pct = self.results["token_level_correct"] / total * 100
        decode_pct = self.results["decode_correct"] / total * 100

        print(f"  Total examples checked: {total}")
        print(f"")
        print_result(
            "Character-level alignment",
            char_pct == 100,
            f"{self.results['char_level_correct']}/{total} ({char_pct:.1f}%)",
        )
        print_result(
            "Token-level alignment",
            token_pct >= 95,  # 95% ì´ìƒì´ë©´ OK (truncationìœ¼ë¡œ ì¼ë¶€ ì†ì‹¤ ê°€ëŠ¥)
            f"{self.results['token_level_correct']}/{total} ({token_pct:.1f}%)",
        )
        print_result(
            "Decode correctness",
            decode_pct >= 95,
            f"{self.results['decode_correct']}/{total} ({decode_pct:.1f}%)",
        )

        if self.results["failures"]:
            print(f"\n  âš ï¸ {len(self.results['failures'])} failures detected")
            if len(self.results["failures"]) <= 5:
                for f in self.results["failures"]:
                    print(f"     - {f['id']}: {f['error']}")

        # ìµœì¢… íŒì •
        print("\n" + "-" * 70)
        all_pass = char_pct == 100 and token_pct >= 95 and decode_pct >= 95
        if all_pass:
            print("  ğŸ‰ All sanity checks PASSED!")
        else:
            print("  âŒ Some sanity checks FAILED - investigate before training!")

        return all_pass


def create_mock_retrieved_examples(
    num_samples: int = 20,
    use_title: bool = True,
    sep_token: str = "</s>",
) -> List[Dict]:
    """
    ì‹¤ì œ retrieval ê²°ê³¼ì™€ ìœ ì‚¬í•œ mock ë°ì´í„° ìƒì„±

    ë‹¤ì–‘í•œ ì¼€ì´ìŠ¤:
    1. ì •ë‹µì´ ë³¸ë¬¸ì— ìˆëŠ” ê²½ìš°
    2. ì •ë‹µì´ ì—¬ëŸ¬ ë²ˆ ë“±ì¥ (title + ë³¸ë¬¸)
    3. ê¸´ context
    4. íŠ¹ìˆ˜ë¬¸ì í¬í•¨
    """
    examples = []

    # ì¼€ì´ìŠ¤ 1: ê¸°ë³¸ ì¼€ì´ìŠ¤
    for i in range(num_samples // 4):
        title = "ëŒ€í•œë¯¼êµ­"
        body = f"ëŒ€í•œë¯¼êµ­ì€ ë™ì•„ì‹œì•„ì— ìœ„ì¹˜í•œ ë‚˜ë¼ì´ë‹¤. ìˆ˜ë„ëŠ” ì„œìš¸ì´ë©°, ì¸êµ¬ëŠ” ì•½ 5000ë§Œëª…ì´ë‹¤. (ìƒ˜í”Œ {i})"
        context = f"{title} {sep_token} {body}" if use_title else body

        examples.append(
            {
                "id": f"basic_{i}",
                "question": "ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ”?",
                "context": context,
                "answers": {"text": ["ì„œìš¸"], "answer_start": [999]},  # ì˜ëª»ëœ ê°’
            }
        )

    # ì¼€ì´ìŠ¤ 2: ì •ë‹µì´ titleê³¼ ë³¸ë¬¸ ë‘˜ ë‹¤ ìˆëŠ” ê²½ìš°
    for i in range(num_samples // 4):
        title = "ì„œìš¸íŠ¹ë³„ì‹œ"
        body = f"ì„œìš¸ì€ ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ì´ë‹¤. ì„œìš¸ì˜ ì¸êµ¬ëŠ” ì•½ 1000ë§Œëª…ì´ë‹¤. (ìƒ˜í”Œ {i})"
        context = f"{title} {sep_token} {body}" if use_title else body

        examples.append(
            {
                "id": f"title_body_{i}",
                "question": "ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ”?",
                "context": context,
                "answers": {"text": ["ì„œìš¸"], "answer_start": [0]},
            }
        )

    # ì¼€ì´ìŠ¤ 3: ìˆ«ì/ë‚ ì§œ ì •ë‹µ
    for i in range(num_samples // 4):
        title = "ì—­ì‚¬"
        body = f"ëŒ€í•œë¯¼êµ­ì€ 1948ë…„ 8ì›” 15ì¼ì— ê±´êµ­ë˜ì—ˆë‹¤. ì´ëŠ” ê´‘ë³µ 3ë…„ í›„ì˜ ì¼ì´ë‹¤. (ìƒ˜í”Œ {i})"
        context = f"{title} {sep_token} {body}" if use_title else body

        examples.append(
            {
                "id": f"date_{i}",
                "question": "ëŒ€í•œë¯¼êµ­ì˜ ê±´êµ­ì¼ì€?",
                "context": context,
                "answers": {"text": ["1948ë…„ 8ì›” 15ì¼"], "answer_start": [0]},
            }
        )

    # ì¼€ì´ìŠ¤ 4: ê¸´ ì •ë‹µ
    for i in range(num_samples // 4):
        title = "ì§€ë¦¬"
        body = f"ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ” ì„œìš¸íŠ¹ë³„ì‹œì´ë©°, ë©´ì ì€ ì•½ 100,000 ì œê³±í‚¬ë¡œë¯¸í„°ì´ë‹¤. (ìƒ˜í”Œ {i})"
        context = f"{title} {sep_token} {body}" if use_title else body

        examples.append(
            {
                "id": f"long_{i}",
                "question": "ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ì˜ ì •ì‹ ëª…ì¹­ì€?",
                "context": context,
                "answers": {"text": ["ì„œìš¸íŠ¹ë³„ì‹œ"], "answer_start": [0]},
            }
        )

    return examples


def load_real_validation_data(data_path: str = "./data/train_dataset") -> List[Dict]:
    """ì‹¤ì œ validation ë°ì´í„° ë¡œë“œ"""
    try:
        datasets = load_from_disk(data_path)
        examples = []
        for ex in datasets["validation"]:
            examples.append(
                {
                    "id": ex["id"],
                    "question": ex["question"],
                    "context": ex["context"],  # gold context
                    "answers": ex["answers"],
                }
            )
        return examples
    except Exception as e:
        print(f"  âš ï¸ Could not load real data: {e}")
        return []


def main():
    parser = argparse.ArgumentParser(description="Answer Realignment Sanity Check")
    parser.add_argument("--verbose", "-v", action="store_true", help="Verbose output")
    parser.add_argument(
        "--num_samples", type=int, default=20, help="Number of mock samples"
    )
    parser.add_argument(
        "--use_real_data", action="store_true", help="Use real validation data"
    )
    parser.add_argument(
        "--tokenizer", default="klue/roberta-large", help="Tokenizer name"
    )
    parser.add_argument(
        "--use_title", action="store_true", default=True, help="Include title"
    )
    parser.add_argument("--no_title", action="store_true", help="Disable title")
    args = parser.parse_args()

    use_title = not args.no_title

    # Checker ì´ˆê¸°í™”
    checker = AnswerRealignmentSanityChecker(
        tokenizer_name=args.tokenizer,
        verbose=args.verbose,
    )

    sep_token = checker.tokenizer.sep_token
    print(f"\nğŸ” Using sep_token: '{sep_token}'")

    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
    if args.use_real_data:
        print("\nğŸ“‚ Loading real validation data...")
        examples = load_real_validation_data()
        if not examples:
            print("  Falling back to mock data...")
            examples = create_mock_retrieved_examples(
                num_samples=args.num_samples,
                use_title=use_title,
                sep_token=sep_token,
            )
    else:
        print(f"\nğŸ”§ Creating {args.num_samples} mock examples...")
        examples = create_mock_retrieved_examples(
            num_samples=args.num_samples,
            use_title=use_title,
            sep_token=sep_token,
        )

    # ê²€ì¦ ì‹¤í–‰
    checker.run_on_dataset(
        examples=examples,
        use_title=use_title,
        sep_token=sep_token,
    )

    # ê²°ê³¼ ì¶œë ¥
    all_pass = checker.print_summary()

    # Title OFF í…ŒìŠ¤íŠ¸ë„ ì‹¤í–‰
    print("\n" + "=" * 70)
    print("  ğŸ”„ Also testing with use_title=False...")
    print("=" * 70)

    examples_no_title = create_mock_retrieved_examples(
        num_samples=args.num_samples,
        use_title=False,
        sep_token=sep_token,
    )

    checker.run_on_dataset(
        examples=examples_no_title,
        use_title=False,
        sep_token=None,
    )
    all_pass_no_title = checker.print_summary()

    # ì¢…í•© ê²°ê³¼
    print("\n" + "=" * 70)
    print("  ğŸ“Š FINAL VERDICT")
    print("=" * 70)

    if all_pass and all_pass_no_title:
        print("  âœ… All sanity checks PASSED for both use_title=True and False!")
        return 0
    else:
        print("  âŒ Some checks FAILED - review before training!")
        return 1


if __name__ == "__main__":
    sys.exit(main())
