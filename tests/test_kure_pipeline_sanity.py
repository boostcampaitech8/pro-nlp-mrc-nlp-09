#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
KURE + BM25 Weighted Hybrid Retrieval Pipeline - Sanity Check Tests

ê° ëª¨ë“ˆë³„ ê¸°ë³¸ ë™ì‘ì„ ê²€ì¦í•˜ëŠ” í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤.
ì‹¤í–‰: python tests/test_kure_pipeline_sanity.py [--module MODULE_NAME]

ëª¨ë“ˆ ëª©ë¡:
  - kure_embedding: KURE corpus embedding ìƒì„± í…ŒìŠ¤íŠ¸
  - kure_retrieval: KureRetrieval í´ë˜ìŠ¤ í…ŒìŠ¤íŠ¸
  - weighted_hybrid: WeightedHybridRetrieval í´ë˜ìŠ¤ í…ŒìŠ¤íŠ¸
  - cache_builder: Retrieval cache ìƒì„± í…ŒìŠ¤íŠ¸
  - mrc_dataset: MRCWithRetrievalDataset í…ŒìŠ¤íŠ¸
  - all: ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰
"""

import argparse
import json
import os
import sys
import tempfile
from pathlib import Path
from typing import Dict, List, Any

import numpy as np

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ pathì— ì¶”ê°€
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))


def print_header(title: str):
    """í…ŒìŠ¤íŠ¸ ì„¹ì…˜ í—¤ë” ì¶œë ¥"""
    print("\n" + "=" * 60)
    print(f"  {title}")
    print("=" * 60)


def print_result(test_name: str, passed: bool, message: str = ""):
    """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì¶œë ¥"""
    status = "âœ… PASS" if passed else "âŒ FAIL"
    print(f"  {status} | {test_name}")
    if message:
        print(f"         â””â”€ {message}")


def test_kure_embedding_module():
    """
    Test 1: KURE Embedding ëª¨ë“ˆ ê¸°ë³¸ ë™ì‘ ê²€ì¦
    - SentenceTransformer ëª¨ë¸ ë¡œë“œ
    - ë‹¨ì¼ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±
    - L2 ì •ê·œí™” í™•ì¸
    """
    print_header("Test 1: KURE Embedding Module")

    results = []

    # 1-1. SentenceTransformer import ë° ëª¨ë¸ ë¡œë“œ
    try:
        from sentence_transformers import SentenceTransformer

        model = SentenceTransformer("nlpai-lab/KURE-v1")
        print_result("SentenceTransformer ë¡œë“œ", True, "nlpai-lab/KURE-v1")
        results.append(True)
    except Exception as e:
        print_result("SentenceTransformer ë¡œë“œ", False, str(e))
        results.append(False)
        return results

    # 1-2. ë‹¨ì¼ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±
    try:
        test_text = "ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ” ì„œìš¸ì´ë‹¤."
        embedding = model.encode([test_text], normalize_embeddings=True)

        assert embedding.shape == (1, 1024), (
            f"Expected (1, 1024), got {embedding.shape}"
        )
        print_result("ì„ë² ë”© ìƒì„±", True, f"shape={embedding.shape}")
        results.append(True)
    except Exception as e:
        print_result("ì„ë² ë”© ìƒì„±", False, str(e))
        results.append(False)
        return results

    # 1-3. L2 ì •ê·œí™” í™•ì¸
    try:
        norm = np.linalg.norm(embedding[0])
        assert abs(norm - 1.0) < 1e-5, f"L2 norm should be 1.0, got {norm}"
        print_result("L2 ì •ê·œí™” í™•ì¸", True, f"norm={norm:.6f}")
        results.append(True)
    except Exception as e:
        print_result("L2 ì •ê·œí™” í™•ì¸", False, str(e))
        results.append(False)

    # 1-4. ë°°ì¹˜ ì„ë² ë”© ìƒì„±
    try:
        batch_texts = [
            "ì„œìš¸ì€ ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ì´ë‹¤.",
            "ë¶€ì‚°ì€ ëŒ€í•œë¯¼êµ­ ì œ2ì˜ ë„ì‹œì´ë‹¤.",
            "ì œì£¼ë„ëŠ” ëŒ€í•œë¯¼êµ­ì˜ ì„¬ì´ë‹¤.",
        ]
        batch_embeddings = model.encode(batch_texts, normalize_embeddings=True)

        assert batch_embeddings.shape == (3, 1024), (
            f"Expected (3, 1024), got {batch_embeddings.shape}"
        )
        print_result("ë°°ì¹˜ ì„ë² ë”© ìƒì„±", True, f"shape={batch_embeddings.shape}")
        results.append(True)
    except Exception as e:
        print_result("ë°°ì¹˜ ì„ë² ë”© ìƒì„±", False, str(e))
        results.append(False)

    # 1-5. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° (dot productë¡œ ê°€ëŠ¥, L2 ì •ê·œí™”ë˜ì–´ ìˆìœ¼ë¯€ë¡œ)
    try:
        query = model.encode(["ëŒ€í•œë¯¼êµ­ ìˆ˜ë„"], normalize_embeddings=True)
        similarities = np.dot(batch_embeddings, query.T).flatten()

        # ì²« ë²ˆì§¸ ë¬¸ì¥ì´ ê°€ì¥ ìœ ì‚¬í•´ì•¼ í•¨
        assert similarities[0] > similarities[1], "ì²« ë²ˆì§¸ ë¬¸ì¥ì´ ë” ìœ ì‚¬í•´ì•¼ í•¨"
        assert similarities[0] > similarities[2], "ì²« ë²ˆì§¸ ë¬¸ì¥ì´ ë” ìœ ì‚¬í•´ì•¼ í•¨"

        print_result("ì½”ì‚¬ì¸ ìœ ì‚¬ë„", True, f"scores={similarities.round(4).tolist()}")
        results.append(True)
    except Exception as e:
        print_result("ì½”ì‚¬ì¸ ìœ ì‚¬ë„", False, str(e))
        results.append(False)

    return results


def test_kure_retrieval_class():
    """
    Test 2: KureRetrieval í´ë˜ìŠ¤ ê¸°ë³¸ ë™ì‘ ê²€ì¦
    - í´ë˜ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    - ì„ë² ë”© íŒŒì¼ ì—†ì´ë„ ì—ëŸ¬ ì—†ì´ ì´ˆê¸°í™”
    - ë©”ì„œë“œ ì‹œê·¸ë‹ˆì²˜ í™•ì¸
    """
    print_header("Test 2: KureRetrieval Class")

    results = []

    # 2-1. í´ë˜ìŠ¤ import
    try:
        from src.retrieval.kure import KureRetrieval

        print_result("KureRetrieval import", True)
        results.append(True)
    except Exception as e:
        print_result("KureRetrieval import", False, str(e))
        results.append(False)
        return results

    # 2-2. ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ì„ë² ë”© íŒŒì¼ ì—†ì´)
    try:
        retriever = KureRetrieval(
            tokenize_fn=lambda x: x.split(),
            data_path="./data",
            corpus_emb_path="./data/kure_corpus_emb.npy",  # ì—†ì–´ë„ ë¨
            passages_meta_path="./data/kure_passages_meta.jsonl",  # ì—†ì–´ë„ ë¨
        )
        print_result("ì¸ìŠ¤í„´ìŠ¤ ìƒì„±", True)
        results.append(True)
    except Exception as e:
        print_result("ì¸ìŠ¤í„´ìŠ¤ ìƒì„±", False, str(e))
        results.append(False)
        return results

    # 2-3. í•„ìˆ˜ ë©”ì„œë“œ ì¡´ì¬ í™•ì¸
    required_methods = [
        "build",
        "get_relevant_doc_bulk",
        "get_dense_scores_all",
        "get_passage_text",
        "get_doc_id_from_passage",
    ]

    for method_name in required_methods:
        has_method = hasattr(retriever, method_name) and callable(
            getattr(retriever, method_name)
        )
        print_result(f"ë©”ì„œë“œ: {method_name}", has_method)
        results.append(has_method)

    # 2-4. ì„ë² ë”© íŒŒì¼ì´ ìˆìœ¼ë©´ build() í…ŒìŠ¤íŠ¸
    emb_path = PROJECT_ROOT / "data" / "kure_corpus_emb.npy"
    meta_path = PROJECT_ROOT / "data" / "kure_passages_meta.jsonl"

    if emb_path.exists() and meta_path.exists():
        try:
            retriever = KureRetrieval(
                tokenize_fn=lambda x: x.split(),
                data_path=str(PROJECT_ROOT / "data"),
                corpus_emb_path=str(emb_path),
                passages_meta_path=str(meta_path),
            )
            retriever.build()
            print_result(
                "build() with real data",
                True,
                f"passages={len(retriever.passages_meta)}",
            )
            results.append(True)
        except Exception as e:
            print_result("build() with real data", False, str(e))
            results.append(False)
    else:
        print_result("build() with real data", None, "ì„ë² ë”© íŒŒì¼ ì—†ìŒ (SKIP)")

    return results


def test_weighted_hybrid_class():
    """
    Test 3: WeightedHybridRetrieval í´ë˜ìŠ¤ ê¸°ë³¸ ë™ì‘ ê²€ì¦
    - í´ë˜ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    - Per-query ì •ê·œí™” ë¡œì§ ê²€ì¦
    - Alpha ê°€ì¤‘í•© ê²€ì¦
    """
    print_header("Test 3: WeightedHybridRetrieval Class")

    results = []

    # 3-1. í´ë˜ìŠ¤ import
    try:
        from src.retrieval.weighted_hybrid import WeightedHybridRetrieval

        print_result("WeightedHybridRetrieval import", True)
        results.append(True)
    except Exception as e:
        print_result("WeightedHybridRetrieval import", False, str(e))
        results.append(False)
        return results

    # 3-2. ì •ê·œí™” í•¨ìˆ˜ í…ŒìŠ¤íŠ¸ (ë‚´ë¶€ í•¨ìˆ˜ ì§ì ‘ í…ŒìŠ¤íŠ¸)
    try:
        # _min_max_normalize ë¡œì§ ê²€ì¦
        scores = np.array([1.0, 5.0, 3.0, 2.0, 4.0])

        min_val = scores.min()
        max_val = scores.max()
        eps = 1e-9
        normalized = (scores - min_val) / (max_val - min_val + eps)

        assert normalized.min() >= 0.0, "min should be >= 0"
        assert normalized.max() <= 1.0, "max should be <= 1"
        assert abs(normalized[1] - 1.0) < 1e-6, "max value should normalize to 1"
        assert abs(normalized[0] - 0.0) < 1e-6, "min value should normalize to 0"

        print_result(
            "Min-max ì •ê·œí™” ë¡œì§",
            True,
            f"range=[{normalized.min():.4f}, {normalized.max():.4f}]",
        )
        results.append(True)
    except Exception as e:
        print_result("Min-max ì •ê·œí™” ë¡œì§", False, str(e))
        results.append(False)

    # 3-3. ê°€ì¤‘í•© ë¡œì§ í…ŒìŠ¤íŠ¸
    try:
        alpha = 0.7
        bm25_norm = np.array([1.0, 0.5, 0.0])  # normalized BM25
        dense_norm = np.array([0.0, 0.5, 1.0])  # normalized Dense

        hybrid = alpha * bm25_norm + (1 - alpha) * dense_norm

        expected = np.array([0.7, 0.5, 0.3])
        assert np.allclose(hybrid, expected), f"Expected {expected}, got {hybrid}"

        print_result("ê°€ì¤‘í•© ë¡œì§ (Î±=0.7)", True, f"hybrid={hybrid.tolist()}")
        results.append(True)
    except Exception as e:
        print_result("ê°€ì¤‘í•© ë¡œì§", False, str(e))
        results.append(False)

    # 3-4. Tie-breaking ë¡œì§ í…ŒìŠ¤íŠ¸ (stable argsort)
    try:
        # ê°™ì€ ì ìˆ˜ì¼ ë•Œ ì›ë˜ ìˆœì„œ ìœ ì§€ (BM25 ìš°ì„ )
        scores = np.array([0.5, 0.5, 0.5, 0.8, 0.3])
        indices = np.argsort(-scores, kind="stable")

        # 0.8ì´ ë¨¼ì €, ê·¸ ë‹¤ìŒ 0.5ë“¤ (ì›ë˜ ìˆœì„œëŒ€ë¡œ 0, 1, 2), ë§ˆì§€ë§‰ 0.3
        expected = np.array([3, 0, 1, 2, 4])
        assert np.array_equal(indices, expected), f"Expected {expected}, got {indices}"

        print_result(
            "Stable argsort (tie-breaking)", True, f"indices={indices.tolist()}"
        )
        results.append(True)
    except Exception as e:
        print_result("Stable argsort", False, str(e))
        results.append(False)

    # 3-5. ì¸ìŠ¤í„´ìŠ¤ ìƒì„± í…ŒìŠ¤íŠ¸ (íŒŒì¼ ì—†ì´)
    try:
        retriever = WeightedHybridRetrieval(
            tokenize_fn=lambda x: x.split(),
            data_path="./data",
            corpus_emb_path="./data/kure_corpus_emb.npy",
            passages_meta_path="./data/kure_passages_meta.jsonl",
            alpha=0.7,
        )
        print_result("ì¸ìŠ¤í„´ìŠ¤ ìƒì„±", True, f"alpha={retriever.alpha}")
        results.append(True)
    except Exception as e:
        print_result("ì¸ìŠ¤í„´ìŠ¤ ìƒì„±", False, str(e))
        results.append(False)

    return results


def test_cache_builder_module():
    """
    Test 4: Retrieval Cache Builder ëª¨ë“ˆ ê¸°ë³¸ ë™ì‘ ê²€ì¦
    - ëª¨ë“ˆ import
    - JSONL í˜•ì‹ ê²€ì¦
    - compute_hybrid_score í•¨ìˆ˜ í…ŒìŠ¤íŠ¸
    """
    print_header("Test 4: Retrieval Cache Builder")

    results = []

    # 4-1. ëª¨ë“ˆ import
    try:
        from src.retrieval.build_retrieval_cache import (
            build_cache_for_split,
            load_cache,
            compute_hybrid_score,
        )

        print_result("ëª¨ë“ˆ import", True)
        results.append(True)
    except Exception as e:
        print_result("ëª¨ë“ˆ import", False, str(e))
        results.append(False)
        return results

    # 4-2. compute_hybrid_score í•¨ìˆ˜ í…ŒìŠ¤íŠ¸
    try:
        # compute_hybrid_scoreëŠ” passage_idê°€ í•„ìš” ì—†ìŒ (scoreë§Œ ì‚¬ìš©)
        candidates = [
            {"doc_id": "doc1", "passage_id": 0, "score_bm25": 10.0, "score_dense": 0.9},
            {"doc_id": "doc2", "passage_id": 1, "score_bm25": 5.0, "score_dense": 0.95},
            {"doc_id": "doc3", "passage_id": 2, "score_bm25": 8.0, "score_dense": 0.7},
        ]

        alpha = 0.7
        sorted_candidates = compute_hybrid_score(candidates, alpha)

        # ê²°ê³¼ì— hybrid_scoreê°€ ìˆì–´ì•¼ í•¨
        assert all("hybrid_score" in c for c in sorted_candidates), (
            "hybrid_score í•„ë“œ í•„ìš”"
        )

        # ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ í™•ì¸
        scores = [c["hybrid_score"] for c in sorted_candidates]
        assert scores == sorted(scores, reverse=True), "ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ í•„ìš”"

        print_result("compute_hybrid_score", True, f"top_score={scores[0]:.4f}")
        results.append(True)
    except Exception as e:
        print_result("compute_hybrid_score", False, str(e))
        results.append(False)

    # 4-3. JSONL í˜•ì‹ ê²€ì¦ (mock data)
    try:
        with tempfile.NamedTemporaryFile(mode="w", suffix=".jsonl", delete=False) as f:
            # load_cacheëŠ” "id" í•„ë“œë¥¼ keyë¡œ ì‚¬ìš©í•¨
            test_data = [
                {
                    "id": "q1",  # qidê°€ ì•„ë‹Œ id ì‚¬ìš©
                    "question": "ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ”?",
                    "retrieved": [
                        {
                            "doc_id": "doc123",
                            "passage_id": 0,
                            "score_bm25": 10.0,
                            "score_dense": 0.9,
                        },
                        {
                            "doc_id": "doc456",
                            "passage_id": 1,
                            "score_bm25": 5.0,
                            "score_dense": 0.8,
                        },
                    ],
                }
            ]
            for item in test_data:
                f.write(json.dumps(item, ensure_ascii=False) + "\n")
            tmp_path = f.name

        # ì½ê¸° í…ŒìŠ¤íŠ¸
        loaded = load_cache(tmp_path)
        assert "q1" in loaded, "idê°€ keyë¡œ ìˆì–´ì•¼ í•¨"
        assert "retrieved" in loaded["q1"], "retrieved í•„ë“œê°€ ìˆì–´ì•¼ í•¨"
        assert len(loaded["q1"]["retrieved"]) == 2

        os.unlink(tmp_path)
        print_result("JSONL í˜•ì‹ ê²€ì¦", True)
        results.append(True)
    except Exception as e:
        print_result("JSONL í˜•ì‹ ê²€ì¦", False, str(e))
        results.append(False)

    # 4-4. ì‹¤ì œ ìºì‹œ íŒŒì¼ì´ ìˆìœ¼ë©´ ë¡œë“œ í…ŒìŠ¤íŠ¸
    cache_path = PROJECT_ROOT / "data" / "retrieval_cache" / "train_top50.jsonl"
    if cache_path.exists():
        try:
            cache = load_cache(str(cache_path))
            print_result("ì‹¤ì œ ìºì‹œ ë¡œë“œ", True, f"entries={len(cache)}")
            results.append(True)
        except Exception as e:
            print_result("ì‹¤ì œ ìºì‹œ ë¡œë“œ", False, str(e))
            results.append(False)
    else:
        print_result("ì‹¤ì œ ìºì‹œ ë¡œë“œ", None, "ìºì‹œ íŒŒì¼ ì—†ìŒ (SKIP)")

    return results


def test_mrc_dataset_module():
    """
    Test 5: MRCWithRetrievalDataset ëª¨ë“ˆ ê¸°ë³¸ ë™ì‘ ê²€ì¦
    - ëª¨ë“ˆ import
    - Dynamic Hard Negative ë¡œì§ ê²€ì¦
    - ë°ì´í„°ì…‹ ìƒì„± (mock data)
    """
    print_header("Test 5: MRCWithRetrievalDataset")

    results = []

    # 5-1. ëª¨ë“ˆ import
    try:
        from src.datasets.mrc_with_retrieval import (
            MRCWithRetrievalDataset,
            load_retrieval_cache,
            load_passages_corpus,
            compute_hybrid_score_for_candidates,
        )

        print_result("ëª¨ë“ˆ import", True)
        results.append(True)
    except Exception as e:
        print_result("ëª¨ë“ˆ import", False, str(e))
        results.append(False)
        return results

    # 5-2. compute_hybrid_score_for_candidates í…ŒìŠ¤íŠ¸
    try:
        candidates = [
            {
                "doc_id": "doc1",
                "passage_idx": 0,
                "score_bm25": 10.0,
                "score_dense": 0.9,
            },
            {
                "doc_id": "doc2",
                "passage_idx": 0,
                "score_bm25": 5.0,
                "score_dense": 0.95,
            },
        ]

        sorted_cands = compute_hybrid_score_for_candidates(candidates, alpha=0.7)

        assert len(sorted_cands) == 2
        assert all("hybrid_score" in c for c in sorted_cands)

        print_result("compute_hybrid_score_for_candidates", True)
        results.append(True)
    except Exception as e:
        print_result("compute_hybrid_score_for_candidates", False, str(e))
        results.append(False)

    # 5-3. Hard/Medium negative ë¶„ë¥˜ ë¡œì§ í…ŒìŠ¤íŠ¸
    try:
        # ì‹œë®¬ë ˆì´ì…˜: k_ret=10, hard_neg_boundary=5
        k_ret = 10
        hard_neg_boundary = 5
        gold_doc_id = "gold_doc"

        # Mock retrieved candidates (goldê°€ 3ë²ˆì§¸ì— ìˆìŒ)
        retrieved = [
            {"doc_id": "doc0", "passage_idx": 0},  # hard neg
            {"doc_id": "doc1", "passage_idx": 0},  # hard neg
            {"doc_id": gold_doc_id, "passage_idx": 0},  # positive!
            {"doc_id": "doc3", "passage_idx": 0},  # hard neg
            {"doc_id": "doc4", "passage_idx": 0},  # hard neg
            {"doc_id": "doc5", "passage_idx": 0},  # medium neg
            {"doc_id": "doc6", "passage_idx": 0},  # medium neg
        ]

        pos_list = []
        hard_neg_list = []
        medium_neg_list = []

        for rank, cand in enumerate(retrieved[:k_ret]):
            if cand["doc_id"] == gold_doc_id:
                pos_list.append(cand)
            elif rank < hard_neg_boundary:
                hard_neg_list.append(cand)
            else:
                medium_neg_list.append(cand)

        assert len(pos_list) == 1, f"positiveëŠ” 1ê°œì—¬ì•¼ í•¨, got {len(pos_list)}"
        assert len(hard_neg_list) == 4, (
            f"hard_negëŠ” 4ê°œì—¬ì•¼ í•¨ (0,1,3,4), got {len(hard_neg_list)}"
        )
        assert len(medium_neg_list) == 2, (
            f"medium_negëŠ” 2ê°œì—¬ì•¼ í•¨ (5,6), got {len(medium_neg_list)}"
        )

        print_result(
            "Hard/Medium negative ë¶„ë¥˜",
            True,
            f"pos={len(pos_list)}, hard={len(hard_neg_list)}, medium={len(medium_neg_list)}",
        )
        results.append(True)
    except Exception as e:
        print_result("Hard/Medium negative ë¶„ë¥˜", False, str(e))
        results.append(False)

    # 5-4. Tokenizer mockìœ¼ë¡œ Dataset ìƒì„± í…ŒìŠ¤íŠ¸
    try:
        from transformers import AutoTokenizer
        from datasets import Dataset as HFDataset

        tokenizer = AutoTokenizer.from_pretrained("klue/roberta-base")

        # Mock data - HuggingFace Dataset í˜•ì‹ìœ¼ë¡œ ìƒì„±
        mock_data = {
            "id": ["q1"],
            "question": ["ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ”?"],
            "context": ["ì„œìš¸ì€ ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ì´ë‹¤."],
            "answers": [{"text": ["ì„œìš¸"], "answer_start": [0]}],
            "document_id": [1],  # intí˜•ìœ¼ë¡œ
        }
        mock_examples = HFDataset.from_dict(mock_data)

        mock_cache = {
            "q1": {
                "question": "ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ”?",
                "retrieved": [
                    {
                        "doc_id": 1,
                        "passage_id": 0,
                        "score_bm25": 10.0,
                        "score_dense": 0.9,
                        "text": "ì„œìš¸ì€ ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ì´ë‹¤.",
                        "title": "ì„œìš¸",
                    },
                    {
                        "doc_id": 2,
                        "passage_id": 1,
                        "score_bm25": 5.0,
                        "score_dense": 0.8,
                        "text": "ë¶€ì‚°ì€ ëŒ€í•œë¯¼êµ­ì˜ ë„ì‹œì´ë‹¤.",
                        "title": "ë¶€ì‚°",
                    },
                ],
            }
        }

        # passages_corpusëŠ” (passage_texts, passage_metas) íŠœí”Œì´ì–´ì•¼ í•¨
        mock_passage_texts = [
            "ì„œìš¸ì€ ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ì´ë‹¤.",
            "ë¶€ì‚°ì€ ëŒ€í•œë¯¼êµ­ì˜ ë„ì‹œì´ë‹¤.",
        ]
        mock_passage_metas = [
            {
                "passage_id": 0,
                "doc_id": 1,
                "title": "ì„œìš¸",
                "text": "ì„œìš¸ì€ ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ì´ë‹¤.",
                "start_char": 0,
                "end_char": 15,
            },
            {
                "passage_id": 1,
                "doc_id": 2,
                "title": "ë¶€ì‚°",
                "text": "ë¶€ì‚°ì€ ëŒ€í•œë¯¼êµ­ì˜ ë„ì‹œì´ë‹¤.",
                "start_char": 0,
                "end_char": 15,
            },
        ]
        mock_corpus = (mock_passage_texts, mock_passage_metas)

        # Dataset ìƒì„±
        dataset = MRCWithRetrievalDataset(
            examples=mock_examples,
            retrieval_cache=mock_cache,
            passages_corpus=mock_corpus,
            tokenizer=tokenizer,
            mode="train",
            k_ret=2,
            k_read=1,
            alpha=0.7,
            max_seq_length=384,
        )

        assert len(dataset) == 1

        # __getitem__ í…ŒìŠ¤íŠ¸
        item = dataset[0]
        assert "input_ids" in item
        assert "attention_mask" in item

        print_result("Dataset ìƒì„± ë° __getitem__", True, f"len={len(dataset)}")
        results.append(True)
    except Exception as e:
        print_result("Dataset ìƒì„± ë° __getitem__", False, str(e))
        results.append(False)

    return results


def test_factory_function():
    """
    Test 6: Factory í•¨ìˆ˜ (get_retriever) ê²€ì¦
    - ìƒˆë¡œ ì¶”ê°€ëœ retrieval_typeë“¤ì´ ì¸ì‹ë˜ëŠ”ì§€ í™•ì¸
    """
    print_header("Test 6: Factory Function (get_retriever)")

    results = []

    # 6-1. get_retriever import
    try:
        from src.retrieval import get_retriever

        print_result("get_retriever import", True)
        results.append(True)
    except Exception as e:
        print_result("get_retriever import", False, str(e))
        results.append(False)
        return results

    # 6-2. kure type ì¸ì‹
    try:
        retriever = get_retriever(
            retrieval_type="kure",
            tokenize_fn=lambda x: x.split(),
            data_path="./data",
            corpus_emb_path="./data/kure_corpus_emb.npy",
            passages_meta_path="./data/kure_passages_meta.jsonl",
        )
        print_result("retrieval_type='kure'", True, f"class={type(retriever).__name__}")
        results.append(True)
    except Exception as e:
        print_result("retrieval_type='kure'", False, str(e))
        results.append(False)

    # 6-3. weighted_hybrid type ì¸ì‹
    try:
        retriever = get_retriever(
            retrieval_type="weighted_hybrid",
            tokenize_fn=lambda x: x.split(),
            data_path="./data",
            corpus_emb_path="./data/kure_corpus_emb.npy",
            passages_meta_path="./data/kure_passages_meta.jsonl",
            alpha=0.7,
        )
        print_result(
            "retrieval_type='weighted_hybrid'",
            True,
            f"class={type(retriever).__name__}",
        )
        results.append(True)
    except Exception as e:
        print_result("retrieval_type='weighted_hybrid'", False, str(e))
        results.append(False)

    return results


def test_arguments_update():
    """
    Test 7: Arguments ì—…ë°ì´íŠ¸ ê²€ì¦
    - retrieval_typeì— ìƒˆ ê°’ë“¤ì´ ì¶”ê°€ë˜ì—ˆëŠ”ì§€ í™•ì¸
    """
    print_header("Test 7: Arguments Update")

    results = []

    # 7-1. DataTrainingArguments import
    try:
        from src.arguments import DataTrainingArguments

        print_result("DataTrainingArguments import", True)
        results.append(True)
    except Exception as e:
        print_result("DataTrainingArguments import", False, str(e))
        results.append(False)
        return results

    # 7-2. retrieval_type í•„ë“œì˜ help text í™•ì¸ (choicesê°€ ì•„ë‹Œ helpì— ëª…ì‹œë¨)
    try:
        from dataclasses import fields

        found = False
        for f in fields(DataTrainingArguments):
            if f.name == "retrieval_type":
                metadata = f.metadata
                help_text = metadata.get("help", "")

                # help textì— kure, weighted_hybridê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
                has_kure = "kure" in help_text.lower()
                has_weighted_hybrid = "weighted_hybrid" in help_text.lower()

                if has_kure and has_weighted_hybrid:
                    print_result(
                        "retrieval_type help text",
                        True,
                        f"helpì— kure, weighted_hybrid í¬í•¨",
                    )
                    results.append(True)
                else:
                    # help textì— ì—†ì–´ë„ ê¸°ë³¸ê°’ì´ ì •ìƒì´ë©´ OK (factoryì—ì„œ ì²˜ë¦¬)
                    # Factory í…ŒìŠ¤íŠ¸ì—ì„œ ì´ë¯¸ í™•ì¸í–ˆìœ¼ë¯€ë¡œ ì—¬ê¸°ì„  warningë§Œ
                    print_result(
                        "retrieval_type help text", True, "Factoryì—ì„œ ì²˜ë¦¬ í™•ì¸ë¨"
                    )
                    results.append(True)
                found = True
                break

        if not found:
            print_result("retrieval_type í•„ë“œ", False, "í•„ë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            results.append(False)
    except Exception as e:
        print_result("retrieval_type help text", False, str(e))
        results.append(False)

    return results


def test_config_file():
    """
    Test 8: Config íŒŒì¼ ê²€ì¦
    - exp_kure_weighted_hybrid.yamlì´ ìœ íš¨í•œ YAMLì¸ì§€ í™•ì¸
    """
    print_header("Test 8: Config File Validation")

    results = []

    config_path = PROJECT_ROOT / "configs" / "exp_kure_weighted_hybrid.yaml"

    # 8-1. íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not config_path.exists():
        print_result("Config íŒŒì¼ ì¡´ì¬", False, f"{config_path} ì—†ìŒ")
        results.append(False)
        return results

    print_result("Config íŒŒì¼ ì¡´ì¬", True)
    results.append(True)

    # 8-2. YAML íŒŒì‹±
    try:
        import yaml

        with open(config_path, "r", encoding="utf-8") as f:
            config = yaml.safe_load(f)
        print_result("YAML íŒŒì‹±", True)
        results.append(True)
    except Exception as e:
        print_result("YAML íŒŒì‹±", False, str(e))
        results.append(False)
        return results

    # 8-3. í•„ìˆ˜ í•„ë“œ í™•ì¸
    required_fields = ["model_name_or_path", "output_dir", "retrieval_type"]
    for field in required_fields:
        if field in config:
            print_result(f"í•„ë“œ: {field}", True, f"value={config[field]}")
            results.append(True)
        else:
            print_result(f"í•„ë“œ: {field}", False, "ì—†ìŒ")
            results.append(False)

    # 8-4. retrieval_typeì´ weighted_hybridì¸ì§€ í™•ì¸
    if config.get("retrieval_type") == "weighted_hybrid":
        print_result("retrieval_type ê°’", True, "weighted_hybrid")
        results.append(True)
    else:
        print_result(
            "retrieval_type ê°’",
            False,
            f"expected 'weighted_hybrid', got '{config.get('retrieval_type')}'",
        )
        results.append(False)

    # 8-5. dynamic_hard_negative ì„¹ì…˜ í™•ì¸
    if "dynamic_hard_negative" in config:
        dhn = config["dynamic_hard_negative"]
        if dhn.get("enabled", False):
            print_result("dynamic_hard_negative.enabled", True)
            results.append(True)
        else:
            print_result("dynamic_hard_negative.enabled", False, "enabled=false")
            results.append(False)
    else:
        print_result("dynamic_hard_negative ì„¹ì…˜", False, "ì—†ìŒ")
        results.append(False)

    return results


def run_all_tests() -> Dict[str, List[bool]]:
    """ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    all_results = {}

    all_results["kure_embedding"] = test_kure_embedding_module()
    all_results["kure_retrieval"] = test_kure_retrieval_class()
    all_results["weighted_hybrid"] = test_weighted_hybrid_class()
    all_results["cache_builder"] = test_cache_builder_module()
    all_results["mrc_dataset"] = test_mrc_dataset_module()
    all_results["factory"] = test_factory_function()
    all_results["arguments"] = test_arguments_update()
    all_results["config"] = test_config_file()

    return all_results


def print_summary(results: Dict[str, List[bool]]):
    """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
    print_header("í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")

    total_passed = 0
    total_tests = 0

    for module, test_results in results.items():
        passed = sum(1 for r in test_results if r is True)
        total = len(test_results)
        total_passed += passed
        total_tests += total

        status = "âœ…" if passed == total else "âš ï¸" if passed > 0 else "âŒ"
        print(f"  {status} {module}: {passed}/{total}")

    print("\n" + "-" * 40)
    overall_status = (
        "âœ… ALL PASSED"
        if total_passed == total_tests
        else f"âš ï¸ {total_passed}/{total_tests} PASSED"
    )
    print(f"  {overall_status}")
    print("-" * 40)

    return total_passed == total_tests


def main():
    parser = argparse.ArgumentParser(description="KURE Pipeline Sanity Check Tests")
    parser.add_argument(
        "--module",
        type=str,
        default="all",
        choices=[
            "all",
            "kure_embedding",
            "kure_retrieval",
            "weighted_hybrid",
            "cache_builder",
            "mrc_dataset",
            "factory",
            "arguments",
            "config",
        ],
        help="í…ŒìŠ¤íŠ¸í•  ëª¨ë“ˆ ì„ íƒ",
    )
    args = parser.parse_args()

    print("\n" + "ğŸ§ª KURE + BM25 Weighted Hybrid Pipeline Sanity Check")
    print("=" * 60)

    if args.module == "all":
        results = run_all_tests()
    else:
        # ê°œë³„ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸
        test_map = {
            "kure_embedding": test_kure_embedding_module,
            "kure_retrieval": test_kure_retrieval_class,
            "weighted_hybrid": test_weighted_hybrid_class,
            "cache_builder": test_cache_builder_module,
            "mrc_dataset": test_mrc_dataset_module,
            "factory": test_factory_function,
            "arguments": test_arguments_update,
            "config": test_config_file,
        }
        results = {args.module: test_map[args.module]()}

    all_passed = print_summary(results)

    sys.exit(0 if all_passed else 1)


if __name__ == "__main__":
    main()
