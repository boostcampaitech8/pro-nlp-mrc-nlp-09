"""
Retrieval Top-K Recall í…ŒìŠ¤íŠ¸

KoE5 Dense Retrievalê³¼ TF-IDF Sparse Retrieval ì„±ëŠ¥ì„ ì¸¡ì •í•©ë‹ˆë‹¤.
- Top-K Recall: ì •ë‹µ ë¬¸ì„œê°€ ìƒìœ„ Kê°œ ì•ˆì— í¬í•¨ë˜ëŠ” ë¹„ìœ¨ (%)
- ì‹¤í–‰ ì‹œê°„: validation 240ê°œ ê¸°ì¤€ ~10-30ì´ˆ

ğŸ“– ì‚¬ìš©ë²•:
    # KoE5 Dense Retrieval í…ŒìŠ¤íŠ¸
    python tests/test_retrieval_recall.py --retriever koe5 --topk 1,5,10,20,50

    # TF-IDF Sparse Retrieval í…ŒìŠ¤íŠ¸
    python tests/test_retrieval_recall.py --retriever tfidf --topk 1,5,10,20,50

    # validation ëŒ€ì‹  train split ì‚¬ìš©
    python tests/test_retrieval_recall.py --retriever koe5 --split train

    # ë„ì›€ë§
    python tests/test_retrieval_recall.py --help

ğŸ“Š ì˜ˆìƒ ê²°ê³¼:
    KoE5 (Dense):
        recall@1  : ~45-50%
        recall@10 : ~75-85%
        recall@50 : ~85-90%

    TF-IDF (Sparse):
        recall@1  : ~35-45%
        recall@10 : ~60-70%
        recall@50 : ~75-85%
"""

import argparse
import os
import sys
import time
from typing import List, Dict

import numpy as np
from datasets import load_from_disk
from tqdm import tqdm

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

from src.retrieval import SparseRetrieval, KoE5Retrieval


def initialize_retriever(retriever_type: str):
    """Retriever ì´ˆê¸°í™” (koe5 ë˜ëŠ” tfidf)"""
    if retriever_type == "koe5":
        return KoE5Retrieval(
            data_path="./data",
            context_path="wikipedia_documents.json",
            corpus_emb_path="./data/koe5_corpus_emb.npy",
        )
    else:  # tfidf
        from transformers import AutoTokenizer

        tokenizer = AutoTokenizer.from_pretrained("klue/bert-base")
        return SparseRetrieval(
            tokenize_fn=tokenizer.tokenize,
            data_path="./data",
            context_path="wikipedia_documents.json",
        )


def analyze_full_dataset(
    retriever_type: str,
    dataset_path: str,
    topk_list: List[int] = [1, 5, 10, 20, 50, 100],
) -> None:
    """
    ì „ì²´ train+validation ë°ì´í„°ì…‹ìœ¼ë¡œ recall@k ë¶„ì„ (retrieval_sanity.py ìŠ¤íƒ€ì¼)

    ì„±ëŠ¥ ìµœì í™”: k=max(topk_list) í•œ ë²ˆë§Œ ê³„ì‚° í›„ ìŠ¬ë¼ì´ì‹±
    """
    print("\n" + "=" * 80)
    print(f"ğŸ“Š FULL DATASET ANALYSIS ({retriever_type.upper()})")
    print("=" * 80)

    # ì „ì²´ ë°ì´í„°ì…‹ ë¡œë“œ
    from datasets import concatenate_datasets

    ds = load_from_disk(dataset_path)
    train_ds = ds["train"].flatten_indices()
    valid_ds = ds["validation"].flatten_indices()

    print(f"ğŸ“ Train samples: {len(train_ds)}")
    print(f"ğŸ“ Valid samples: {len(valid_ds)}")

    # Retriever ì´ˆê¸°í™”
    retriever = initialize_retriever(retriever_type)
    retriever.build()

    max_k = max(topk_list)

    # Train ë¶„ì„
    print(f"\nâ³ Train ë°ì´í„°ì…‹ ë¶„ì„ ì¤‘... (Top-{max_k} ê³„ì‚°)")
    _, train_doc_indices = retriever.get_relevant_doc_bulk(
        train_ds["question"], k=max_k
    )

    print("\n" + "=" * 80)
    print(f"ğŸ“ˆ Train Dataset (n={len(train_ds)})")
    print("=" * 80)
    print(f"{'Top-K':<8} {'Recall@K':<12} {'Match/Total':<15}")
    print("-" * 80)

    # ì •ë‹µ ë¬¸ì„œ ì¶”ì¶œ (í•œ ë²ˆë§Œ)
    train_gold_contexts = [ex["context"] for ex in train_ds]

    for k in topk_list:
        # ê° Kì— ëŒ€í•´ recall ê³„ì‚° (ì´ë¯¸ ê³„ì‚°ëœ indices ìŠ¬ë¼ì´ì‹±)
        hits = 0
        for gold_ctx, indices in zip(train_gold_contexts, train_doc_indices):
            topk_contexts = [retriever.contexts[idx] for idx in indices[:k]]
            if gold_ctx in topk_contexts:
                hits += 1

        recall = hits / len(train_ds)
        print(f"{k:<8} {recall:<12.1%} {hits}/{len(train_ds)}")

    # Validation ë¶„ì„
    print(f"\nâ³ Validation ë°ì´í„°ì…‹ ë¶„ì„ ì¤‘... (Top-{max_k} ê³„ì‚°)")
    _, valid_doc_indices = retriever.get_relevant_doc_bulk(
        valid_ds["question"], k=max_k
    )

    print("\n" + "=" * 80)
    print(f"ğŸ“ˆ Validation Dataset (n={len(valid_ds)})")
    print("=" * 80)
    print(f"{'Top-K':<8} {'Recall@K':<12} {'Match/Total':<15}")
    print("-" * 80)

    # ì •ë‹µ ë¬¸ì„œ ì¶”ì¶œ (í•œ ë²ˆë§Œ)
    valid_gold_contexts = [ex["context"] for ex in valid_ds]

    for k in topk_list:
        hits = 0
        for gold_ctx, indices in zip(valid_gold_contexts, valid_doc_indices):
            topk_contexts = [retriever.contexts[idx] for idx in indices[:k]]
            if gold_ctx in topk_contexts:
                hits += 1

        recall = hits / len(valid_ds)
        print(f"{k:<8} {recall:<12.1%} {hits}/{len(valid_ds)}")

    # ì „ì²´ ë°ì´í„° ë¶„ì„
    full_ds = concatenate_datasets([train_ds, valid_ds])
    print(f"\nâ³ Full ë°ì´í„°ì…‹ ê³„ì‚° ì¤‘...")
    full_doc_indices = train_doc_indices + valid_doc_indices
    full_gold_contexts = train_gold_contexts + valid_gold_contexts

    print("\n" + "=" * 80)
    print(f"ğŸ“ˆ Full Dataset - Train + Valid (n={len(full_ds)})")
    print("=" * 80)
    print(f"{'Top-K':<8} {'Recall@K':<12} {'Match/Total':<15}")
    print("-" * 80)

    for k in topk_list:
        hits = 0
        for gold_ctx, indices in zip(full_gold_contexts, full_doc_indices):
            topk_contexts = [retriever.contexts[idx] for idx in indices[:k]]
            if gold_ctx in topk_contexts:
                hits += 1

        recall = hits / len(full_ds)
        print(f"{k:<8} {recall:<12.1%} {hits}/{len(full_ds)}")

    print("=" * 80)
    print("\nğŸ’¡ í•´ì„:")
    print("  - Recall@K: Question ë˜ì¡Œì„ ë•Œ ì •ë‹µ documentê°€ Top-K ì•ˆì— ìˆëŠ” ë¹„ìœ¨")
    print("  - ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ (Retrieverê°€ ì •ë‹µ ë¬¸ì„œë¥¼ ì˜ ì°¾ìŒ)")
    print("  - Kê°€ í´ìˆ˜ë¡ Recallì€ ì¦ê°€ (ë” ë§ì€ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ë¯€ë¡œ)")


def calculate_recall(
    dataset,
    retriever,
    topk_list: List[int] = [1, 5, 10, 20, 50],
) -> Dict[int, Dict[str, float]]:
    """
    Top-K Recall ê³„ì‚°

    Args:
        dataset: HF Dataset (question, context, id í¬í•¨)
        retriever: build()ëœ retrieval ê°ì²´
        topk_list: ì¸¡ì •í•  K ê°’ë“¤

    Returns:
        {k: {"recall": 0.xx, "match": N, "total": M}, ...}
    """
    max_k = max(topk_list)

    # Retrieval ìˆ˜í–‰ (ìµœëŒ€ Kë¡œ ê²€ìƒ‰)
    print(f"\n[1/3] Retrieving top-{max_k} documents...")
    queries = dataset["question"]
    doc_scores, doc_indices = retriever.get_relevant_doc_bulk(queries, k=max_k)

    # ì •ë‹µ ë¬¸ì„œ ì¶”ì¶œ (gold context)
    print(f"[2/3] Extracting gold contexts...")
    gold_contexts = [ex["context"] for ex in dataset]

    # Recall ê³„ì‚°
    print(f"[3/3] Calculating recall@K...")
    recalls = {k: [] for k in topk_list}

    for i, (gold_ctx, indices) in enumerate(
        tqdm(zip(gold_contexts, doc_indices), total=len(gold_contexts), disable=True)
    ):
        # ê²€ìƒ‰ëœ ë¬¸ì„œë“¤
        retrieved_contexts = [retriever.contexts[idx] for idx in indices]

        # ê° Kì— ëŒ€í•´ ì •ë‹µì´ í¬í•¨ë˜ì—ˆëŠ”ì§€ í™•ì¸
        for k in topk_list:
            topk_contexts = retrieved_contexts[:k]
            # ì •ë‹µ ë¬¸ì„œê°€ top-k ì•ˆì— ìˆìœ¼ë©´ 1, ì—†ìœ¼ë©´ 0
            hit = int(gold_ctx in topk_contexts)
            recalls[k].append(hit)

    # ê²°ê³¼ êµ¬ì¡°í™”
    results = {}
    total_samples = len(gold_contexts)
    for k in topk_list:
        match_count = int(np.sum(recalls[k]))
        recall = match_count / total_samples if total_samples > 0 else 0.0
        results[k] = {
            "recall": recall,
            "match": match_count,
            "total": total_samples,
        }

    return results


def main():
    parser = argparse.ArgumentParser(
        description="Retrieval Top-K Recall í…ŒìŠ¤íŠ¸ - KoE5 vs TF-IDF ì„±ëŠ¥ ë¹„êµ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  # KoE5 Dense Retrieval í…ŒìŠ¤íŠ¸ (validation)
  python tests/test_retrieval_recall.py --retriever koe5 --topk 1,5,10,20,50
  
  # TF-IDF Sparse Retrieval í…ŒìŠ¤íŠ¸ (validation)
  python tests/test_retrieval_recall.py --retriever tfidf --topk 1,5,10,20,50
  
  # Train splitìœ¼ë¡œ í…ŒìŠ¤íŠ¸
  python tests/test_retrieval_recall.py --retriever koe5 --split train --topk 1,5,10,20,50,100
  
  # ì „ì²´ ë°ì´í„°ì…‹ ë¶„ì„ (train + validation)
  python tests/test_retrieval_recall.py --retriever koe5 --analyze_full

ì¶œë ¥ í•´ì„:
  - Recall@K: ì •ë‹µ ë¬¸ì„œê°€ Top-K ì•ˆì— í¬í•¨ëœ ë¹„ìœ¨
  - Match/Total: ì •ë‹µì„ ì°¾ì€ ê°œìˆ˜ / ì „ì²´ ìƒ˜í”Œ ìˆ˜
  - Kê°€ í´ìˆ˜ë¡ Recallì€ ì¦ê°€ (ë” ë§ì€ ë¬¸ì„œë¥¼ ê²€ìƒ‰)
        """,
    )
    parser.add_argument(
        "--retriever",
        type=str,
        default="koe5",
        choices=["koe5", "tfidf"],
        help="Retrieval method (koe5: dense KoE5, tfidf: sparse TF-IDF)",
    )
    parser.add_argument(
        "--topk",
        type=str,
        default="1,5,10,20,50",
        help="Top-K values to evaluate (comma-separated, e.g., '1,5,10,20,50')",
    )
    parser.add_argument(
        "--dataset",
        type=str,
        default="./data/train_dataset",
        help="Dataset path (HF datasets format)",
    )
    parser.add_argument(
        "--split",
        type=str,
        default="validation",
        choices=["train", "validation"],
        help="Dataset split to evaluate",
    )
    parser.add_argument(
        "--analyze_full",
        action="store_true",
        help="ì „ì²´ train+valid ë°ì´í„°ì…‹ìœ¼ë¡œ recall@k ë¶„ì„ ìˆ˜í–‰ (retrieval_sanity.py ìŠ¤íƒ€ì¼)",
    )

    args = parser.parse_args()

    # Top-K íŒŒì‹±
    if args.analyze_full:
        topk_list = [1, 5, 10, 20, 50, 100]
    else:
        topk_list = sorted([int(k.strip()) for k in args.topk.split(",")])

    # Config ì¶œë ¥
    print("=" * 40)
    print("Retrieval Recall Test Config")
    print("=" * 40)
    print(f"retriever       = {args.retriever}")
    print(f"dataset         = {args.dataset}")
    print(f"split           = {args.split}")
    print(f"topk            = {topk_list}")
    print(f"analyze_full    = {args.analyze_full}")
    print("=" * 40)
    print()

    # ì „ì²´ ë°ì´í„°ì…‹ ë¶„ì„ ëª¨ë“œ
    if args.analyze_full:
        analyze_full_dataset(
            retriever_type=args.retriever,
            dataset_path=args.dataset,
            topk_list=topk_list,
        )
        return

    # ë‹¨ì¼ split ë¶„ì„ ëª¨ë“œ
    print("=" * 80)
    print(f"ğŸ” RETRIEVAL RECALL EVALUATION ({args.retriever.upper()})")
    print("=" * 80)
    print(f"Dataset  : {args.dataset} ({args.split} split)")
    print(f"Top-K    : {topk_list}")
    print("=" * 80)

    # 1. ë°ì´í„°ì…‹ ë¡œë“œ
    print(f"\n[LOAD] Loading dataset...")
    datasets = load_from_disk(args.dataset)
    eval_dataset = datasets[args.split]
    print(f"   âœ“ Loaded {len(eval_dataset)} examples")

    # 2. Retriever ì´ˆê¸°í™”
    print(f"\n[BUILD] Initializing {args.retriever.upper()} retriever...")
    start_time = time.time()
    retriever = initialize_retriever(args.retriever)
    retriever.build()
    build_time = time.time() - start_time
    print(f"   âœ“ Build completed in {build_time:.2f}s")

    # 3. Recall ê³„ì‚°
    print(f"\n[EVAL] Calculating recall...")
    start_time = time.time()
    results = calculate_recall(eval_dataset, retriever, topk_list)
    eval_time = time.time() - start_time

    # 4. ê²°ê³¼ ì¶œë ¥ (retrieval_sanity.py ìŠ¤íƒ€ì¼)
    total_samples = results[topk_list[0]]["total"]

    print("\n" + "=" * 80)
    print(f"ğŸ“Š {args.split.capitalize()} Dataset (n={total_samples})")
    print("=" * 80)
    print(f"{'Top-K':<8} {'Recall@K':<12} {'Match/Total':<15}")
    print("-" * 80)

    for k in topk_list:
        recall = results[k]["recall"]
        match = results[k]["match"]
        total = results[k]["total"]
        print(f"{k:<8} {recall:<12.1%} {match}/{total}")

    print("=" * 80)
    print(
        f"â±ï¸  Evaluation time: {eval_time:.2f}s ({eval_time / total_samples * 1000:.1f}ms per query)"
    )
    print("=" * 80)


if __name__ == "__main__":
    main()
