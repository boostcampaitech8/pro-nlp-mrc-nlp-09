#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Retrieval ìºì‹œ Sanity Check ìŠ¤í¬ë¦½íŠ¸

train/val/test retrieval ìºì‹œ íŒŒì¼ì˜ ë¬´ê²°ì„±ì„ ê²€ì¦í•©ë‹ˆë‹¤.

Usage:
    python -m tests.sanity_cache
    python -m tests.sanity_cache --split val
    python -m tests.sanity_cache --all

Checks:
    1. ìºì‹œ íŒŒì¼ ì¡´ì¬ ë° ë¡œë“œ ê°€ëŠ¥ ì—¬ë¶€
    2. ìºì‹œ í•­ëª© êµ¬ì¡° ê²€ì¦ (id, question, retrieved)
    3. Retrieved í›„ë³´ êµ¬ì¡° ê²€ì¦ (passage_id, doc_id, score_dense, score_bm25)
    4. Score ë¶„í¬ í†µê³„
    5. Passage ID ë²”ìœ„ ê²€ì¦ (embeddingê³¼ ì¼ì¹˜í•˜ëŠ”ì§€)
"""

import argparse
import json
import os
import sys
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import numpy as np

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
ROOT_DIR = Path(__file__).parent.parent
sys.path.insert(0, str(ROOT_DIR))

from src.retrieval.paths import get_path


def load_cache(cache_path: str) -> List[Dict]:
    """ìºì‹œ íŒŒì¼ ë¡œë“œ."""
    cache = []
    with open(cache_path, "r", encoding="utf-8") as f:
        for line in f:
            cache.append(json.loads(line.strip()))
    return cache


def check_cache_file(
    cache_path: str, max_passage_id: Optional[int] = None
) -> Tuple[bool, Dict]:
    """
    ìºì‹œ íŒŒì¼ ê²€ì¦.

    Args:
        cache_path: ìºì‹œ íŒŒì¼ ê²½ë¡œ
        max_passage_id: passage_id ìµœëŒ€ê°’ (embedding í¬ê¸° - 1)

    Returns:
        (success, results_dict)
    """
    results = {
        "file_exists": False,
        "num_questions": 0,
        "candidates_per_question": None,
        "structure_valid": False,
        "score_stats": None,
        "passage_id_range": None,
        "passage_id_valid": None,
        "sample_entry": None,
    }

    required_entry_fields = ["id", "question", "retrieved"]
    required_candidate_fields = ["passage_id", "doc_id", "score_dense", "score_bm25"]

    if not os.path.exists(cache_path):
        return False, results

    results["file_exists"] = True

    try:
        cache = load_cache(cache_path)
    except Exception as e:
        results["error"] = str(e)
        return False, results

    results["num_questions"] = len(cache)

    if len(cache) == 0:
        return False, results

    # êµ¬ì¡° ê²€ì¦
    first = cache[0]
    entry_fields_ok = all(field in first for field in required_entry_fields)

    if entry_fields_ok and len(first["retrieved"]) > 0:
        cand_fields_ok = all(
            field in first["retrieved"][0] for field in required_candidate_fields
        )
    else:
        cand_fields_ok = False

    results["structure_valid"] = entry_fields_ok and cand_fields_ok

    # í›„ë³´ ìˆ˜ í™•ì¸
    candidates_counts = [len(item["retrieved"]) for item in cache]
    results["candidates_per_question"] = {
        "min": min(candidates_counts),
        "max": max(candidates_counts),
        "mean": np.mean(candidates_counts),
    }

    # Score í†µê³„
    all_bm25 = []
    all_dense = []
    all_passage_ids = []

    for item in cache:
        for cand in item["retrieved"]:
            all_bm25.append(cand["score_bm25"])
            all_dense.append(cand["score_dense"])
            all_passage_ids.append(cand["passage_id"])

    results["score_stats"] = {
        "bm25": {
            "min": float(np.min(all_bm25)),
            "max": float(np.max(all_bm25)),
            "mean": float(np.mean(all_bm25)),
            "std": float(np.std(all_bm25)),
        },
        "dense": {
            "min": float(np.min(all_dense)),
            "max": float(np.max(all_dense)),
            "mean": float(np.mean(all_dense)),
            "std": float(np.std(all_dense)),
        },
    }

    # Passage ID ë²”ìœ„
    results["passage_id_range"] = {
        "min": min(all_passage_ids),
        "max": max(all_passage_ids),
    }

    # Passage ID ìœ íš¨ì„± (embedding í¬ê¸°ì™€ ë¹„êµ)
    if max_passage_id is not None:
        results["passage_id_valid"] = max(all_passage_ids) <= max_passage_id

    # ìƒ˜í”Œ í•­ëª©
    sample_entry = {
        "id": first["id"],
        "question": first["question"][:80] + "..."
        if len(first["question"]) > 80
        else first["question"],
        "num_retrieved": len(first["retrieved"]),
        "first_candidate": first["retrieved"][0] if first["retrieved"] else None,
    }
    results["sample_entry"] = sample_entry

    success = results["structure_valid"]
    if results["passage_id_valid"] is not None:
        success = success and results["passage_id_valid"]

    return success, results


def run_cache_sanity_check(
    splits: List[str] = None,
    cache_dir: str = None,
    embedding_path: str = None,
    verbose: bool = True,
) -> Dict:
    """
    ìºì‹œ sanity check ì‹¤í–‰.

    Args:
        splits: ì²´í¬í•  split ëª©ë¡ (ê¸°ë³¸: ["train", "val", "test"])
        cache_dir: ìºì‹œ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        embedding_path: ì„ë² ë”© íŒŒì¼ ê²½ë¡œ (passage_id ê²€ì¦ìš©)
        verbose: ìƒì„¸ ì¶œë ¥ ì—¬ë¶€

    Returns:
        ì¢…í•© ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    if splits is None:
        splits = ["train", "val", "test"]

    if cache_dir is None:
        cache_dir = get_path("retrieval_cache_dir")

    # Max passage ID í™•ì¸
    max_passage_id = None
    if embedding_path is None:
        embedding_path = get_path("kure_corpus_emb")

    if os.path.exists(embedding_path):
        emb = np.load(embedding_path)
        max_passage_id = emb.shape[0] - 1

    results = {
        "cache_dir": cache_dir,
        "max_passage_id": max_passage_id,
        "splits": {},
        "overall_pass": True,
    }

    if verbose:
        print("=" * 70)
        print("ğŸ“Š Retrieval Cache Sanity Check")
        print("=" * 70)
        if max_passage_id is not None:
            print(f"Max passage_id (from embedding): {max_passage_id}")

    for i, split in enumerate(splits):
        cache_path = os.path.join(cache_dir, f"{split}_top50.jsonl")

        if verbose:
            print(f"\n[{i + 1}/{len(splits)}] {split.upper()}: {cache_path}")
            print("-" * 50)

        success, split_results = check_cache_file(cache_path, max_passage_id)
        results["splits"][split] = split_results

        if not success:
            results["overall_pass"] = False

        if verbose:
            if split_results["file_exists"]:
                print(f"      Questions: {split_results['num_questions']}")
                if split_results["candidates_per_question"]:
                    cpc = split_results["candidates_per_question"]
                    print(
                        f"      Candidates/question: {cpc['min']}-{cpc['max']} (mean={cpc['mean']:.1f})"
                    )
                print(
                    f"      Structure valid: {'âœ…' if split_results['structure_valid'] else 'âŒ'}"
                )

                if split_results["score_stats"]:
                    bm25 = split_results["score_stats"]["bm25"]
                    dense = split_results["score_stats"]["dense"]
                    print(
                        f"      BM25 scores: [{bm25['min']:.2f}, {bm25['max']:.2f}], mean={bm25['mean']:.2f}"
                    )
                    print(
                        f"      Dense scores: [{dense['min']:.3f}, {dense['max']:.3f}], mean={dense['mean']:.3f}"
                    )

                if split_results["passage_id_range"]:
                    pid = split_results["passage_id_range"]
                    print(f"      Passage ID range: [{pid['min']}, {pid['max']}]")

                if split_results["passage_id_valid"] is not None:
                    print(
                        f"      Passage ID valid: {'âœ…' if split_results['passage_id_valid'] else 'âŒ'}"
                    )

                print(f"      â†’ {'âœ… PASS' if success else 'âŒ FAIL'}")
            else:
                print(f"      â†’ âŒ FAIL (file not found)")

    if verbose:
        print("\n" + "=" * 70)
        if results["overall_pass"]:
            print("âœ… All Cache Sanity Checks PASSED")
        else:
            print("âŒ Some Cache Sanity Checks FAILED")
        print("=" * 70)

    return results


def main():
    parser = argparse.ArgumentParser(description="Retrieval Cache Sanity Check")
    parser.add_argument(
        "--splits",
        nargs="+",
        default=None,
        help="Splits to check (default: train val test)",
    )
    parser.add_argument(
        "--all",
        action="store_true",
        help="Check all splits",
    )
    parser.add_argument(
        "--cache_dir",
        type=str,
        default=None,
        help="Cache directory path",
    )
    parser.add_argument(
        "--embedding_path",
        type=str,
        default=None,
        help="Embedding file path for passage_id validation",
    )
    parser.add_argument(
        "--quiet",
        action="store_true",
        help="Suppress verbose output",
    )
    parser.add_argument(
        "--json",
        action="store_true",
        help="Output results as JSON",
    )

    args = parser.parse_args()

    splits = args.splits
    if args.all or splits is None:
        splits = ["train", "val", "test"]

    results = run_cache_sanity_check(
        splits=splits,
        cache_dir=args.cache_dir,
        embedding_path=args.embedding_path,
        verbose=not args.quiet and not args.json,
    )

    if args.json:
        import json as json_module

        print(json_module.dumps(results, indent=2, ensure_ascii=False))

    return 0 if results["overall_pass"] else 1


if __name__ == "__main__":
    sys.exit(main())
