#!/usr/bin/env python3
"""
Retrieval ì¼ê´€ì„± ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸

BaseRetrieval ë¦¬íŒ©í† ë§ í›„ ë‹¤ìŒì„ ê²€ì¦í•©ë‹ˆë‹¤:
1. Corpus ë¡œë”©: contexts, ids, titles ê°œìˆ˜ ì¼ì¹˜
2. Embedding í¬ê¸°: p_embedding.shape[0] == len(contexts)
3. ì¸ë±ìŠ¤ ì•ˆì „ì„±: doc_indicesì˜ ëª¨ë“  ê°’ì´ contexts ë²”ìœ„ ë‚´
4. ë‹¨ì¼/ë²Œí¬ ì¿¼ë¦¬ ì¼ê´€ì„±: get_relevant_docì™€ get_relevant_doc_bulk ê²°ê³¼ ë™ì¼
5. ids-titles ì •ë ¬: contexts[i] <-> ids[i] <-> titles[i] ë§¤í•‘ ì¼ì¹˜

Usage:
    python tests/verify_retrieval_consistency.py --retrieval_type sparse
"""

import argparse
import sys
import os

# MRC í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€
sys.path.insert(0, os.path.join(os.path.dirname(__file__), ".."))

from transformers import AutoTokenizer
from src.retrieval.sparse import SparseRetrieval


def verify_corpus_consistency(retriever):
    """ì½”í¼ìŠ¤ ë¡œë”© ì¼ê´€ì„± ê²€ì¦"""
    print("\n" + "=" * 60)
    print("ğŸ” 1. Corpus ë¡œë”© ì¼ê´€ì„± ê²€ì¦")
    print("=" * 60)

    contexts_len = len(retriever.contexts)
    ids_len = len(retriever.ids)
    titles_len = len(retriever.titles)

    print(f"âœ“ Contexts ê°œìˆ˜: {contexts_len}")
    print(f"âœ“ IDs ê°œìˆ˜: {ids_len}")
    print(f"âœ“ Titles ê°œìˆ˜: {titles_len}")

    if contexts_len == ids_len == titles_len:
        print("âœ… ëª¨ë“  ë°°ì—´ ê¸¸ì´ ì¼ì¹˜!")
        return True
    else:
        print("âŒ ë°°ì—´ ê¸¸ì´ ë¶ˆì¼ì¹˜ ê°ì§€!")
        return False


def verify_embedding_size(retriever):
    """Embedding í¬ê¸° ê²€ì¦"""
    print("\n" + "=" * 60)
    print("ğŸ” 2. Embedding í¬ê¸° ê²€ì¦")
    print("=" * 60)

    if not hasattr(retriever, "p_embedding") or retriever.p_embedding is None:
        print("âš ï¸  Embeddingì´ ì•„ì§ ë¹Œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. build() í˜¸ì¶œ ì¤‘...")
        retriever.build()

    emb_size = retriever.p_embedding.shape[0]
    ctx_size = len(retriever.contexts)

    print(f"âœ“ Embedding shape: {retriever.p_embedding.shape}")
    print(f"âœ“ Contexts ê°œìˆ˜: {ctx_size}")

    if emb_size == ctx_size:
        print("âœ… Embedding í¬ê¸°ì™€ contexts ê°œìˆ˜ ì¼ì¹˜!")
        return True
    else:
        print(f"âŒ í¬ê¸° ë¶ˆì¼ì¹˜! Embedding({emb_size}) != Contexts({ctx_size})")
        return False


def verify_index_safety(retriever, num_samples=50):
    """ì¸ë±ìŠ¤ ë²”ìœ„ ì•ˆì „ì„± ê²€ì¦"""
    print("\n" + "=" * 60)
    print("ğŸ” 3. ì¸ë±ìŠ¤ ë²”ìœ„ ì•ˆì „ì„± ê²€ì¦")
    print("=" * 60)

    test_queries = [
        "ëŒ€í†µë ¹ì„ í¬í•¨í•œ ë¯¸êµ­ì˜ í–‰ì •ë¶€ ê²¬ì œê¶Œì„ ê°–ëŠ” êµ­ê°€ ê¸°ê´€ì€?",
        "ë©•ì‹œì½”ì˜ ìˆ˜ë„ëŠ”?",
        "í•œêµ­ì˜ ì „í†µ ìŒì‹ì€?",
        "íŒŒì´ì¬ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì˜ íŠ¹ì§•ì€?",
        "ì„¸ê³„ì—ì„œ ê°€ì¥ ë†’ì€ ì‚°ì€?",
    ]

    max_contexts_idx = len(retriever.contexts) - 1
    print(f"âœ“ Contexts ìµœëŒ€ ì¸ë±ìŠ¤: {max_contexts_idx}")

    all_safe = True
    for query in test_queries[:num_samples]:
        _, doc_indices = retriever.get_relevant_doc(query, k=10)

        max_idx = max(doc_indices)
        if max_idx > max_contexts_idx:
            print(f"âŒ ì¸ë±ìŠ¤ ë²”ìœ„ ì´ˆê³¼ ê°ì§€! max_idx={max_idx} > {max_contexts_idx}")
            print(f"   Query: {query[:50]}...")
            all_safe = False
            break

    if all_safe:
        print(f"âœ… ëª¨ë“  ì¿¼ë¦¬ì—ì„œ ì¸ë±ìŠ¤ ë²”ìœ„ ì•ˆì „ (í…ŒìŠ¤íŠ¸ {len(test_queries)}ê°œ)")
        return True
    else:
        return False


def verify_single_bulk_consistency(retriever):
    """ë‹¨ì¼ ì¿¼ë¦¬ì™€ ë²Œí¬ ì¿¼ë¦¬ ê²°ê³¼ ì¼ê´€ì„± ê²€ì¦"""
    print("\n" + "=" * 60)
    print("ğŸ” 4. ë‹¨ì¼/ë²Œí¬ ì¿¼ë¦¬ ì¼ê´€ì„± ê²€ì¦")
    print("=" * 60)

    test_query = "ëŒ€í†µë ¹ì„ í¬í•¨í•œ ë¯¸êµ­ì˜ í–‰ì •ë¶€ ê²¬ì œê¶Œì„ ê°–ëŠ” êµ­ê°€ ê¸°ê´€ì€?"
    k = 5

    # ë‹¨ì¼ ì¿¼ë¦¬
    scores_single, indices_single = retriever.get_relevant_doc(test_query, k=k)

    # ë²Œí¬ ì¿¼ë¦¬ (ê¸¸ì´ 1)
    scores_bulk, indices_bulk = retriever.get_relevant_doc_bulk([test_query], k=k)
    scores_bulk = scores_bulk[0]
    indices_bulk = indices_bulk[0]

    print(f"âœ“ ë‹¨ì¼ ì¿¼ë¦¬ ê²°ê³¼: scores={scores_single[:3]}, indices={indices_single[:3]}")
    print(f"âœ“ ë²Œí¬ ì¿¼ë¦¬ ê²°ê³¼: scores={scores_bulk[:3]}, indices={indices_bulk[:3]}")

    if scores_single == scores_bulk and indices_single == indices_bulk:
        print("âœ… ë‹¨ì¼/ë²Œí¬ ì¿¼ë¦¬ ê²°ê³¼ ì™„ì „ ì¼ì¹˜!")
        return True
    else:
        print("âŒ ê²°ê³¼ ë¶ˆì¼ì¹˜ ê°ì§€!")
        return False


def verify_ids_titles_mapping(retriever, num_samples=10):
    """contexts-ids-titles ë§¤í•‘ ì¼ê´€ì„± ê²€ì¦"""
    print("\n" + "=" * 60)
    print("ğŸ” 5. Contexts-IDs-Titles ë§¤í•‘ ê²€ì¦")
    print("=" * 60)

    print(f"âœ“ {num_samples}ê°œ ìƒ˜í”Œ ê²€ì¦ ì¤‘...\n")

    import random

    sample_indices = random.sample(range(len(retriever.contexts)), num_samples)

    for i in sample_indices:
        ctx = retriever.contexts[i][:50]  # ì• 50ìë§Œ
        doc_id = retriever.ids[i]
        title = retriever.titles[i]

        print(f"[{i}] doc_id={doc_id}, title='{title}', context='{ctx}...'")

    print("\nâœ… ë§¤í•‘ ìƒ˜í”Œ ì¶œë ¥ ì™„ë£Œ (ìˆ˜ë™ ê²€ì¦ í•„ìš”)")
    return True


def main():
    parser = argparse.ArgumentParser(description="Retrieval ì¼ê´€ì„± ê²€ì¦")
    parser.add_argument(
        "--retrieval_type",
        type=str,
        default="sparse",
        choices=["sparse", "dense"],
        help="ê²€ì¦í•  retrieval íƒ€ì…",
    )
    parser.add_argument("--data_path", type=str, default="./data", help="ë°ì´í„° ê²½ë¡œ")
    parser.add_argument(
        "--context_path",
        type=str,
        default="wikipedia_documents.json",
        help="Wikipedia ë¬¸ì„œ íŒŒì¼ëª…",
    )
    parser.add_argument(
        "--model_name", type=str, default="klue/roberta-large", help="Tokenizer ëª¨ë¸ëª…"
    )

    args = parser.parse_args()

    print("\n" + "=" * 60)
    print("ğŸš€ Retrieval ì¼ê´€ì„± ê²€ì¦ ì‹œì‘")
    print("=" * 60)
    print(f"Retrieval Type: {args.retrieval_type}")
    print(f"Data Path: {args.data_path}")
    print(f"Context Path: {args.context_path}")

    # Retriever ì´ˆê¸°í™”
    if args.retrieval_type == "sparse":
        tokenizer = AutoTokenizer.from_pretrained(args.model_name, use_fast=False)
        retriever = SparseRetrieval(
            tokenize_fn=tokenizer.tokenize,
            data_path=args.data_path,
            context_path=args.context_path,
        )
        retriever.build()
    else:
        raise NotImplementedError("Dense retrievalì€ ì•„ì§ êµ¬í˜„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    # ê²€ì¦ ì‹¤í–‰
    results = {}
    results["corpus"] = verify_corpus_consistency(retriever)
    results["embedding"] = verify_embedding_size(retriever)
    results["index_safety"] = verify_index_safety(retriever)
    results["single_bulk"] = verify_single_bulk_consistency(retriever)
    results["mapping"] = verify_ids_titles_mapping(retriever)

    # ìµœì¢… ê²°ê³¼
    print("\n" + "=" * 60)
    print("ğŸ“Š ìµœì¢… ê²€ì¦ ê²°ê³¼")
    print("=" * 60)

    all_passed = all(results.values())

    for test_name, passed in results.items():
        status = "âœ… PASS" if passed else "âŒ FAIL"
        print(f"{status} - {test_name}")

    print("=" * 60)
    if all_passed:
        print("ğŸ‰ ëª¨ë“  ê²€ì¦ í†µê³¼!")
        return 0
    else:
        print("âš ï¸  ì¼ë¶€ ê²€ì¦ ì‹¤íŒ¨. ìœ„ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        return 1


if __name__ == "__main__":
    sys.exit(main())
