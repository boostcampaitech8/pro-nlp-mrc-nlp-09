"""
Retrieval ê´€ë ¨ íŒŒì¼ ê²½ë¡œ ì¤‘ì•™ ê´€ë¦¬ ëª¨ë“ˆ

ëª¨ë“  ì„ë² ë”©, ì¸ë±ìŠ¤, ìºì‹œ íŒŒì¼ì˜ ê²½ë¡œë¥¼ ì´ê³³ì—ì„œ í†µí•© ê´€ë¦¬í•©ë‹ˆë‹¤.
ê° ëª¨ë“ˆì—ì„œ í•˜ë“œì½”ë”©ëœ ê²½ë¡œ ëŒ€ì‹  ì´ ëª¨ë“ˆì˜ ìƒìˆ˜ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.

Usage:
    from src.retrieval.paths import PATHS, get_path

    # ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
    kure_emb_path = get_path("kure_corpus_emb")
    bm25_index_path = get_path("bm25_index")

    # ì „ì²´ ê²½ë¡œ ë”•ì…”ë„ˆë¦¬
    print(PATHS)
"""

import os
from pathlib import Path
from typing import Optional


# ============================================================
# ê¸°ë³¸ ê²½ë¡œ ì„¤ì •
# ============================================================

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ (ì´ íŒŒì¼ ê¸°ì¤€ìœ¼ë¡œ 3ë‹¨ê³„ ìƒìœ„)
PROJECT_ROOT = Path(__file__).parent.parent.parent.resolve()

# ë°ì´í„° ë£¨íŠ¸
DATA_ROOT = PROJECT_ROOT / "data"

# í•˜ìœ„ ë””ë ‰í† ë¦¬ë“¤
EMBEDDINGS_DIR = DATA_ROOT / "embeddings"  # Dense ì„ë² ë”© ì €ì¥
INDICES_DIR = DATA_ROOT / "indices"  # Sparse/Dense ì¸ë±ìŠ¤
CACHE_DIR = DATA_ROOT / "cache"  # Retrieval ìºì‹œ (JSONL)


# ============================================================
# íŒŒì¼ ê²½ë¡œ ì •ì˜ (ë‹¨ì¼ ì§„ì‹¤ì˜ ì›ì²œ)
# ============================================================

PATHS = {
    # === Wikipedia Corpus ===
    "wiki_corpus": str(DATA_ROOT / "wikipedia_documents.json"),
    # === Dense Embeddings (Sentence Transformers) ===
    # KoE5 (ê¸°ì¡´)
    "koe5_corpus_emb": str(EMBEDDINGS_DIR / "koe5_corpus_emb.npy"),
    # KURE-v1 (ì‹ ê·œ)
    "kure_corpus_emb": str(EMBEDDINGS_DIR / "kure_corpus_emb.npy"),
    "kure_passages_meta": str(EMBEDDINGS_DIR / "kure_passages_meta.jsonl"),
    # === Sparse Indices (BM25) ===
    "bm25_index_dir": str(INDICES_DIR / "bm25"),
    "bm25_model": str(INDICES_DIR / "bm25" / "bm25_model.pkl"),
    # === TF-IDF (Legacy) ===
    "tfidf_embedding": str(INDICES_DIR / "sparse" / "sparse_embedding.bin"),
    "tfidf_vectorizer": str(INDICES_DIR / "sparse" / "tfidv.bin"),
    # === Retrieval Cache (Dynamic Hard Negativeìš©) ===
    "retrieval_cache_dir": str(CACHE_DIR / "retrieval"),
    "train_cache": str(CACHE_DIR / "retrieval" / "train_top50.jsonl"),
    "val_cache": str(CACHE_DIR / "retrieval" / "val_top50.jsonl"),
    "test_cache": str(CACHE_DIR / "retrieval" / "test_top50.jsonl"),
    # === Dataset Paths ===
    "train_dataset": str(DATA_ROOT / "train_dataset"),
    "test_dataset": str(DATA_ROOT / "test_dataset"),
}

# Output ë””ë ‰í† ë¦¬ (ë™ì ìœ¼ë¡œ ê²°ì •ë¨)
OUTPUT_ROOT = PROJECT_ROOT / "outputs"


# ============================================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ============================================================


def get_path(key: str) -> str:
    """
    ê²½ë¡œ í‚¤ë¡œ íŒŒì¼ ê²½ë¡œë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.

    Args:
        key: PATHS ë”•ì…”ë„ˆë¦¬ì˜ í‚¤

    Returns:
        í•´ë‹¹ íŒŒì¼/ë””ë ‰í† ë¦¬ì˜ ì ˆëŒ€ ê²½ë¡œ

    Raises:
        KeyError: ì¡´ì¬í•˜ì§€ ì•ŠëŠ” í‚¤
    """
    if key not in PATHS:
        available = ", ".join(sorted(PATHS.keys()))
        raise KeyError(f"Unknown path key: '{key}'. Available: {available}")
    return PATHS[key]


def ensure_dir(key: str) -> str:
    """
    ê²½ë¡œì˜ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ê³ , ì—†ìœ¼ë©´ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        key: PATHS ë”•ì…”ë„ˆë¦¬ì˜ í‚¤

    Returns:
        í•´ë‹¹ ê²½ë¡œ (ë””ë ‰í† ë¦¬ê°€ ìƒì„±ëœ ìƒíƒœ)
    """
    path = get_path(key)

    # íŒŒì¼ ê²½ë¡œì¸ ê²½ìš° ìƒìœ„ ë””ë ‰í† ë¦¬ ìƒì„±
    if "." in os.path.basename(path):
        os.makedirs(os.path.dirname(path), exist_ok=True)
    else:
        os.makedirs(path, exist_ok=True)

    return path


def get_analysis_dir(output_dir: str, subdir: str = "val_analysis") -> Path:
    """
    ë¶„ì„ ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

    Args:
        output_dir: ëª¨ë¸ ì¶œë ¥ ë””ë ‰í† ë¦¬ (ì˜ˆ: ./outputs/dahyeong/model_name)
        subdir: í•˜ìœ„ ë””ë ‰í† ë¦¬ ì´ë¦„ (ê¸°ë³¸: val_analysis)

    Returns:
        ë¶„ì„ ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ Path (ì—†ìœ¼ë©´ ìƒì„±)
    """
    analysis_dir = Path(output_dir) / subdir
    analysis_dir.mkdir(parents=True, exist_ok=True)
    return analysis_dir


def exists(key: str) -> bool:
    """
    í•´ë‹¹ ê²½ë¡œì˜ íŒŒì¼/ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.

    Args:
        key: PATHS ë”•ì…”ë„ˆë¦¬ì˜ í‚¤

    Returns:
        ì¡´ì¬ ì—¬ë¶€
    """
    return os.path.exists(get_path(key))


def get_file_size(key: str) -> Optional[str]:
    """
    íŒŒì¼ í¬ê¸°ë¥¼ ì‚¬ëŒì´ ì½ê¸° ì¢‹ì€ í˜•íƒœë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.

    Args:
        key: PATHS ë”•ì…”ë„ˆë¦¬ì˜ í‚¤

    Returns:
        íŒŒì¼ í¬ê¸° ë¬¸ìì—´ (ì˜ˆ: "231.5 MB") ë˜ëŠ” None (íŒŒì¼ ì—†ìŒ)
    """
    path = get_path(key)
    if not os.path.exists(path):
        return None

    size = os.path.getsize(path)
    for unit in ["B", "KB", "MB", "GB"]:
        if size < 1024:
            return f"{size:.1f} {unit}"
        size /= 1024
    return f"{size:.1f} TB"


def print_status():
    """
    ëª¨ë“  ê´€ë¦¬ë˜ëŠ” íŒŒì¼ë“¤ì˜ ì¡´ì¬ ì—¬ë¶€ì™€ í¬ê¸°ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.
    """
    print("\n" + "=" * 70)
    print("ğŸ“ Retrieval Files Status")
    print("=" * 70)

    categories = {
        "Dense Embeddings": [
            "koe5_corpus_emb",
            "kure_corpus_emb",
            "kure_passages_meta",
        ],
        "Sparse Indices": ["bm25_index_dir", "tfidf_embedding", "tfidf_vectorizer"],
        "Retrieval Cache": ["train_cache", "val_cache", "test_cache"],
        "Corpus": ["wiki_corpus"],
    }

    for category, keys in categories.items():
        print(f"\nğŸ“‚ {category}")
        print("-" * 50)
        for key in keys:
            path = get_path(key)
            if os.path.exists(path):
                size = get_file_size(key) or "DIR"
                print(f"  âœ… {key}: {size}")
                print(f"     â””â”€ {path}")
            else:
                print(f"  âŒ {key}: NOT FOUND")
                print(f"     â””â”€ {path}")

    print("\n" + "=" * 70)


# ============================================================
# ë§ˆì´ê·¸ë ˆì´ì…˜ í—¬í¼ (ê¸°ì¡´ ê²½ë¡œì—ì„œ ìƒˆ ê²½ë¡œë¡œ ì´ë™)
# ============================================================

LEGACY_PATHS = {
    # ê¸°ì¡´ ê²½ë¡œ -> ìƒˆ ê²½ë¡œ í‚¤ ë§¤í•‘
    "./data/koe5_corpus_emb.npy": "koe5_corpus_emb",
    "./data/kure_corpus_emb.npy": "kure_corpus_emb",
    "./data/kure_passages_meta.jsonl": "kure_passages_meta",
    "./data/indices/dense/koe5_corpus_emb.npy": "koe5_corpus_emb",
    "./data/retrieval_cache/train_top50.jsonl": "train_cache",
    "./data/retrieval_cache/val_top50.jsonl": "val_cache",
    "./data/retrieval_cache/test_top50.jsonl": "test_cache",
}


def migrate_legacy_files(dry_run: bool = True) -> None:
    """
    ê¸°ì¡´ ê²½ë¡œì˜ íŒŒì¼ë“¤ì„ ìƒˆ ê²½ë¡œë¡œ ì´ë™í•©ë‹ˆë‹¤.

    Args:
        dry_run: Trueë©´ ì‹¤ì œ ì´ë™ ì—†ì´ ê³„íšë§Œ ì¶œë ¥
    """
    import shutil

    print("\n" + "=" * 70)
    print("ğŸ”„ Legacy File Migration")
    print(f"   Mode: {'DRY RUN' if dry_run else 'EXECUTE'}")
    print("=" * 70)

    for legacy_path, new_key in LEGACY_PATHS.items():
        abs_legacy = str(PROJECT_ROOT / legacy_path.lstrip("./"))
        new_path = get_path(new_key)

        if os.path.exists(abs_legacy):
            if abs_legacy == new_path:
                print(f"  â­ï¸  SAME: {legacy_path}")
                continue

            if os.path.exists(new_path):
                print(f"  âš ï¸  CONFLICT: {legacy_path}")
                print(f"      Both exist! Manual resolution needed.")
                continue

            print(f"  ğŸ“¦ MOVE: {legacy_path}")
            print(f"      â†’ {new_path}")

            if not dry_run:
                os.makedirs(os.path.dirname(new_path), exist_ok=True)
                shutil.move(abs_legacy, new_path)
                print(f"      âœ… Done")
        else:
            print(f"  â­ï¸  SKIP: {legacy_path} (not found)")

    print("\n" + "=" * 70)


# ============================================================
# CLI
# ============================================================

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Retrieval paths management")
    parser.add_argument("--status", action="store_true", help="Show file status")
    parser.add_argument(
        "--migrate", action="store_true", help="Migrate legacy files (dry run)"
    )
    parser.add_argument(
        "--migrate-execute", action="store_true", help="Actually migrate files"
    )

    args = parser.parse_args()

    if args.status:
        print_status()
    elif args.migrate:
        migrate_legacy_files(dry_run=True)
    elif args.migrate_execute:
        migrate_legacy_files(dry_run=False)
    else:
        print("Available paths:")
        for key, path in sorted(PATHS.items()):
            status = "âœ…" if os.path.exists(path) else "âŒ"
            print(f"  {status} {key}: {path}")
