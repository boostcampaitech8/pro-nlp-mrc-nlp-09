"""
Re-ranker Inference
- BM25ë¡œ ìƒìœ„ Nê°œ í›„ë³´ ì¶”ì¶œ
- Cross-encoderë¡œ ì¬ì •ë ¬í•˜ì—¬ ìµœì¢… top-k ì„ íƒ
"""

import os
import json
import pickle
import torch
import numpy as np
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from konlpy.tag import Okt
from rank_bm25 import BM25Okapi
import re

# ============================
# Config
# ============================
CROSS_ENCODER_PATH = "./outputs/reranker/cross_encoder"  # í•™ìŠµëœ ëª¨ë¸
WIKI_PATH = "./data/wikipedia_documents.json"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# Re-ranking ì„¤ì •
CANDIDATE_K = 100  # 1ë‹¨ê³„ì—ì„œ ì¶”ì¶œí•  í›„ë³´ ê°œìˆ˜
TOP_K = 5          # ìµœì¢… ë°˜í™˜í•  ë¬¸ì„œ ê°œìˆ˜

print(f"Device: {DEVICE}")
print(f"Model path: {CROSS_ENCODER_PATH}")

# ============================
# Utility Functions
# ============================
def split_numbers(tokens):
    """ìˆ«ì ë¶„ë¦¬ (BM25 ì„±ëŠ¥ í–¥ìƒ)"""
    new_tokens = []
    for t in tokens:
        split_t = re.sub(r'([0-9]+)([ê°€-í£A-Za-z])', r'\1 \2', t)
        split_t = re.sub(r'([ê°€-í£A-Za-z])([0-9]+)', r'\1 \2', split_t)
        parts = split_t.split()
        new_tokens.append(t)
        new_tokens.extend(parts)
    return new_tokens

# ============================
# Load Wikipedia
# ============================
print("\n" + "="*60)
print("Loading Wikipedia documents...")
print("="*60)

wiki_cache_path = "./data/embeddings/wiki_texts_dedup.pkl"

if os.path.exists(wiki_cache_path):
    print("Loading from cache...")
    with open(wiki_cache_path, "rb") as f:
        cached = pickle.load(f)
        wiki_texts = cached["wiki_texts"]
        wiki_ids = cached["wiki_ids"]
else:
    print("Loading from JSON and deduplicating...")
    with open(WIKI_PATH, "r", encoding="utf-8") as f:
        raw_wiki = json.load(f)
    
    seen = set()
    wiki_texts = []
    wiki_ids = []
    
    for k, v in raw_wiki.items():
        text = v["text"].strip()
        sig = text[:200]
        if sig not in seen:
            seen.add(sig)
            wiki_texts.append(text)
            wiki_ids.append(k)
    
    os.makedirs(os.path.dirname(wiki_cache_path), exist_ok=True)
    with open(wiki_cache_path, "wb") as f:
        pickle.dump({"wiki_texts": wiki_texts, "wiki_ids": wiki_ids}, f)

print(f"Total documents: {len(wiki_texts)}")

# ============================
# BM25 Setup
# ============================
print("\n" + "="*60)
print("Setting up BM25...")
print("="*60)

okt = Okt()
tokens_cache_path = "./data/embeddings/wiki_corpus_okt_tokens.pkl"

if os.path.exists(tokens_cache_path):
    print("Loading cached tokens...")
    with open(tokens_cache_path, "rb") as f:
        wiki_corpus_tokens = pickle.load(f)
else:
    print("Tokenizing with Okt...")
    wiki_corpus_tokens = []
    for text in tqdm(wiki_texts):
        base_tokens = okt.morphs(text)
        tokens = split_numbers(base_tokens)
        wiki_corpus_tokens.append(tokens)
    
    os.makedirs(os.path.dirname(tokens_cache_path), exist_ok=True)
    with open(tokens_cache_path, "wb") as f:
        pickle.dump(wiki_corpus_tokens, f)

bm25 = BM25Okapi(wiki_corpus_tokens)
print("BM25 ready!")

# ============================
# Cross-Encoder Setup
# ============================
print("\n" + "="*60)
print("Loading Cross-Encoder model...")
print("="*60)

if not os.path.exists(CROSS_ENCODER_PATH):
    print(f"ERROR: Model not found at {CROSS_ENCODER_PATH}")
    print("Please train the model first using reranker_train.py")
    exit(1)

tokenizer = AutoTokenizer.from_pretrained(CROSS_ENCODER_PATH)
model = AutoModelForSequenceClassification.from_pretrained(CROSS_ENCODER_PATH).to(DEVICE)
model.eval()
print("Cross-Encoder loaded!")

# ============================
# Re-ranking Function
# ============================
def retrieve_and_rerank(query, candidate_k=CANDIDATE_K, top_k=TOP_K, batch_size=16):
    """
    2ë‹¨ê³„ ê²€ìƒ‰ + ì¬ì •ë ¬
    
    Args:
        query: ê²€ìƒ‰ ì§ˆë¬¸
        candidate_k: 1ë‹¨ê³„ì—ì„œ ì¶”ì¶œí•  í›„ë³´ ê°œìˆ˜
        top_k: ìµœì¢… ë°˜í™˜í•  ë¬¸ì„œ ê°œìˆ˜
        batch_size: Cross-encoder ë°°ì¹˜ í¬ê¸°
    
    Returns:
        List[Tuple[float, int, str]]: (score, doc_idx, document)
    """
    
    # ==========================================
    # Stage 1: BM25 Retrieval
    # ==========================================
    print(f"\n[Stage 1] BM25 retrieval (top-{candidate_k})...")
    
    q_tokens = split_numbers(okt.morphs(query))
    bm25_scores = bm25.get_scores(q_tokens)
    bm25_indices = np.argsort(bm25_scores)[-candidate_k:][::-1]
    
    candidates = [(i, wiki_texts[i]) for i in bm25_indices]
    
    print(f"Retrieved {len(candidates)} candidates")
    
    # ==========================================
    # Stage 2: Cross-Encoder Re-ranking
    # ==========================================
    print(f"\n[Stage 2] Cross-encoder re-ranking...")
    
    all_scores = []
    
    with torch.no_grad():
        for i in tqdm(range(0, len(candidates), batch_size), desc="Re-ranking"):
            batch_candidates = candidates[i:i+batch_size]
            batch_texts = [doc for _, doc in batch_candidates]
            
            # Tokenize query-document pairs
            inputs = tokenizer(
                [query] * len(batch_texts),
                batch_texts,
                truncation=True,
                max_length=512,
                padding=True,
                return_tensors="pt"
            ).to(DEVICE)
            
            # Get scores
            outputs = model(**inputs)
            logits = outputs.logits.squeeze(-1)
            scores = torch.sigmoid(logits).cpu().numpy()
            
            all_scores.extend(scores)
    
    # ==========================================
    # Sort by cross-encoder score
    # ==========================================
    results = []
    for (doc_idx, doc_text), score in zip(candidates, all_scores):
        results.append((score, doc_idx, doc_text))
    
    results.sort(key=lambda x: x[0], reverse=True)
    
    return results[:top_k]

# ============================
# Test Queries
# ============================
test_queries = [
    "ëŒ€í†µë ¹ì„ í¬í•¨í•œ ë¯¸êµ­ì˜ í–‰ì •ë¶€ ê²¬ì œê¶Œì„ ê°–ëŠ” êµ­ê°€ ê¸°ê´€ì€?",
    "í˜„ëŒ€ì  ì¸ì‚¬ì¡°ì§ê´€ë¦¬ì˜ ì‹œë°œì ì´ ëœ ì±…ì€?",
    "ê°•í¬ì œê°€ 1717ë…„ì— ì“´ ê¸€ì€ ëˆ„êµ¬ë¥¼ ìœ„í•´ ì“°ì—¬ì¡ŒëŠ”ê°€?",
    "11~12ì„¸ê¸°ì— ì œì‘ëœ ë³¸ì¡´ë¶ˆì€ ë³´í†µ ì–´ë–¤ ë‚˜ë¼ì˜ íŠ¹ì§•ì´ ì „íŒŒë˜ì—ˆë‚˜ìš”?",
    "ëª…ë¬¸ì´ ì íŒ ìœ ë¬¼ì„ êµ¬ì„±í•˜ëŠ” ê·¸ë¦‡ì˜ ì´ ê°œìˆ˜ëŠ”?",
]

print("\n" + "="*60)
print("Running Test Queries")
print("="*60)

for query in test_queries:
    print("\n" + "="*60)
    print(f"Query: {query}")
    print("="*60)
    
    results = retrieve_and_rerank(query, candidate_k=CANDIDATE_K, top_k=TOP_K)
    
    print(f"\nğŸ“Š Top-{TOP_K} Results:\n")
    for rank, (score, doc_idx, doc_text) in enumerate(results, 1):
        print(f"[Rank {rank}] Score: {score:.4f} | Doc ID: {doc_idx}")
        print(f"{doc_text[:300]}...")
        print()

# ============================
# Evaluation Mode
# ============================
def evaluate_on_dataset(dataset_path="./data/train_dataset", split="validation"):
    """
    ì „ì²´ ë°ì´í„°ì…‹ì— ëŒ€í•œ í‰ê°€
    """
    from datasets import load_from_disk
    
    print("\n" + "="*60)
    print(f"Evaluating on {split} set")
    print("="*60)
    
    dataset = load_from_disk(dataset_path)[split]
    
    correct = 0
    mrr_sum = 0.0
    
    for example in tqdm(dataset, desc="Evaluating"):
        query = example["question"]
        gold_context = example["context"]
        
        results = retrieve_and_rerank(query, candidate_k=CANDIDATE_K, top_k=TOP_K)
        retrieved_docs = [doc for _, _, doc in results]
        
        # Top-k accuracy
        if gold_context in retrieved_docs:
            correct += 1
            
            # MRR
            rank = retrieved_docs.index(gold_context) + 1
            mrr_sum += 1.0 / rank
    
    accuracy = correct / len(dataset)
    mrr = mrr_sum / len(dataset)
    
    print("\n" + "="*60)
    print("Evaluation Results")
    print("="*60)
    print(f"Top-{TOP_K} Accuracy: {accuracy:.4f} ({correct}/{len(dataset)})")
    print(f"MRR: {mrr:.4f}")
    
    return accuracy, mrr

# Uncomment to run evaluation
# evaluate_on_dataset()