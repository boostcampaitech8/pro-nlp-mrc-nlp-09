"""
Prediction Î∂ÑÏÑù Ïú†Ìã∏Î¶¨Ìã∞
- Logits Ï†ÄÏû• Î∞è confidence Í≥ÑÏÇ∞
- ÏÇ¨ÌõÑ Î∂ÑÏÑùÏùÑ ÏúÑÌïú ÏÉÅÏÑ∏ Ï†ïÎ≥¥ Ï∂îÏ∂ú
"""

import os
import json
import logging
from typing import Dict, List, Optional

import torch
import numpy as np
import pandas as pd
from transformers.trainer_utils import PredictionOutput
from scipy.special import softmax

logger = logging.getLogger(__name__)


def save_prediction_analysis(
    predictions: PredictionOutput,
    examples: List[Dict],
    output_dir: str,
    split: str,
    answer_column_name: str = "answers",
):
    """
    Prediction Í≤∞Í≥ºÏóêÏÑú logits, confidence scores Ï∂îÏ∂ú Î∞è Ï†ÄÏû•

    Ï†ÄÏû• ÌååÏùº:
    - {split}_logits.pt: Raw logits (torch tensor, Ïû¨ÌòÑ/ÏïôÏÉÅÎ∏îÏö©)
    - {split}_confidence.csv: IDÎ≥Ñ confidence scores (Îπ†Î•∏ ÌïÑÌÑ∞ÎßÅÏö©)

    Args:
        predictions: trainer.predict() Í≤∞Í≥º (PredictionOutput)
        examples: ÏõêÎ≥∏ dataset examples
        output_dir: Ï†ÄÏû• ÎîîÎ†âÌÜ†Î¶¨
        split: 'train', 'validation', 'test'
        answer_column_name: Ï†ïÎãµ ÌïÑÎìú Ïù¥Î¶Ñ
    """
    os.makedirs(output_dir, exist_ok=True)

    # 1. Logits Ï∂îÏ∂ú
    # predictions.predictionsÎäî (start_logits, end_logits) ÌäúÌîå ÎòêÎäî
    # postprocessing ÌõÑ dict Î¶¨Ïä§Ìä∏Ïùº Ïàò ÏûàÏùå
    # Ïó¨Í∏∞ÏÑúÎäî QuestionAnsweringTrainerÏùò predict()Í∞Ä Ïù¥ÎØ∏ postprocessing ÏôÑÎ£åÌïú
    # ÏÉÅÌÉúÎ•º Î∞õÏúºÎØÄÎ°ú, Ïã§Ï†úÎ°úÎäî logitsÍ∞Ä ÏïÑÎãå predictionsÎßå ÏûàÏùå
    # Îî∞ÎùºÏÑú logits Ï†ÄÏû•ÏùÄ trainer ÎÇ¥Î∂ÄÏóêÏÑú Ìï¥Ïïº ÌïòÍ±∞ÎÇò,
    # Ïó¨Í∏∞ÏÑúÎäî prediction confidenceÎßå Í≥ÑÏÇ∞

    # predictions.predictionsÎäî [{"id": ..., "prediction_text": ...}, ...] ÌòïÌÉú
    pred_list = (
        predictions.predictions if hasattr(predictions, "predictions") else predictions
    )

    # ÏòàÏ∏° Í≤∞Í≥ºÎ•º dictÎ°ú Î≥ÄÌôò
    if isinstance(pred_list, list) and len(pred_list) > 0:
        if isinstance(pred_list[0], dict):
            pred_dict = {p["id"]: p["prediction_text"] for p in pred_list}
        else:
            # ÌòπÏãú Îã§Î•∏ ÌòïÌÉúÏùº Í≤ΩÏö∞ ÎåÄÎπÑ
            logger.warning(f"Unexpected prediction format: {type(pred_list[0])}")
            pred_dict = {}
    else:
        pred_dict = {}

    # Logits ÌååÏùº Î°úÎìú ÏãúÎèÑ
    logits_dict = {}
    logits_file = os.path.join(output_dir, f"logits_{split}.json")
    if os.path.exists(logits_file):
        try:
            with open(logits_file, "r", encoding="utf-8") as f:
                logits_dict = json.load(f)
            logger.info(f"‚úÖ Loaded logits from {logits_file}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  Failed to load logits: {e}")

    # 2. ExampleÎ≥Ñ Ï†ïÎ≥¥ ÏàòÏßë
    results = []
    correct_count = 0

    for example in examples:
        example_id = example["id"]
        prediction = pred_dict.get(example_id, "")

        # Ï†ïÎãµ ÌôïÏù∏
        is_correct = False
        if answer_column_name in example and example[answer_column_name]:
            answers = example[answer_column_name]
            if isinstance(answers, dict) and "text" in answers:
                answer_texts = answers["text"]
            elif isinstance(answers, list):
                answer_texts = answers
            else:
                answer_texts = []

            # Exact match Ï≤¥ÌÅ¨
            is_correct = any(prediction.strip() == ans.strip() for ans in answer_texts)
            if is_correct:
                correct_count += 1

        # Confidence Í≥ÑÏÇ∞ (logitsÍ∞Ä ÏûàÏúºÎ©¥)
        max_prob = -1.0
        avg_prob = -1.0
        if example_id in logits_dict:
            logit_info = logits_dict[example_id]
            start_logit = logit_info.get("start_logit", 0.0)
            end_logit = logit_info.get("end_logit", 0.0)

            # Probability from logits: exp(logit) / sum(exp(all_logits))
            # Ïó¨Í∏∞ÏÑúÎäî Îã®Ïùº Í∞íÎßå ÏûàÏúºÎØÄÎ°ú softmax ÎåÄÏã† sigmoid ÏÇ¨Ïö©
            # ÎòêÎäî Ïù¥ÎØ∏ Ï†ÄÏû•Îêú probability ÏÇ¨Ïö©
            if "probability" in logit_info:
                max_prob = logit_info["probability"]
                avg_prob = logit_info["probability"]
            else:
                # Simple approximation: sigmoid of combined score
                combined_score = start_logit + end_logit
                max_prob = 1.0 / (1.0 + np.exp(-combined_score))
                avg_prob = max_prob

        results.append(
            {
                "id": example_id,
                "prediction": prediction,
                "max_prob": max_prob,
                "avg_prob": avg_prob,
                "is_correct": 1 if is_correct else 0,
                "pred_length": len(prediction),
            }
        )

    # 3. CSV Ï†ÄÏû•
    df = pd.DataFrame(results)
    csv_path = os.path.join(output_dir, f"{split}_confidence.csv")
    df.to_csv(csv_path, index=False, encoding="utf-8")
    logger.info(f"‚úÖ Confidence analysis saved to {csv_path}")

    # 4. ÌÜµÍ≥Ñ Î°úÍπÖ
    accuracy = correct_count / len(examples) if len(examples) > 0 else 0
    avg_pred_length = df["pred_length"].mean()

    logger.info(f"üìä Analysis stats ({split}):")
    logger.info(f"   Total samples: {len(examples)}")
    logger.info(f"   Correct predictions: {correct_count} ({accuracy:.2%})")
    logger.info(f"   Average prediction length: {avg_pred_length:.1f}")

    # Bottom 10% (is_correct Í∏∞Ï§Ä)
    if len(df) > 0:
        incorrect_df = df[df["is_correct"] == 0]
        if len(incorrect_df) > 0:
            logger.info(
                f"   Incorrect predictions: {len(incorrect_df)} ({len(incorrect_df) / len(df):.2%})"
            )

            # ÏòàÏ∏° Í∏∏Ïù¥Í∞Ä ÏßßÏùÄ ÏàúÏúºÎ°ú Ï†ïÎ†¨ (confidence proxy)
            bottom_10_pct = max(1, int(len(incorrect_df) * 0.1))
            shortest_incorrect = incorrect_df.nsmallest(bottom_10_pct, "pred_length")
            logger.info(
                f"   Bottom 10% incorrect (shortest predictions): {bottom_10_pct} samples"
            )
            logger.info(
                f"   Avg length in bottom 10%: {shortest_incorrect['pred_length'].mean():.1f}"
            )

    return csv_path


# TODO: confidence Í≥ÑÏÇ∞ Ïò§Î•ò Ï°¥Ïû¨Ìï®
def save_prediction_analysis_with_logits(
    start_logits: np.ndarray,
    end_logits: np.ndarray,
    predictions: List[Dict],
    examples: List[Dict],
    output_dir: str,
    split: str,
    answer_column_name: str = "answers",
):
    """
    LogitsÎ•º Ìè¨Ìï®Ìïú ÏÉÅÏÑ∏ Î∂ÑÏÑù (trainer ÎÇ¥Î∂ÄÏóêÏÑú Ìò∏Ï∂ú Ïãú ÏÇ¨Ïö©)

    Args:
        start_logits: Start position logits (N, seq_len)
        end_logits: End position logits (N, seq_len)
        predictions: Postprocessed predictions [{"id": ..., "prediction_text": ...}]
        examples: ÏõêÎ≥∏ examples
        output_dir: Ï†ÄÏû• ÎîîÎ†âÌÜ†Î¶¨
        split: 'train', 'validation', 'test'
        answer_column_name: Ï†ïÎãµ ÌïÑÎìú Ïù¥Î¶Ñ
    """
    os.makedirs(output_dir, exist_ok=True)

    # 1. Logits Ï†ÄÏû• (.pt - Ïû¨ÌòÑ/ÏïôÏÉÅÎ∏îÏö©)
    logits_path = os.path.join(output_dir, f"{split}_logits.pt")
    torch.save(
        {
            "start_logits": torch.from_numpy(start_logits)
            if isinstance(start_logits, np.ndarray)
            else start_logits,
            "end_logits": torch.from_numpy(end_logits)
            if isinstance(end_logits, np.ndarray)
            else end_logits,
            "metadata": {
                "split": split,
                "num_samples": len(examples),
            },
        },
        logits_path,
    )
    logger.info(f"‚úÖ Logits saved to {logits_path}")

    # 2. Softmax Í≥ÑÏÇ∞
    start_probs = softmax(start_logits, axis=-1)  # (N, seq_len)
    end_probs = softmax(end_logits, axis=-1)

    # Í∞Å ÏÉòÌîåÏùò ÏµúÎåÄ ÌôïÎ•†
    max_start_probs = np.max(start_probs, axis=-1)  # (N,)
    max_end_probs = np.max(end_probs, axis=-1)

    # MaxÏôÄ Average confidence
    max_probs = np.maximum(max_start_probs, max_end_probs)
    avg_probs = (max_start_probs + max_end_probs) / 2

    # 3. PredictionsÎ•º dictÎ°ú Î≥ÄÌôò
    pred_dict = {p["id"]: p["prediction_text"] for p in predictions}

    # 4. ExampleÎ≥Ñ Ï†ïÎ≥¥ ÏàòÏßë
    results = []
    correct_count = 0

    for i, example in enumerate(examples):
        example_id = example["id"]
        prediction = pred_dict.get(example_id, "")

        # Ï†ïÎãµ ÌôïÏù∏
        is_correct = False
        if answer_column_name in example and example[answer_column_name]:
            answers = example[answer_column_name]
            if isinstance(answers, dict) and "text" in answers:
                answer_texts = answers["text"]
            elif isinstance(answers, list):
                answer_texts = answers
            else:
                answer_texts = []

            is_correct = any(prediction.strip() == ans.strip() for ans in answer_texts)
            if is_correct:
                correct_count += 1

        results.append(
            {
                "id": example_id,
                "prediction": prediction,
                "max_prob": float(max_probs[i]),
                "avg_prob": float(avg_probs[i]),
                "is_correct": 1 if is_correct else 0,
            }
        )

    # 5. CSV Ï†ÄÏû•
    df = pd.DataFrame(results)
    csv_path = os.path.join(output_dir, f"{split}_confidence.csv")
    df.to_csv(csv_path, index=False, encoding="utf-8")
    logger.info(f"‚úÖ Confidence analysis saved to {csv_path}")

    # 6. ÌÜµÍ≥Ñ Î°úÍπÖ
    accuracy = correct_count / len(examples) if len(examples) > 0 else 0
    mean_conf = np.mean(avg_probs)
    std_conf = np.std(avg_probs)
    bottom_10_pct_threshold = np.percentile(avg_probs, 10)

    logger.info(f"üìä Confidence stats ({split}):")
    logger.info(f"   Mean: {mean_conf:.3f}, Std: {std_conf:.3f}")
    logger.info(f"   Bottom 10% threshold: {bottom_10_pct_threshold:.3f}")
    logger.info(f"   Accuracy: {correct_count}/{len(examples)} ({accuracy:.2%})")

    # Bottom 10% Î∂ÑÏÑù
    bottom_10_mask = avg_probs <= bottom_10_pct_threshold
    bottom_10_df = df[bottom_10_mask]
    bottom_10_incorrect = bottom_10_df[bottom_10_df["is_correct"] == 0]

    logger.info(f"   Bottom 10% samples: {len(bottom_10_df)}")
    logger.info(f"   Bottom 10% incorrect: {len(bottom_10_incorrect)}")

    return logits_path, csv_path
