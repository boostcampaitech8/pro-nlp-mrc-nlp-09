"""
ìµœì¢… ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ë° ê²°ê³¼ ì €ì¥ ìœ í‹¸ë¦¬í‹°
"""

import json
import os
from datetime import datetime
from typing import Dict, Optional
import evaluate


class FinalEvaluator:
    """
    í•™ìŠµ ì™„ë£Œ í›„ train/validation/testì— ëŒ€í•œ ì¢…í•© í‰ê°€ ìˆ˜í–‰
    """

    def __init__(self, output_dir: str):
        self.output_dir = output_dir
        self.metric = evaluate.load("squad")
        self.results = {
            "evaluation_time": datetime.now().isoformat(),
            "train_performance": {},
            "validation_performance": {},
            "validation_with_retrieval_performance": {},
            "test_performance": {},
        }

    def evaluate_split(
        self,
        predictions: Dict,
        references: Dict,
        split_name: str,
        with_retrieval: bool = False,
    ) -> Dict:
        """
        íŠ¹ì • splitì— ëŒ€í•œ í‰ê°€ ìˆ˜í–‰

        Args:
            predictions: {id: prediction_text}
            references: {id: answers}
            split_name: 'train', 'validation', 'test'
            with_retrieval: retrieval ì‚¬ìš© ì—¬ë¶€
        """
        # predictionsë¥¼ squad í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        formatted_predictions = [
            {"id": k, "prediction_text": v} for k, v in predictions.items()
        ]
        formatted_references = [{"id": k, "answers": v} for k, v in references.items()]

        # ë©”íŠ¸ë¦­ ê³„ì‚°
        metrics = self.metric.compute(
            predictions=formatted_predictions, references=formatted_references
        )

        result = {
            "exact_match": metrics["exact_match"],
            "f1": metrics["f1"],
            "total_samples": len(predictions),
            "with_retrieval": with_retrieval,
        }

        # ê²°ê³¼ ì €ì¥
        if split_name == "train":
            self.results["train_performance"] = result
        elif split_name == "validation":
            if with_retrieval:
                self.results["validation_with_retrieval_performance"] = result
            else:
                self.results["validation_performance"] = result
        elif split_name == "test":
            self.results["test_performance"] = result

        return result

    def save_summary(self):
        """ì¢…í•© í‰ê°€ ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ì €ì¥"""
        summary_path = os.path.join(self.output_dir, "final_evaluation_summary.json")
        with open(summary_path, "w", encoding="utf-8") as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        print(f"âœ… Final evaluation summary saved to {summary_path}")

    def print_summary(self):
        """ì¢…í•© í‰ê°€ ê²°ê³¼ë¥¼ ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥"""
        print("\n" + "=" * 80)
        print("ğŸ¯ FINAL MODEL PERFORMANCE SUMMARY")
        print("=" * 80)

        def print_performance(title: str, perf: Dict):
            if not perf:
                print(f"\n{title}: Not evaluated")
                return
            print(f"\n{title}:")
            print(f"  ğŸ“Š Exact Match: {perf['exact_match']:.2f}")
            print(f"  ğŸ“Š F1 Score: {perf['f1']:.2f}")
            print(f"  ğŸ“ Total Samples: {perf['total_samples']}")
            if perf.get("with_retrieval") is not None:
                print(
                    f"  ğŸ” With Retrieval: {'Yes' if perf['with_retrieval'] else 'No'}"
                )

        print_performance(
            "ğŸ“˜ Train Performance", self.results.get("train_performance", {})
        )
        print_performance(
            "ğŸ“— Validation Performance (Direct Context)",
            self.results.get("validation_performance", {}),
        )
        print_performance(
            "ğŸ“™ Validation Performance (With Retrieval)",
            self.results.get("validation_with_retrieval_performance", {}),
        )
        print_performance(
            "ğŸ“• Test Performance", self.results.get("test_performance", {})
        )

        print("=" * 80 + "\n")


def save_predictions(
    predictions: Dict, output_path: str, split_name: str = "predictions"
):
    """
    Predictionsë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥

    Args:
        predictions: {id: prediction_text}
        output_path: ì €ì¥í•  ë””ë ‰í† ë¦¬ ê²½ë¡œ
        split_name: íŒŒì¼ëª…ì— ì‚¬ìš©í•  split ì´ë¦„
    """
    os.makedirs(output_path, exist_ok=True)
    pred_file = os.path.join(output_path, f"{split_name}_predictions.json")

    with open(pred_file, "w", encoding="utf-8") as f:
        json.dump(predictions, f, indent=2, ensure_ascii=False)

    print(f"âœ… Predictions saved to {pred_file}")
    return pred_file


def save_detailed_results(
    predictions: Dict,
    examples: list,
    output_path: str,
    split_name: str = "detailed",
):
    """
    ì‚¬í›„ ë¶„ì„ì„ ìœ„í•œ ìƒì„¸ ê²°ê³¼ ì €ì¥

    Args:
        predictions: {id: prediction_text}
        examples: ì›ë³¸ examples (question, context, answers í¬í•¨)
        output_path: ì €ì¥í•  ë””ë ‰í† ë¦¬ ê²½ë¡œ
        split_name: íŒŒì¼ëª…ì— ì‚¬ìš©í•  split ì´ë¦„
    """
    os.makedirs(output_path, exist_ok=True)
    detailed_file = os.path.join(output_path, f"{split_name}_detailed_results.json")

    detailed_results = []
    metric = evaluate.load("squad")

    for example in examples:
        example_id = example["id"]
        prediction = predictions.get(example_id, "")

        # ê°œë³„ ë©”íŠ¸ë¦­ ê³„ì‚°
        if "answers" in example and example["answers"]["text"]:
            individual_metric = metric.compute(
                predictions=[{"id": example_id, "prediction_text": prediction}],
                references=[{"id": example_id, "answers": example["answers"]}],
            )
            em_score = individual_metric["exact_match"]
            f1_score = individual_metric["f1"]
        else:
            em_score = None
            f1_score = None

        detailed_results.append(
            {
                "id": example_id,
                "question": example.get("question", ""),
                "context": example.get("context", "")[:500]
                + "...",  # contextëŠ” ì• 500ìë§Œ
                "prediction": prediction,
                "ground_truth": example.get("answers", {}).get("text", []),
                "em_score": em_score,
                "f1_score": f1_score,
            }
        )

    with open(detailed_file, "w", encoding="utf-8") as f:
        json.dump(detailed_results, f, indent=2, ensure_ascii=False)

    print(f"âœ… Detailed results saved to {detailed_file}")
    return detailed_file
