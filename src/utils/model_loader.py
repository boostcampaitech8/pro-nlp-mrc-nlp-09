"""
ëª¨ë¸ ê²½ë¡œ ìë™ íƒìƒ‰ ìœ í‹¸ë¦¬í‹°

Train ì‹œ: model_name_or_pathë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš© (pretrained model)
Inference ì‹œ: use_trained_model=Trueì´ë©´ output_dirì—ì„œ best checkpoint ìë™ íƒìƒ‰
"""

import os
import json
from typing import Optional


def get_model_path(model_args, training_args, for_inference: bool = False) -> str:
    """
    í•™ìŠµ/ì¶”ë¡  ìƒí™©ì— ë§ëŠ” ëª¨ë¸ ê²½ë¡œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

    Args:
        model_args: ModelArguments ì¸ìŠ¤í„´ìŠ¤
        training_args: TrainingArguments ì¸ìŠ¤í„´ìŠ¤
        for_inference: inference ëª¨ë“œì¸ì§€ ì—¬ë¶€

    Returns:
        ì‚¬ìš©í•  ëª¨ë¸ ê²½ë¡œ (pretrained model name ë˜ëŠ” checkpoint path)
    """
    # Train ëª¨ë“œì´ê±°ë‚˜ use_trained_model=Falseì¸ ê²½ìš°
    if not for_inference or not model_args.use_trained_model:
        return model_args.model_name_or_path

    # Inference ëª¨ë“œì—ì„œ trained model ì‚¬ìš©
    output_dir = training_args.output_dir

    if not os.path.exists(output_dir):
        raise FileNotFoundError(
            f"Output directory not found: {output_dir}\n"
            f"Set use_trained_model=false in YAML to use pretrained model directly."
        )

    # 1ìˆœìœ„: best_checkpoint_path.txt íŒŒì¼ í™•ì¸
    best_checkpoint_file = os.path.join(output_dir, "best_checkpoint_path.txt")
    if os.path.exists(best_checkpoint_file):
        with open(best_checkpoint_file, "r") as f:
            checkpoint_path = f.read().strip()
            if checkpoint_path and os.path.exists(checkpoint_path):
                print(
                    f"âœ… Using best checkpoint from best_checkpoint_path.txt: {checkpoint_path}"
                )
                return checkpoint_path

    # 2ìˆœìœ„: trainer_state.jsonì—ì„œ best_model_checkpoint ì½ê¸°
    trainer_state_file = os.path.join(output_dir, "trainer_state.json")
    if os.path.exists(trainer_state_file):
        with open(trainer_state_file, "r") as f:
            trainer_state = json.load(f)
            best_checkpoint = trainer_state.get("best_model_checkpoint")
            if best_checkpoint and os.path.exists(best_checkpoint):
                print(
                    f"âœ… Using best checkpoint from trainer_state.json: {best_checkpoint}"
                )
                return best_checkpoint

    # 3ìˆœìœ„: checkpoint-* í´ë” ì¤‘ ìˆ«ìê°€ ê°€ì¥ í° ê²ƒ ì„ íƒ (fallback)
    checkpoint_dirs = [
        d
        for d in os.listdir(output_dir)
        if d.startswith("checkpoint-") and os.path.isdir(os.path.join(output_dir, d))
    ]

    if checkpoint_dirs:
        # checkpoint-1234 í˜•ì‹ì—ì„œ ìˆ«ì ì¶”ì¶œí•˜ì—¬ ì •ë ¬
        def get_checkpoint_number(dirname):
            try:
                return int(dirname.split("-")[-1])
            except ValueError:
                return -1

        latest_checkpoint = max(checkpoint_dirs, key=get_checkpoint_number)
        checkpoint_path = os.path.join(output_dir, latest_checkpoint)
        print(
            f"âš ï¸  Best checkpoint info not found. Using latest checkpoint: {checkpoint_path}"
        )
        return checkpoint_path

    # ëª¨ë“  ì‹œë„ ì‹¤íŒ¨
    raise FileNotFoundError(
        f"No checkpoint found in {output_dir}\n"
        f"Please run training first, or set use_trained_model=false to use pretrained model."
    )


def load_inference_dataset(data_args, inference_split: Optional[str] = None):
    """
    inference_splitì— ë”°ë¼ ì ì ˆí•œ ë°ì´í„°ì…‹ì„ ë¡œë“œí•©ë‹ˆë‹¤.

    Args:
        data_args: DataTrainingArguments ì¸ìŠ¤í„´ìŠ¤
        inference_split: 'train', 'validation', ë˜ëŠ” 'test' (Noneì´ë©´ data_args.inference_split ì‚¬ìš©)

    Returns:
        ë¡œë“œëœ ë°ì´í„°ì…‹ (DatasetDict)
    """
    from datasets import load_from_disk

    split = inference_split or data_args.inference_split

    if split == "test":
        # test split: infer_dataset_name ì‚¬ìš©
        dataset_path = data_args.infer_dataset_name
        print(f"ğŸ“¦ Loading test dataset from: {dataset_path}")
        return load_from_disk(dataset_path)

    elif split in ["train", "validation"]:
        # train/validation split: train_dataset_nameì—ì„œ í•´ë‹¹ split ì‚¬ìš©
        dataset_path = data_args.train_dataset_name
        print(f"ğŸ“¦ Loading {split} split from: {dataset_path}")
        datasets = load_from_disk(dataset_path)

        if split not in datasets:
            raise ValueError(
                f"Split '{split}' not found in {dataset_path}. "
                f"Available splits: {list(datasets.keys())}"
            )

        # validation splitì„ "validation" í‚¤ë¡œ ë°˜í™˜ (inference.py ê¸°ëŒ€ í˜•ì‹)
        from datasets import DatasetDict

        return DatasetDict({"validation": datasets[split]})

    else:
        raise ValueError(
            f"Invalid inference_split: {split}. "
            f"Must be one of: 'train', 'validation', 'test'"
        )
