import logging
import pandas as pd
from datasets import Dataset, DatasetDict, Features, Sequence, Value
from src.arguments import DataTrainingArguments
from src.retrieval.base import BaseRetrieval

logger = logging.getLogger(__name__)


def realign_answers_in_retrieved_context(example):
    """
    ê²€ìƒ‰ëœ contextì—ì„œ ì •ë‹µ(answers['text'])ì„ ì°¾ì•„ answer_startë¥¼ ê°±ì‹ í•©ë‹ˆë‹¤.
    """
    retrieved_context = example["context"]
    original_answers = example["answers"]

    new_text = []
    new_answer_start = []

    # ì›ë³¸ ì •ë‹µ í…ìŠ¤íŠ¸ë“¤ì´ ê²€ìƒ‰ëœ contextì— ì¡´ìž¬í•˜ëŠ”ì§€ í™•ì¸
    for answer_text in original_answers["text"]:
        # ê²€ìƒ‰ëœ contextì—ì„œ ì •ë‹µ í…ìŠ¤íŠ¸ì˜ ì‹œìž‘ ìœ„ì¹˜ ì°¾ê¸°
        # ì£¼ì˜: ì—¬ëŸ¬ ë²ˆ ë“±ìž¥í•  ìˆ˜ ìžˆì§€ë§Œ, ì—¬ê¸°ì„œëŠ” ì²« ë²ˆì§¸ ë“±ìž¥ ìœ„ì¹˜ë§Œ ì°¾ê±°ë‚˜
        # ëª¨ë“  ë“±ìž¥ ìœ„ì¹˜ë¥¼ ì°¾ì„ ìˆ˜ë„ ìžˆìŒ. ì¼ë°˜ì ìœ¼ë¡œ ì²« ë²ˆì§¸ë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜,
        # ë¬¸ë§¥ìƒ ê°€ìž¥ ì ì ˆí•œ ê²ƒì„ ì°¾ì•„ì•¼ í•˜ì§€ë§Œ, ë‹¨ìˆœ ë§¤ì¹­ìœ¼ë¡œëŠ” ì²« ë²ˆì§¸ë¥¼ ì‚¬ìš©í•¨.
        start_idx = retrieved_context.find(answer_text)

        # ì •ë‹µì„ ì°¾ì€ ê²½ìš°ì—ë§Œ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
        if start_idx != -1:
            new_text.append(answer_text)
            new_answer_start.append(start_idx)

    # ê°±ì‹ ëœ answersë¡œ êµì²´
    # ì°¾ì§€ ëª»í•œ ê²½ìš° ë¹ˆ ë¦¬ìŠ¤íŠ¸ê°€ ë¨ -> ì´í›„ í•„í„°ë§ ëŒ€ìƒ
    example["answers"] = {"text": new_text, "answer_start": new_answer_start}
    return example


def retrieve_and_build_dataset(
    retriever: BaseRetrieval,
    dataset: Dataset,
    data_args: DataTrainingArguments,
    split_name: str = "validation",
    is_train: bool = False,
    tokenizer=None,
) -> Dataset:
    """
    Retrieverë¥¼ ì‚¬ìš©í•´ questionì— ë§žëŠ” contextë¥¼ ê²€ìƒ‰í•˜ê³  MRCìš© ë°ì´í„°ì…‹ ìƒì„±.

    Args:
        retriever: ì´ë¯¸ build()ëœ retrieval ê°ì²´
        dataset: ì›ë³¸ ë°ì´í„°ì…‹ (ë‹¨ì¼ split)
        data_args: top_k_retrieval ë“± ì„¤ì •
        split_name: ë¡œê·¸ìš© split ì´ë¦„
        is_train: Trueì¼ ê²½ìš° ì •ë‹µ ìž¬ì •ë ¬(realignment) ë° í•„í„°ë§ ìˆ˜í–‰
        tokenizer: Tokenizer ê°ì²´ (Title í¬í•¨ Context ìƒì„±ì„ ìœ„í•´ í•„ìš”)

    Returns:
        Retrieved contextê°€ í¬í•¨ëœ Dataset
    """
    logger.info(
        f"ðŸ” Running retrieval on {split_name} split (top_k={data_args.top_k_retrieval})..."
    )

    if tokenizer:
        logger.info(
            f"ðŸ§© Tokenizer detected. Title will be prepended to context using separator: '{tokenizer.sep_token}'"
        )
    else:
        logger.warning(
            "âš ï¸ Tokenizer not provided. Title will NOT be included in context."
        )

    # 1. Retrieval ìˆ˜í–‰
    # retriever.retrieveëŠ” DataFrameì„ ë°˜í™˜í•¨
    df = retriever.retrieve(
        dataset, topk=data_args.top_k_retrieval, tokenizer=tokenizer
    )

    # 2. ì‹¤ì œ DataFrameì— answers ì»¬ëŸ¼ì´ ìžˆëŠ”ì§€ í™•ì¸
    has_answers = "answers" in df.columns

    # 3. HF Features ì •ì˜
    if has_answers:
        used_columns = ["id", "question", "context", "answers"]
        features = Features(
            {
                "id": Value(dtype="string", id=None),
                "question": Value(dtype="string", id=None),
                "context": Value(dtype="string", id=None),
                "answers": Sequence(
                    feature={
                        "text": Value(dtype="string", id=None),
                        "answer_start": Value(dtype="int32", id=None),
                    },
                    length=-1,
                    id=None,
                ),
            }
        )
    else:
        used_columns = ["id", "question", "context"]
        features = Features(
            {
                "id": Value(dtype="string", id=None),
                "question": Value(dtype="string", id=None),
                "context": Value(dtype="string", id=None),
            }
        )

    # 4. í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ë‚¨ê¸°ê³  HF Datasetìœ¼ë¡œ ë³€í™˜
    df = df[used_columns].reset_index(drop=True)
    new_dataset = Dataset.from_pandas(df, features=features)

    # --- RERANKING LOGIC ---
    if kwargs.get("reranker") and has_answers: # Reranking mostly useful when context is retrieved. 
        # But wait, df already contains 'context'. 
        # Reranker should ideally run BEFORE constructing the final context if we are selecting top-1 from top-k?
        # Typically reranker takes top-k retrieved, scores them, and we select top-N (usually smaller).
        # Here `df` from `retriever.retrieve` already has the top-k documents. 
        # But `retriever.retrieve` usually joins top-k contexts? 
        # Let's check `BaseRetrieval.retrieve`. It typically returns a dataframe with 'context' column 
        # which is a concatenation of top-k passages?
        # If so, reranking is hard here. Reranking needs individual passages.
        # I need to check `BaseRetrieval.retrieve` implementation.
        pass
    
    # Actually, `BaseRetrieval.retrieve` returns a DataFrame where 'context' is already the joined string of top-k docs.
    # This means I CANNOT rerank here easily unless `retrieve` returns candidates.
    # I need to modify `retrieve_and_build_dataset` to handle reranking differently.
    # The standard pattern:
    # 1. get_relevant_doc_bulk -> returns indices/scores
    # 2. Extract passage texts
    # 3. Rerank these texts
    # 4. Select top-N from reranked
    # 5. Join them into 'context' column
    
    # So I shouldn't rely on `retriever.retrieve` which does everything.
    # I should reproduce what `retriever.retrieve` does but inject reranking.
    
    # However, `retriever.retrieve` is convenient.
    # Let's modify `src/retrieval/base.py` or just do it here manually calling `get_relevant_doc_bulk`.
    
    # Since I cannot see `BaseRetrieval` right now in this turn, I will assume I need to manually call `get_relevant_doc_bulk` here if reranker is present.
    
    reranker = kwargs.get("reranker")
    if reranker:
        logger.info(f"ðŸ”„ Reranking retrieved passages using {reranker.model_name}...")
        
        # 1. Get initial top-k (maybe larger than final k?)
        # Let's use data_args.top_k_retrieval for initial, or maybe we need a separate arg 'top_k_rerank'.
        # For now, let's assume we retrieve top_k * 2 candidates for reranking?
        # Or just use top_k.
        initial_k = data_args.top_k_retrieval
        
        doc_scores, doc_indices = retriever.get_relevant_doc_bulk(
            dataset["question"], k=initial_k
        )
        
        final_contexts = []
        
        for idx, (scores, indices) in enumerate(zip(doc_scores, doc_indices)):
            question = dataset[idx]["question"]
            passages = [retriever.contexts[i] for i in indices]
            
            # Rerank
            rerank_scores = reranker.rerank(question, passages)
            
            # Sort by new scores
            # Zip together: (score, passage_text)
            scored_passages = list(zip(rerank_scores, passages))
            scored_passages.sort(key=lambda x: x[0], reverse=True)
            
            # Take top-k (or all of them sorted)
            sorted_passages = [p for _, p in scored_passages]
            
            # Join for MRC context
            final_contexts.append(" ".join(sorted_passages))
            
        # Update DataFrame with new contexts
        df["context"] = final_contexts
        
        # Re-create dataset
        new_dataset = Dataset.from_pandas(df[used_columns], features=features)

    # 5. Trainingì¼ ê²½ìš° Answer Realignment ìˆ˜í–‰
    if is_train and has_answers:
        logger.info("ðŸ”„ Realigning answers in retrieved contexts for training...")
        original_len = len(new_dataset)

        # Answer ìœ„ì¹˜ ìž¬ê³„ì‚°
        new_dataset = new_dataset.map(realign_answers_in_retrieved_context)

        # ì •ë‹µì„ ì°¾ì§€ ëª»í•œ ë°ì´í„°(ë¹ˆ ë¦¬ìŠ¤íŠ¸) í•„í„°ë§
        # answers['text']ê°€ ë¹„ì–´ìžˆìœ¼ë©´ ì •ë‹µì´ ì—†ëŠ” ê²ƒ
        def filter_valid_answers(example):
            return len(example["answers"]["text"]) > 0

        new_dataset = new_dataset.filter(filter_valid_answers)

        filtered_len = len(new_dataset)
        lost_count = original_len - filtered_len
        lost_ratio = (lost_count / original_len) * 100 if original_len > 0 else 0

        logger.warning(
            f"ðŸ“‰ Retrieval-Augmented Training Stats:\n"
            f"   - Original: {original_len}\n"
            f"   - Filtered (Answer Found): {filtered_len}\n"
            f"   - Lost: {lost_count} ({lost_ratio:.2f}%)\n"
            f"   * Lost examples means the correct answer was NOT found in top-{data_args.top_k_retrieval} retrieved passages."
        )

    return new_dataset
