import logging
from typing import Optional
from datasets import Dataset, Features, Sequence, Value
from src.arguments import DataTrainingArguments
from src.retrieval.base import BaseRetrieval

logger = logging.getLogger(__name__)


def realign_answers_in_retrieved_context(
    example,
    sep_token: Optional[str] = None,
    use_title: bool = False,
):
    """
    ê²€ìƒ‰ëœ contextì—ì„œ ì •ë‹µ(answers['text'])ì„ ì°¾ì•„ answer_startë¥¼ ê°±ì‹ í•©ë‹ˆë‹¤.

    Args:
        example: HF Dataset example (context, answers í¬í•¨)
        sep_token: Titleê³¼ ë³¸ë¬¸ì„ êµ¬ë¶„í•˜ëŠ” separator (ì˜ˆ: "[SEP]", "</s>")
        use_title: Trueì´ë©´ contextì— titleì´ í¬í•¨ë˜ì–´ ìˆë‹¤ê³  ê°€ì •í•˜ê³ ,
                   title ì˜ì—­ì„ ê±´ë„ˆë›°ê³  ë³¸ë¬¸ì—ì„œë§Œ ì •ë‹µì„ ì°¾ìŠµë‹ˆë‹¤.

    Returns:
        answer_startê°€ ê°±ì‹ ëœ example

    Note:
        - use_title=Trueì´ê³  sep_tokenì´ ì£¼ì–´ì§€ë©´:
          context = "ì œëª© [SEP] ë³¸ë¬¸" í˜•íƒœì—ì„œ ë³¸ë¬¸ ë¶€ë¶„ë§Œ ê²€ìƒ‰
        - ì •ë‹µì´ titleì—ë§Œ ìˆê³  ë³¸ë¬¸ì— ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜ (ì´í›„ í•„í„°ë§)
    """
    retrieved_context = example["context"]
    original_answers = example["answers"]

    new_text = []
    new_answer_start = []

    # Title ì˜ì—­ ê±´ë„ˆë›°ê¸° ìœ„í•œ offset ê³„ì‚°
    search_start_offset = 0
    if use_title and sep_token:
        # sep_token ìœ„ì¹˜ ì°¾ê¸° (ì²« ë²ˆì§¸ passageì˜ separator)
        sep_pos = retrieved_context.find(sep_token)
        if sep_pos != -1:
            # separator ë’¤ë¶€í„° ê²€ìƒ‰ ì‹œì‘ (sep_token ê¸¸ì´ + ê³µë°± ê³ ë ¤)
            search_start_offset = sep_pos + len(sep_token)
            # separator ë’¤ì˜ ê³µë°± ê±´ë„ˆë›°ê¸°
            while (
                search_start_offset < len(retrieved_context)
                and retrieved_context[search_start_offset] == " "
            ):
                search_start_offset += 1

    # ì›ë³¸ ì •ë‹µ í…ìŠ¤íŠ¸ë“¤ì´ ê²€ìƒ‰ëœ contextì— ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    for answer_text in original_answers["text"]:
        # Title ì˜ì—­ì„ ê±´ë„ˆë›´ ìœ„ì¹˜ì—ì„œë¶€í„° ê²€ìƒ‰
        start_idx = retrieved_context.find(answer_text, search_start_offset)

        # ì •ë‹µì„ ì°¾ì€ ê²½ìš°ì—ë§Œ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
        if start_idx != -1:
            new_text.append(answer_text)
            new_answer_start.append(start_idx)

    # ê°±ì‹ ëœ answersë¡œ êµì²´
    # ì°¾ì§€ ëª»í•œ ê²½ìš° ë¹ˆ ë¦¬ìŠ¤íŠ¸ê°€ ë¨ -> ì´í›„ í•„í„°ë§ ëŒ€ìƒ
    example["answers"] = {"text": new_text, "answer_start": new_answer_start}
    return example


def retrieve_and_build_dataset(
    retriever: BaseRetrieval,
    dataset: Dataset,
    data_args: DataTrainingArguments,
    split_name: str = "validation",
    is_train: bool = False,
    tokenizer=None,
    top_k_override: int = None,
    use_title_override: Optional[bool] = None,
) -> Dataset:
    """
    Retrieverë¥¼ ì‚¬ìš©í•´ questionì— ë§ëŠ” contextë¥¼ ê²€ìƒ‰í•˜ê³  MRCìš© ë°ì´í„°ì…‹ ìƒì„±.

    Args:
        retriever: ì´ë¯¸ build()ëœ retrieval ê°ì²´
        dataset: ì›ë³¸ ë°ì´í„°ì…‹ (ë‹¨ì¼ split)
        data_args: top_k_retrieval, use_title ë“± ì„¤ì •
        split_name: ë¡œê·¸ìš© split ì´ë¦„
        is_train: Trueì¼ ê²½ìš° ì •ë‹µ ì¬ì •ë ¬(realignment) ë° í•„í„°ë§ ìˆ˜í–‰
        tokenizer: Tokenizer ê°ì²´ (use_title=Trueì¼ ë•Œ sep_token ì‚¬ìš©)
        top_k_override: top_kë¥¼ ê°•ì œ ì§€ì • (Noneì´ë©´ data_argsì—ì„œ ê²°ì •)
        use_title_override: use_title ê°•ì œ ì§€ì • (Noneì´ë©´ data_args.use_title ì‚¬ìš©)

    Returns:
        Retrieved contextê°€ í¬í•¨ëœ Dataset
    """
    # === use_title ê²°ì • ë¡œì§ ===
    use_title = (
        use_title_override
        if use_title_override is not None
        else getattr(data_args, "use_title", True)
    )

    # === Top-k ê²°ì • ë¡œì§ ===
    # 1) top_k_overrideê°€ ì§€ì •ë˜ë©´ ìµœìš°ì„ 
    # 2) is_train=Trueì´ë©´ train_top_k_retrieval (ì—†ìœ¼ë©´ top_k_retrieval)
    # 3) is_train=Falseì´ë©´ infer_top_k_retrieval (ì—†ìœ¼ë©´ top_k_retrieval)
    if top_k_override is not None:
        effective_top_k = top_k_override
        logger.info(f"ğŸ”§ top_k_override specified: {effective_top_k}")
    elif is_train:
        effective_top_k = (
            getattr(data_args, "train_top_k_retrieval", None)
            or data_args.top_k_retrieval
        )
        if getattr(data_args, "train_top_k_retrieval", None):
            logger.info(f"ğŸ“š Using train_top_k_retrieval: {effective_top_k}")
    else:
        effective_top_k = (
            getattr(data_args, "infer_top_k_retrieval", None)
            or data_args.top_k_retrieval
        )
        if getattr(data_args, "infer_top_k_retrieval", None):
            logger.info(f"ğŸ” Using infer_top_k_retrieval: {effective_top_k}")

    logger.info(
        f"ğŸ” Running retrieval on {split_name} split "
        f"(effective_top_k={effective_top_k}, use_title={use_title})..."
    )

    # use_title=Trueì´ë©´ tokenizer í•„ìˆ˜
    if use_title and tokenizer is None:
        logger.warning(
            "âš ï¸ use_title=True but tokenizer not provided. "
            "Title will NOT be included in context. Pass tokenizer to enable title."
        )
        use_title = False

    if use_title and tokenizer:
        logger.info(
            f"ğŸ§© Title enabled. Format: '{{title}} {tokenizer.sep_token} {{passage}}'"
        )
    else:
        logger.info("ğŸ“„ Title disabled. Using passage text only.")

    # 1. Retrieval ìˆ˜í–‰
    # retriever.retrieveëŠ” DataFrameì„ ë°˜í™˜í•¨
    # tokenizerë¥¼ ë„˜ê¸°ë©´ titleì´ í¬í•¨ë¨ (BaseRetrieval.retrieve ë‚´ë¶€ ë¡œì§)
    df = retriever.retrieve(
        dataset, topk=effective_top_k, tokenizer=tokenizer if use_title else None
    )

    # 2. ì‹¤ì œ DataFrameì— answers ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸
    has_answers = "answers" in df.columns

    # 3. HF Features ì •ì˜
    if has_answers:
        used_columns = ["id", "question", "context", "answers"]
        features = Features(
            {
                "id": Value(dtype="string", id=None),
                "question": Value(dtype="string", id=None),
                "context": Value(dtype="string", id=None),
                "answers": Sequence(
                    feature={
                        "text": Value(dtype="string", id=None),
                        "answer_start": Value(dtype="int32", id=None),
                    },
                    length=-1,
                    id=None,
                ),
            }
        )
    else:
        used_columns = ["id", "question", "context"]
        features = Features(
            {
                "id": Value(dtype="string", id=None),
                "question": Value(dtype="string", id=None),
                "context": Value(dtype="string", id=None),
            }
        )

    # 4. í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ë‚¨ê¸°ê³  HF Datasetìœ¼ë¡œ ë³€í™˜
    df = df[used_columns].reset_index(drop=True)
    new_dataset = Dataset.from_pandas(df, features=features)

    # 5. Trainingì¼ ê²½ìš° Answer Realignment ìˆ˜í–‰
    if is_train and has_answers:
        logger.info("ğŸ”„ Realigning answers in retrieved contexts for training...")
        original_len = len(new_dataset)

        # sep_token ê²°ì • (use_titleì´ë©´ tokenizer.sep_token ì‚¬ìš©)
        sep_token = tokenizer.sep_token if (use_title and tokenizer) else None

        # Answer ìœ„ì¹˜ ì¬ê³„ì‚° (title-aware)
        def realign_fn(example):
            return realign_answers_in_retrieved_context(
                example, sep_token=sep_token, use_title=use_title
            )

        new_dataset = new_dataset.map(realign_fn)

        # ì •ë‹µì„ ì°¾ì§€ ëª»í•œ ë°ì´í„°(ë¹ˆ ë¦¬ìŠ¤íŠ¸) í•„í„°ë§
        # answers['text']ê°€ ë¹„ì–´ìˆìœ¼ë©´ ì •ë‹µì´ ì—†ëŠ” ê²ƒ
        def filter_valid_answers(example):
            return len(example["answers"]["text"]) > 0

        new_dataset = new_dataset.filter(filter_valid_answers)

        filtered_len = len(new_dataset)
        lost_count = original_len - filtered_len
        lost_ratio = (lost_count / original_len) * 100 if original_len > 0 else 0

        logger.warning(
            f"ğŸ“‰ Retrieval-Augmented Training Stats:\n"
            f"   - Original: {original_len}\n"
            f"   - Filtered (Answer Found): {filtered_len}\n"
            f"   - Lost: {lost_count} ({lost_ratio:.2f}%)\n"
            f"   * Lost examples means the correct answer was NOT found in top-{effective_top_k} retrieved passages."
        )

        # === Opus í”¼ë“œë°±: lost_ratio ë†’ì„ ë•Œ ê²½ê³  ===
        if lost_ratio > 25.0:
            logger.warning(
                f"âš ï¸ HIGH LOST RATIO ALERT ({lost_ratio:.1f}% > 25%)\n"
                f"   Consider increasing train_top_k_retrieval to 3 or higher.\n"
                f"   Current: top_k={effective_top_k}"
            )

    return new_dataset
