import logging
import os
import sys
import numpy as np
import torch
import evaluate
from typing import NoReturn
from collections import defaultdict

from arguments import DataTrainingArguments, ModelArguments
from datasets import DatasetDict, load_from_disk
from trainer_qa import QuestionAnsweringTrainer
from transformers import (
    AutoConfig,
    AutoModelForQuestionAnswering,
    AutoTokenizer,
    DataCollatorWithPadding,
    EvalPrediction,
    HfArgumentParser,
    TrainingArguments,
    set_seed,
)
from utils_qa import check_no_error, postprocess_qa_predictions


logger = logging.getLogger(__name__)


# ============================================================================
# EDA ê¸°ë°˜ ì¶”ê°€ í•¨ìˆ˜ë“¤
# ============================================================================

def remove_duplicates(dataset):
    """
    EDA ê²°ê³¼: 832ê°œ ì¤‘ë³µ ì¸ë±ìŠ¤ ë°œê²¬
    â†’ í•™ìŠµ ì „ì— ì¤‘ë³µ ì œê±°
    """
    if '__index_level_0__' not in dataset.column_names:
        logger.info("âš ï¸  __index_level_0__ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. ì¤‘ë³µ ì œê±°ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return dataset
    
    index_to_samples = defaultdict(list)
    
    for i, idx in enumerate(dataset['__index_level_0__']):
        index_to_samples[idx].append(i)
    
    # ì¤‘ë³µëœ ê²½ìš° ì²« ë²ˆì§¸ë§Œ ìœ ì§€
    unique_indices = []
    duplicates_removed = 0
    
    for idx, sample_indices in index_to_samples.items():
        unique_indices.append(sample_indices[0])
        if len(sample_indices) > 1:
            duplicates_removed += len(sample_indices) - 1
    
    logger.info(f"âœ… ì¤‘ë³µ ì œê±° ì™„ë£Œ: {duplicates_removed}ê°œ ì œê±°ë¨")
    logger.info(f"   ì›ë³¸: {len(dataset)}ê°œ â†’ ì •ì œ: {len(unique_indices)}ê°œ")
    
    return dataset.select(sorted(unique_indices))


def main():
    parser = HfArgumentParser(
        (ModelArguments, DataTrainingArguments, TrainingArguments)
    )
    model_args, data_args, training_args = parser.parse_args_into_dataclasses()
    print(model_args.model_name_or_path)

    print(f"model is from {model_args.model_name_or_path}")
    print(f"data is from {data_args.dataset_name}")

    # logging ì„¤ì •
    logging.basicConfig(
        format="%(asctime)s - %(levelname)s - %(name)s -    %(message)s",
        datefmt="%m/%d/%Y %H:%M:%S",
        handlers=[logging.StreamHandler(sys.stdout)],
    )

    # verbosity ì„¤ì • : Transformers loggerì˜ ì •ë³´ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤ (on main process only)
    logger.info("Training/evaluation parameters %s", training_args)

    # ëª¨ë¸ì„ ì´ˆê¸°í™”í•˜ê¸° ì „ì— ë‚œìˆ˜ë¥¼ ê³ ì •í•©ë‹ˆë‹¤.
    set_seed(training_args.seed)

    datasets = load_from_disk(data_args.dataset_name)
    print(datasets)

    # AutoConfigë¥¼ ì´ìš©í•˜ì—¬ pretrained model ê³¼ tokenizerë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
    config = AutoConfig.from_pretrained(
        model_args.config_name
        if model_args.config_name is not None
        else model_args.model_name_or_path,
    )
    tokenizer = AutoTokenizer.from_pretrained(
        model_args.tokenizer_name
        if model_args.tokenizer_name is not None
        else model_args.model_name_or_path,
        use_fast=True,
    )
    model = AutoModelForQuestionAnswering.from_pretrained(
        model_args.model_name_or_path,
        from_tf=bool(".ckpt" in model_args.model_name_or_path),
        config=config,
    )

    print(
        type(training_args),
        type(model_args),
        type(datasets),
        type(tokenizer),
        type(model),
    )

    # do_train mrc model í˜¹ì€ do_eval mrc model
    if training_args.do_train or training_args.do_eval:
        run_mrc(data_args, training_args, model_args, datasets, tokenizer, model)


def run_mrc(
    data_args: DataTrainingArguments,
    training_args: TrainingArguments,
    model_args: ModelArguments,
    datasets: DatasetDict,
    tokenizer,
    model,
) -> NoReturn:

    # datasetì„ ì „ì²˜ë¦¬í•©ë‹ˆë‹¤.
    if training_args.do_train:
        column_names = datasets["train"].column_names
    else:
        column_names = datasets["validation"].column_names

    question_column_name = "question" if "question" in column_names else column_names[0]
    context_column_name = "context" if "context" in column_names else column_names[1]
    answer_column_name = "answers" if "answers" in column_names else column_names[2]

    # Paddingì— ëŒ€í•œ ì˜µì…˜ì„ ì„¤ì •í•©ë‹ˆë‹¤.
    pad_on_right = tokenizer.padding_side == "right"

    # ì˜¤ë¥˜ê°€ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
    last_checkpoint, max_seq_length = check_no_error(
        data_args, training_args, datasets, tokenizer
    )

    # ============================================================================
    # EDA ê¸°ë°˜ ê°œì„ ëœ Train preprocessing
    # ============================================================================
    def prepare_train_features(examples):
        """
        EDA ê²°ê³¼ ê¸°ë°˜ ê°œì„ ëœ ì „ì²˜ë¦¬:
        - 39.09%ê°€ 512 í† í° ì´ˆê³¼ â†’ stride ìµœì í™” (ê¶Œì¥: 128)
        - í‰ê·  ë‹µë³€ ê¸¸ì´ 6.28ì â†’ ë‹µë³€ ê¸¸ì´ ê²€ì¦ ì¶”ê°€
        - ë‹µë³€ ìœ„ì¹˜: ì•ë¶€ë¶„ 46%, ì¤‘ê°„ 30%, ë’·ë¶€ë¶„ 23% â†’ ê°€ì¤‘ì¹˜ ì ìš©
        """
        tokenized_examples = tokenizer(
            examples[question_column_name if pad_on_right else context_column_name],
            examples[context_column_name if pad_on_right else question_column_name],
            truncation="only_second" if pad_on_right else "only_first",
            max_length=max_seq_length,
            stride=data_args.doc_stride,
            return_overflowing_tokens=True,
            return_offsets_mapping=True,
            padding="max_length" if data_args.pad_to_max_length else False,
        )

        sample_mapping = tokenized_examples.pop("overflow_to_sample_mapping")
        offset_mapping = tokenized_examples.pop("offset_mapping")

        # ë°ì´í„°ì…‹ì— "start position", "end position" labelì„ ë¶€ì—¬í•©ë‹ˆë‹¤.
        tokenized_examples["start_positions"] = []
        tokenized_examples["end_positions"] = []
        
        # â­ ì¶”ê°€: ìƒ˜í”Œ ê°€ì¤‘ì¹˜ ì €ì¥
        tokenized_examples["sample_weights"] = []
        
        # í†µê³„ ìˆ˜ì§‘ìš©
        long_answer_count = 0
        out_of_span_count = 0
        total_processed = 0

        for i, offsets in enumerate(offset_mapping):
            input_ids = tokenized_examples["input_ids"][i]
            cls_index = input_ids.index(tokenizer.cls_token_id)
            sequence_ids = tokenized_examples.sequence_ids(i)
            sample_index = sample_mapping[i]
            answers = examples[answer_column_name][sample_index]
            
            # â­ ì¶”ê°€: ê¸°ë³¸ ê°€ì¤‘ì¹˜ ì„¤ì •
            weight = 1.0
            total_processed += 1

            if len(answers["answer_start"]) == 0:
                tokenized_examples["start_positions"].append(cls_index)
                tokenized_examples["end_positions"].append(cls_index)
                tokenized_examples["sample_weights"].append(weight)
            else:
                start_char = answers["answer_start"][0]
                end_char = start_char + len(answers["text"][0])
                
                # â­ ì¶”ê°€: ë‹µë³€ ê¸¸ì´ ê²€ì¦ (EDA: í‰ê·  6.28ì, ìµœëŒ€ 83ì)
                answer_length = len(answers["text"][0])
                if answer_length > 100:
                    long_answer_count += 1
                    if long_answer_count <= 3:  # ì²˜ìŒ 3ê°œë§Œ ë¡œê·¸
                        logger.warning(f"âš ï¸  ë¹„ì •ìƒì ìœ¼ë¡œ ê¸´ ë‹µë³€ ë°œê²¬ (ê¸¸ì´: {answer_length})")
                    tokenized_examples["start_positions"].append(cls_index)
                    tokenized_examples["end_positions"].append(cls_index)
                    tokenized_examples["sample_weights"].append(weight)
                    continue
                
                # â­ ì¶”ê°€: ë‹µë³€ ìœ„ì¹˜ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì¡°ì •
                # EDA: ì•ë¶€ë¶„ 46%, ì¤‘ê°„ 30%, ë’·ë¶€ë¶„ 23%
                context_length = len(examples[context_column_name][sample_index])
                relative_position = start_char / context_length if context_length > 0 else 0
                
                if relative_position > 0.66:  # ë’·ë¶€ë¶„ - ë” ì–´ë ¤ì›€
                    weight *= 1.3
                elif relative_position < 0.33:  # ì•ë¶€ë¶„ - ìƒëŒ€ì ìœ¼ë¡œ ì‰¬ì›€
                    weight *= 0.9
                
                # â­ ì¶”ê°€: ë‹µë³€ íƒ€ì… ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì¡°ì •
                answer_text = answers["text"][0]
                # ì¸ëª…, ê¸°ê´€/ì¡°ì§ (í¬ê·€ íƒ€ì…) - ê°€ì¤‘ì¹˜ ì¦ê°€
                if any(word in answer_text for word in ['ëŒ€í†µë ¹', 'ì¥ê´€', 'ì´ë¦¬', 'ì˜ì›', 'ì™•']):
                    weight *= 1.5
                elif any(word in answer_text for word in ['íšŒì‚¬', 'ê¸°ì—…', 'ëŒ€í•™', 'í•™êµ', 'ì •ë¶€', 'ìœ„ì›íšŒ']):
                    weight *= 1.5
                # ìˆ«ì (17.28%)
                elif any(char.isdigit() for char in answer_text):
                    weight *= 1.1

                token_start_index = 0
                while sequence_ids[token_start_index] != (1 if pad_on_right else 0):
                    token_start_index += 1

                token_end_index = len(input_ids) - 1
                while sequence_ids[token_end_index] != (1 if pad_on_right else 0):
                    token_end_index -= 1

                if not (
                    offsets[token_start_index][0] <= start_char
                    and offsets[token_end_index][1] >= end_char
                ):
                    out_of_span_count += 1
                    tokenized_examples["start_positions"].append(cls_index)
                    tokenized_examples["end_positions"].append(cls_index)
                    tokenized_examples["sample_weights"].append(weight)
                else:
                    while (
                        token_start_index < len(offsets)
                        and offsets[token_start_index][0] <= start_char
                    ):
                        token_start_index += 1
                    tokenized_examples["start_positions"].append(token_start_index - 1)
                    
                    while offsets[token_end_index][1] >= end_char:
                        token_end_index -= 1
                    tokenized_examples["end_positions"].append(token_end_index + 1)
                    
                    # â­ ì¶”ê°€: í† í° ë‹¨ìœ„ ë‹µë³€ ê¸¸ì´ ê²€ì¦ ë° ê²½ê³ 
                    predicted_answer_length = (tokenized_examples["end_positions"][-1] - 
                                              tokenized_examples["start_positions"][-1])
                    
                    if predicted_answer_length > 30:  # EDA: í‰ê·  6.28ì â†’ í† í°ìœ¼ë¡œ ì•½ 10ê°œ ì´í•˜
                        if long_answer_count <= 3:  # ì²˜ìŒ 3ê°œë§Œ ë¡œê·¸
                            logger.warning(f"âš ï¸  ê¸´ í† í° ë‹µë³€ ë°œê²¬ (í† í° ìˆ˜: {predicted_answer_length})")
                        long_answer_count += 1
                    
                    tokenized_examples["sample_weights"].append(weight)
        
        # â­ ì¶”ê°€: ì „ì²˜ë¦¬ í†µê³„ ë¡œê¹…
        if total_processed > 0:
            if long_answer_count > 0:
                logger.info(f"ğŸ“Š ì „ì²˜ë¦¬ í†µê³„: ë¹„ì •ìƒì ìœ¼ë¡œ ê¸´ ë‹µë³€ {long_answer_count}ê°œ ë°œê²¬ "
                           f"({long_answer_count/total_processed*100:.2f}%)")
            if out_of_span_count > 0:
                logger.info(f"ğŸ“Š ì „ì²˜ë¦¬ í†µê³„: Span ë²—ì–´ë‚œ ë‹µë³€ {out_of_span_count}ê°œ ë°œê²¬ "
                           f"({out_of_span_count/total_processed*100:.2f}%)")

        return tokenized_examples

    if training_args.do_train:
        if "train" not in datasets:
            raise ValueError("--do_train requires a train dataset")
        train_dataset = datasets["train"]
        
        # â­ ì¶”ê°€: ì¤‘ë³µ ì œê±°
        logger.info("="*80)
        logger.info("ğŸ”„ ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘")
        logger.info("="*80)
        logger.info(f"ì›ë³¸ Train ë°ì´í„°: {len(train_dataset)}ê°œ")
        
        train_dataset = remove_duplicates(train_dataset)
        logger.info(f"ì¤‘ë³µ ì œê±° í›„: {len(train_dataset)}ê°œ")

        # datasetì—ì„œ train featureë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        logger.info("ğŸ”„ í† í°í™” ì§„í–‰ ì¤‘...")
        train_dataset = train_dataset.map(
            prepare_train_features,
            batched=True,
            num_proc=data_args.preprocessing_num_workers,
            remove_columns=column_names,
            load_from_cache_file=not data_args.overwrite_cache,
            desc="Train tokenization",
        )
        
        logger.info(f"âœ… í† í°í™” ì™„ë£Œ: {len(train_dataset)}ê°œ ìƒ˜í”Œ")
        
        # â­ ì¶”ê°€: ìƒ˜í”Œ ê°€ì¤‘ì¹˜ í†µê³„ ì¶œë ¥
        if 'sample_weights' in train_dataset.column_names:
            weights = np.array(train_dataset['sample_weights'])
            logger.info("="*80)
            logger.info("ğŸ“Š ìƒ˜í”Œ ê°€ì¤‘ì¹˜ í†µê³„")
            logger.info("="*80)
            logger.info(f"í‰ê· : {weights.mean():.3f}")
            logger.info(f"ìµœì†Œ: {weights.min():.3f}")
            logger.info(f"ìµœëŒ€: {weights.max():.3f}")
            logger.info(f"í‘œì¤€í¸ì°¨: {weights.std():.3f}")
            logger.info("="*80)

    # Validation preprocessing
    def prepare_validation_features(examples):
        tokenized_examples = tokenizer(
            examples[question_column_name if pad_on_right else context_column_name],
            examples[context_column_name if pad_on_right else question_column_name],
            truncation="only_second" if pad_on_right else "only_first",
            max_length=max_seq_length,
            stride=data_args.doc_stride,
            return_overflowing_tokens=True,
            return_offsets_mapping=True,
            padding="max_length" if data_args.pad_to_max_length else False,
        )

        sample_mapping = tokenized_examples.pop("overflow_to_sample_mapping")

        tokenized_examples["example_id"] = []

        for i in range(len(tokenized_examples["input_ids"])):
            sequence_ids = tokenized_examples.sequence_ids(i)
            context_index = 1 if pad_on_right else 0

            sample_index = sample_mapping[i]
            tokenized_examples["example_id"].append(examples["id"][sample_index])

            tokenized_examples["offset_mapping"][i] = [
                (o if sequence_ids[k] == context_index else None)
                for k, o in enumerate(tokenized_examples["offset_mapping"][i])
            ]
        return tokenized_examples

    if training_args.do_eval:
        eval_dataset = datasets["validation"]

        # Validation Feature ìƒì„±
        eval_dataset = eval_dataset.map(
            prepare_validation_features,
            batched=True,
            num_proc=data_args.preprocessing_num_workers,
            remove_columns=column_names,
            load_from_cache_file=not data_args.overwrite_cache,
            desc="Validation tokenization",
        )

    # Data collator
    data_collator = DataCollatorWithPadding(
        tokenizer, pad_to_multiple_of=8 if training_args.fp16 else None
    )

    # Post-processing:
    def post_processing_function(examples, features, predictions, training_args):
        predictions = postprocess_qa_predictions(
            examples=examples,
            features=features,
            predictions=predictions,
            max_answer_length=data_args.max_answer_length,
            output_dir=training_args.output_dir,
        )
        formatted_predictions = [
            {"id": k, "prediction_text": v} for k, v in predictions.items()
        ]
        if training_args.do_predict:
            return formatted_predictions

        elif training_args.do_eval:
            references = [
                {"id": ex["id"], "answers": ex[answer_column_name]}
                for ex in datasets["validation"]
            ]
            return EvalPrediction(
                predictions=formatted_predictions, label_ids=references
            )

    metric = evaluate.load("squad")

    def compute_metrics(p: EvalPrediction):
        return metric.compute(predictions=p.predictions, references=p.label_ids)

    # Trainer ì´ˆê¸°í™”
    trainer = QuestionAnsweringTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset if training_args.do_train else None,
        eval_dataset=eval_dataset if training_args.do_eval else None,
        eval_examples=datasets["validation"] if training_args.do_eval else None,
        tokenizer=tokenizer,
        data_collator=data_collator,
        post_process_function=post_processing_function,
        compute_metrics=compute_metrics,
    )
    
    
    # Training
    if training_args.do_train:
        if last_checkpoint is not None:
            checkpoint = last_checkpoint
        elif os.path.isdir(model_args.model_name_or_path):
            checkpoint = model_args.model_name_or_path
        else:
            checkpoint = None
        
        logger.info("="*80)
        logger.info("ğŸš€ ëª¨ë¸ í•™ìŠµ ì‹œì‘")
        logger.info("="*80)
        
        train_result = trainer.train(resume_from_checkpoint=checkpoint)
        trainer.save_model()

        metrics = train_result.metrics
        metrics["train_samples"] = len(train_dataset)

        trainer.log_metrics("train", metrics)
        trainer.save_metrics("train", metrics)
        trainer.save_state()

        output_train_file = os.path.join(training_args.output_dir, "train_results.txt")

        with open(output_train_file, "w") as writer:
            logger.info("***** Train results *****")
            for key, value in sorted(train_result.metrics.items()):
                logger.info(f"  {key} = {value}")
                writer.write(f"{key} = {value}\n")

        trainer.state.save_to_json(
            os.path.join(training_args.output_dir, "trainer_state.json")
        )

    # Evaluation
    if training_args.do_eval:
        logger.info("*** Evaluate ***")
        metrics = trainer.evaluate()

        metrics["eval_samples"] = len(eval_dataset)

        trainer.log_metrics("eval", metrics)
        trainer.save_metrics("eval", metrics)


if __name__ == "__main__":
    main()