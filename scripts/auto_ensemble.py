#!/usr/bin/env python
"""
ìë™ ìµœì  ì•™ìƒë¸” íƒìƒ‰ ë° ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

ë©”íƒ€ë°ì´í„°(nbest_predictions.json, eval_results.json)ë§Œ ìˆìœ¼ë©´
ìµœì ì˜ ì•™ìƒë¸” ì¡°í•©ì„ ìë™ìœ¼ë¡œ ì°¾ì•„ì„œ ì œì¶œ íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    # Val ê¸°ì¤€ ìµœì  ì¡°í•© íƒìƒ‰ë§Œ
    python scripts/auto_ensemble.py --mode search

    # Val ê¸°ì¤€ ìµœì  ì¡°í•©ìœ¼ë¡œ Test ì•™ìƒë¸” ì‹¤í–‰
    python scripts/auto_ensemble.py --mode run

    # Test nbest ìˆëŠ” ëª¨ë¸ë§Œ ì‚¬ìš©
    python scripts/auto_ensemble.py --mode run --test-only

    # íŠ¹ì • ëª¨ë¸ë“¤ë¡œ ì œí•œ
    python scripts/auto_ensemble.py --mode run --models oceann315 HANTAEK_roberta_large_vanilla roberta-large
"""

import argparse
import json
import os
import sys
from itertools import combinations
from pathlib import Path
from typing import Dict, List, Optional, Tuple

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
sys.path.insert(0, str(Path(__file__).resolve().parent.parent))
from src.utils import get_logger

logger = get_logger(__name__)


class AutoEnsemble:
    def __init__(
        self,
        models_dir: str = "/data/ephemeral/home/shared/outputs/dahyeong",
        data_dir: str = "./data",
        output_dir: str = "./outputs/ensemble",
    ):
        self.models_dir = Path(models_dir)
        self.data_dir = Path(data_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # ì •ë‹µ ë¡œë“œ
        self.answers = self._load_answers()

        # ëª¨ë¸ ë©”íƒ€ë°ì´í„° ë¡œë“œ
        self.models = self._scan_models()

    def _load_answers(self) -> Dict[str, List[str]]:
        """Validation ì •ë‹µ ë¡œë“œ"""
        from datasets import load_from_disk

        ds = load_from_disk(str(self.data_dir / "train_dataset"))
        return {ex["id"]: ex["answers"]["text"] for ex in ds["validation"]}

    def _load_test_order(self) -> List[str]:
        """Test dataset ì›ë³¸ ìˆœì„œ ë¡œë“œ"""
        from datasets import load_from_disk

        ds = load_from_disk(str(self.data_dir / "test_dataset"))
        return ds["validation"]["id"]

    def _scan_models(self) -> Dict[str, dict]:
        """ëª¨ë¸ ë””ë ‰í† ë¦¬ ìŠ¤ìº”í•˜ì—¬ ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘"""
        models = {}

        for d in self.models_dir.iterdir():
            if not d.is_dir():
                continue

            val_nbest = d / "nbest_predictions.json"
            test_nbest = d / "nbest_predictions_test.json"
            eval_file = d / "eval_results.json"

            if not val_nbest.exists():
                continue

            model_info = {
                "name": d.name,
                "path": str(d),
                "has_val_nbest": val_nbest.exists(),
                "has_test_nbest": test_nbest.exists(),
                "val_em": None,
                "val_nbest_path": str(val_nbest) if val_nbest.exists() else None,
                "test_nbest_path": str(test_nbest) if test_nbest.exists() else None,
            }

            # Val EM ë¡œë“œ
            if eval_file.exists():
                try:
                    r = json.loads(eval_file.read_text())
                    model_info["val_em"] = r.get(
                        "eval_exact_match", r.get("exact_match", 0)
                    )
                except:
                    pass

            # nbest ê°œìˆ˜ í™•ì¸
            try:
                val_data = json.loads(val_nbest.read_text())
                model_info["val_count"] = len(val_data)
            except:
                model_info["val_count"] = 0

            if test_nbest.exists():
                try:
                    test_data = json.loads(test_nbest.read_text())
                    model_info["test_count"] = len(test_data)
                except:
                    model_info["test_count"] = 0
            else:
                model_info["test_count"] = 0

            # ìœ íš¨í•œ ëª¨ë¸ë§Œ ì¶”ê°€ (val 240ê°œ)
            if model_info["val_count"] == 240:
                models[d.name] = model_info

        return models

    def get_available_models(self, test_only: bool = False) -> List[str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ë°˜í™˜"""
        if test_only:
            return [
                m
                for m, info in self.models.items()
                if info["has_test_nbest"] and info["test_count"] == 600
            ]
        return list(self.models.keys())

    def print_models(self):
        """ëª¨ë¸ ëª©ë¡ ì¶œë ¥"""
        print("\n" + "=" * 70)
        print("ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡")
        print("=" * 70)
        print(f"{'ëª¨ë¸ëª…':<40} | {'Val EM':>8} | {'Test':>6}")
        print("-" * 70)

        sorted_models = sorted(
            self.models.items(), key=lambda x: -(x[1]["val_em"] or 0)
        )

        for name, info in sorted_models:
            em_str = f"{info['val_em']:.2f}%" if info["val_em"] else "N/A"
            test_str = (
                "âœ…" if info["has_test_nbest"] and info["test_count"] == 600 else "âŒ"
            )
            print(f"{name:<40} | {em_str:>8} | {test_str:>6}")

        print("-" * 70)
        print(f"ì´ {len(self.models)}ê°œ ëª¨ë¸")
        test_available = len(self.get_available_models(test_only=True))
        print(f"Test ì•™ìƒë¸” ê°€ëŠ¥: {test_available}ê°œ")
        print()

    def _load_nbest(self, model_name: str, use_test: bool = False) -> dict:
        """nbest predictions ë¡œë“œ"""
        info = self.models[model_name]
        path = info["test_nbest_path"] if use_test else info["val_nbest_path"]
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)

    def _ensemble_predict(
        self,
        model_weights: List[Tuple[str, float]],
        use_test: bool = False,
        top_k: int = 5,
    ) -> Dict[str, str]:
        """ì•™ìƒë¸” ì˜ˆì¸¡ ìˆ˜í–‰"""
        # nbest ë¡œë“œ
        nbest_data = {m: self._load_nbest(m, use_test) for m, _ in model_weights}

        predictions = {}
        first_model = model_weights[0][0]

        for qid in nbest_data[first_model].keys():
            vote = {}

            for model_name, weight in model_weights:
                nbest = nbest_data[model_name].get(qid, [])
                for pred in nbest[:top_k]:
                    text = pred["text"]
                    prob = pred.get("probability", pred.get("score", 0))
                    if text not in vote:
                        vote[text] = 0
                    vote[text] += prob * weight

            if vote:
                predictions[qid] = max(vote.items(), key=lambda x: x[1])[0]
            else:
                predictions[qid] = ""

        return predictions

    def _calc_em(self, predictions: Dict[str, str]) -> float:
        """EM ê³„ì‚°"""
        correct = 0
        total = 0
        for qid, pred in predictions.items():
            if qid in self.answers:
                total += 1
                if pred in self.answers[qid]:
                    correct += 1
        return (correct / total * 100) if total > 0 else 0

    def search_best_combinations(
        self,
        model_names: Optional[List[str]] = None,
        test_only: bool = False,
        max_models: int = 3,
        top_n: int = 10,
    ) -> List[Tuple[float, List[Tuple[str, float]]]]:
        """ìµœì  ì•™ìƒë¸” ì¡°í•© íƒìƒ‰"""

        if model_names:
            available = [m for m in model_names if m in self.models]
        else:
            available = self.get_available_models(test_only=test_only)

        if len(available) < 2:
            logger.error(f"ì•™ìƒë¸”ì— í•„ìš”í•œ ëª¨ë¸ì´ ë¶€ì¡±í•©ë‹ˆë‹¤: {len(available)}ê°œ")
            return []

        logger.info(f"ğŸ” {len(available)}ê°œ ëª¨ë¸ë¡œ ìµœì  ì¡°í•© íƒìƒ‰...")

        all_results = []

        # 2ê°œ ì¡°í•©
        logger.info("  2ê°œ ëª¨ë¸ ì¡°í•© íƒìƒ‰ ì¤‘...")
        for m1, m2 in combinations(available, 2):
            for w1 in [0.4, 0.5, 0.6, 0.7]:
                w2 = round(1 - w1, 2)
                preds = self._ensemble_predict([(m1, w1), (m2, w2)])
                em = self._calc_em(preds)
                all_results.append((em, [(m1, w1), (m2, w2)]))

        # 3ê°œ ì¡°í•©
        if max_models >= 3 and len(available) >= 3:
            logger.info("  3ê°œ ëª¨ë¸ ì¡°í•© íƒìƒ‰ ì¤‘...")
            for m1, m2, m3 in combinations(available, 3):
                for w1 in [0.3, 0.4, 0.5, 0.6]:
                    for w2 in [0.2, 0.3, 0.4]:
                        w3 = round(1 - w1 - w2, 2)
                        if w3 > 0.05:
                            preds = self._ensemble_predict(
                                [(m1, w1), (m2, w2), (m3, w3)]
                            )
                            em = self._calc_em(preds)
                            all_results.append((em, [(m1, w1), (m2, w2), (m3, w3)]))

        # ì •ë ¬
        all_results.sort(key=lambda x: -x[0])

        return all_results[:top_n]

    def print_search_results(
        self, results: List[Tuple[float, List[Tuple[str, float]]]]
    ):
        """íƒìƒ‰ ê²°ê³¼ ì¶œë ¥"""
        print("\n" + "=" * 70)
        print("ğŸ† ìµœì  ì•™ìƒë¸” ì¡°í•© (Val EM ê¸°ì¤€)")
        print("=" * 70)

        for i, (em, weights) in enumerate(results, 1):
            weight_str = " + ".join([f"{m}({w:.1f})" for m, w in weights])
            print(f"  {i:2d}. {em:.2f}% | {weight_str}")

        print("=" * 70)
        print()

    def run_ensemble(
        self,
        model_weights: List[Tuple[str, float]],
        output_name: str,
        use_test: bool = True,
    ) -> dict:
        """ì•™ìƒë¸” ì‹¤í–‰ ë° ê²°ê³¼ ì €ì¥"""

        logger.info(f"ğŸ”€ ì•™ìƒë¸” ì‹¤í–‰: {output_name}")
        for m, w in model_weights:
            logger.info(f"   - {m}: {w:.1%}")

        # ì˜ˆì¸¡
        predictions = self._ensemble_predict(model_weights, use_test=use_test)
        logger.info(f"âœ… ì˜ˆì¸¡ ì™„ë£Œ: {len(predictions)}ê°œ")

        # ì¶œë ¥ ë””ë ‰í† ë¦¬
        out_dir = self.output_dir / output_name
        out_dir.mkdir(parents=True, exist_ok=True)

        # predictions.json ì €ì¥
        pred_path = out_dir / "predictions.json"
        with open(pred_path, "w", encoding="utf-8") as f:
            json.dump(predictions, f, ensure_ascii=False, indent=2)
        logger.info(f"ğŸ’¾ {pred_path}")

        # CSV ì €ì¥ (ì›ë³¸ ìˆœì„œ)
        csv_path = out_dir / "predictions_submit.csv"
        if use_test:
            order = self._load_test_order()
        else:
            order = list(self.answers.keys())

        with open(csv_path, "w", encoding="utf-8") as f:
            for qid in order:
                answer = predictions.get(qid, "")
                f.write(f"{qid}\t{answer}\n")
        logger.info(f"ğŸ’¾ {csv_path}")

        # config.json ì €ì¥
        config = {
            "ensemble_type": "auto_ensemble",
            "models": [m for m, _ in model_weights],
            "weights": [w for _, w in model_weights],
            "use_test": use_test,
            "prediction_count": len(predictions),
        }
        config_path = out_dir / "config.json"
        with open(config_path, "w", encoding="utf-8") as f:
            json.dump(config, f, ensure_ascii=False, indent=2)

        # Val ê¸°ì¤€ EM ê³„ì‚° (testê°€ ì•„ë‹ ë•Œë§Œ)
        result = {"output_dir": str(out_dir), "predictions": len(predictions)}

        if not use_test:
            em = self._calc_em(predictions)
            result["val_em"] = em

            eval_results = {"eval_exact_match": em, "eval_total": len(predictions)}
            eval_path = out_dir / "eval_results.json"
            with open(eval_path, "w", encoding="utf-8") as f:
                json.dump(eval_results, f, ensure_ascii=False, indent=2)
            logger.info(f"ğŸ“Š Val EM: {em:.2f}%")

        return result

    def auto_run(
        self, test_only: bool = True, output_name: Optional[str] = None
    ) -> dict:
        """ìë™ìœ¼ë¡œ ìµœì  ì¡°í•© ì°¾ì•„ì„œ ì‹¤í–‰"""

        # ìµœì  ì¡°í•© íƒìƒ‰
        results = self.search_best_combinations(test_only=test_only)

        if not results:
            logger.error("ìœ íš¨í•œ ì•™ìƒë¸” ì¡°í•©ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return {}

        self.print_search_results(results)

        # ìµœê³  ì¡°í•© ì„ íƒ
        best_em, best_weights = results[0]

        # ì¶œë ¥ ì´ë¦„ ìƒì„±
        if output_name is None:
            model_abbrevs = [m[:10] for m, _ in best_weights]
            output_name = f"auto_{'_'.join(model_abbrevs)}"

        # ì‹¤í–‰
        return self.run_ensemble(
            model_weights=best_weights, output_name=output_name, use_test=test_only
        )


def main():
    parser = argparse.ArgumentParser(description="ìë™ ìµœì  ì•™ìƒë¸” íƒìƒ‰ ë° ì‹¤í–‰")
    parser.add_argument(
        "--mode",
        choices=["search", "run", "list"],
        default="search",
        help="ì‹¤í–‰ ëª¨ë“œ: search(íƒìƒ‰ë§Œ), run(íƒìƒ‰+ì‹¤í–‰), list(ëª¨ë¸ëª©ë¡)",
    )
    parser.add_argument(
        "--test-only", action="store_true", help="Test nbestê°€ ìˆëŠ” ëª¨ë¸ë§Œ ì‚¬ìš©"
    )
    parser.add_argument("--models", nargs="+", help="íŠ¹ì • ëª¨ë¸ë“¤ë§Œ ì‚¬ìš©")
    parser.add_argument(
        "--weights",
        nargs="+",
        type=float,
        help="--modelsì™€ í•¨ê»˜ ì‚¬ìš©í•  ê°€ì¤‘ì¹˜ (ìˆœì„œëŒ€ë¡œ)",
    )
    parser.add_argument("--output-name", help="ì¶œë ¥ ë””ë ‰í† ë¦¬ ì´ë¦„")
    parser.add_argument(
        "--models-dir",
        default="/data/ephemeral/home/shared/outputs/dahyeong",
        help="ëª¨ë¸ ì¶œë ¥ ë””ë ‰í† ë¦¬",
    )
    parser.add_argument("--top-n", type=int, default=10, help="ìƒìœ„ Nê°œ ì¡°í•© ì¶œë ¥")

    args = parser.parse_args()

    # AutoEnsemble ì´ˆê¸°í™”
    auto_ens = AutoEnsemble(models_dir=args.models_dir)

    if args.mode == "list":
        auto_ens.print_models()
        return

    if args.mode == "search":
        results = auto_ens.search_best_combinations(
            model_names=args.models, test_only=args.test_only, top_n=args.top_n
        )
        auto_ens.print_search_results(results)

    elif args.mode == "run":
        if args.models and args.weights:
            # ìˆ˜ë™ ì§€ì •
            if len(args.models) != len(args.weights):
                logger.error("--modelsì™€ --weights ê°œìˆ˜ê°€ ì¼ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.")
                return

            model_weights = list(zip(args.models, args.weights))
            auto_ens.run_ensemble(
                model_weights=model_weights,
                output_name=args.output_name or "manual_ensemble",
                use_test=args.test_only,
            )
        else:
            # ìë™ íƒìƒ‰ í›„ ì‹¤í–‰
            auto_ens.auto_run(test_only=args.test_only, output_name=args.output_name)


if __name__ == "__main__":
    main()
