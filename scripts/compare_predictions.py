"""
Validation ì •ë‹µê³¼ ì˜ˆì¸¡ ê²°ê³¼ ë¹„êµ ìŠ¤í¬ë¦½íŠ¸

Usage:
    python scripts/compare_predictions.py
    python scripts/compare_predictions.py --predictions ./outputs/.../predictions_val.json
"""

import argparse
import json
from pathlib import Path
import pandas as pd
from datasets import load_from_disk


def compare_predictions(
    predictions_path: str,
    dataset_path: str = "./data/train_dataset",
    output_path: str = None
):
    """
    Validation ì •ë‹µê³¼ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë¹„êµí•˜ì—¬ CSVë¡œ ì €ì¥
    
    Args:
        predictions_path: predictions_val.json ê²½ë¡œ
        dataset_path: train_dataset ê²½ë¡œ (validation split í¬í•¨)
        output_path: ì¶œë ¥ CSV ê²½ë¡œ (Noneì´ë©´ ìë™ ìƒì„±)
    """
    print("=" * 80)
    print("ğŸ“Š Prediction Comparison Tool")
    print("=" * 80)
    
    # 1. Dataset ë¡œë“œ
    print(f"\n[1/4] Loading dataset from {dataset_path}...")
    ds = load_from_disk(dataset_path)
    val_ds = ds['validation']
    print(f"   âœ“ Loaded {len(val_ds)} validation samples")
    
    # 2. Predictions ë¡œë“œ
    print(f"\n[2/4] Loading predictions from {predictions_path}...")
    with open(predictions_path, 'r', encoding='utf-8') as f:
        predictions = json.load(f)
    print(f"   âœ“ Loaded {len(predictions)} predictions")
    
    # 2-1. val_results.jsonì—ì„œ ì „ì²´ EM/F1 ì ìˆ˜ ë¡œë“œ
    results_path = Path(predictions_path).parent / "val_results.json"
    overall_em = overall_f1 = None
    if results_path.exists():
        with open(results_path, 'r', encoding='utf-8') as f:
            results = json.load(f)
            overall_em = results.get('eval_exact_match', 0)
            overall_f1 = results.get('eval_f1', 0)
        print(f"   âœ“ Loaded overall metrics: EM={overall_em:.2f}%, F1={overall_f1:.2f}")
    
    # 3. ë¹„êµ ë°ì´í„° ìƒì„±
    print(f"\n[3/4] Comparing predictions with ground truth answers...")
    comparison_data = []
    correct_count = 0
    
    for ex in val_ds:
            qid = ex['id']
            question = ex['question']
            context = ex.get('context', '')
            ground_truth = ex['answers']['text']
            # gold_pred: gold contextì—ì„œ ì¶”ë¡ í•œ ì •ë‹µ (ì‹¤ì œ gold context ê¸°ë°˜ ì˜ˆì¸¡ì´ ìˆìœ¼ë©´ ì—¬ê¸°ì„œ ë¡œë“œ)
            # retrieval_pred: retrievalë¡œ ì°¾ì€ contextì—ì„œ ì¶”ë¡ í•œ ì •ë‹µ
            retrieval_pred = predictions.get(qid, "")
            # gold_predëŠ” ë³„ë„ predictions_gold.json ë“±ì—ì„œ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆìŒ (í˜„ì¬ëŠ” ë¯¸ì‚¬ìš©)
            comparison_data.append({
                "index": qid,
                "question": question,
                "context_snippet": context[:100] + "..." if len(context) > 100 else context,
                "ground_truth": " | ".join(ground_truth),
                "retrieval_pred": retrieval_pred,
            })
    
    # EM/F1 ê³„ì‚°ì€ val_results.jsonì—ì„œë§Œ ë¡œë“œ
    
    # 4. CSV ì €ì¥
    df = pd.DataFrame(comparison_data)
    if output_path is None:
        pred_dir = Path(predictions_path).parent
        output_path = pred_dir / "val_comparison_detailed.csv"
    df.to_csv(output_path, index=False, encoding='utf-8-sig')

    # simple: ground_truth, retrieval_pred
    simple_df = df[['ground_truth', 'retrieval_pred']].copy()
    simple_output = Path(output_path).parent / "val_comparison_simple.csv"
    simple_df.to_csv(simple_output, index=False, encoding='utf-8-sig')

    # wrong_only: ground_truth, retrieval_pred (ì •ë‹µê³¼ ì¼ì¹˜í•˜ì§€ ì•ŠëŠ” ê²ƒë§Œ)
    def is_wrong(row):
        return row['retrieval_pred'] not in row['ground_truth'].split(' | ')
    wrong_df = df[simple_df.apply(is_wrong, axis=1)].copy()
    wrong_output = Path(output_path).parent / "val_comparison_wrong_only.csv"
    wrong_df[['ground_truth', 'retrieval_pred']].to_csv(wrong_output, index=False, encoding='utf-8-sig')

    print(f"\n[4/4] Saved comparison results:")
    print(f"   ğŸ“„ Detailed: {output_path}")
    print(f"   ğŸ“„ Simple: {simple_output}")
    print(f"   ğŸ“„ Wrong only: {wrong_output}")
    print(f"\n   ğŸ“Š Official Metrics (from val_results.json):")
    if overall_em is not None and overall_f1 is not None:
        print(f"      EM: {overall_em:.2f}%")
        print(f"      F1: {overall_f1:.2f}")
    print("\n" + "=" * 80)
    return output_path


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Compare validation predictions with gold answers")
    parser.add_argument(
        "--predictions",
        type=str,
        default="./outputs/dahyeong/HANTAEK_rob-large-kq-v1-qa-finetuned_stride64/predictions_val.json",
        help="Path to predictions_val.json"
    )
    parser.add_argument(
        "--dataset",
        type=str,
        default="./data/train_dataset",
        help="Path to train_dataset (containing validation split)"
    )
    parser.add_argument(
        "--output",
        type=str,
        default=None,
        help="Output CSV path (default: same dir as predictions)"
    )
    
    args = parser.parse_args()
    
    compare_predictions(
        predictions_path=args.predictions,
        dataset_path=args.dataset,
        output_path=args.output
    )
