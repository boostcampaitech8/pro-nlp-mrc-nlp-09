#!/usr/bin/env python2
"""
ë‘ ê°œì˜ prediction CSV íŒŒì¼ì„ ë¹„êµ ë¶„ì„í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸

Usage:
    python analyze_predictions.py <csv1> <csv2> [--output-dir <dir>]
"""

import argparse
import csv
import re
from pathlib import Path
from typing import Dict, List, Tuple
from collections import Counter, defaultdict
from datetime import datetime


def normalize_answer(s: str) -> str:
    """ì •ë‹µ ë¬¸ìì—´ì„ ì •ê·œí™”í•©ë‹ˆë‹¤."""

    def remove_articles(text):
        return re.sub(r"\b(a|an|the)\b", " ", text)

    def white_space_fix(text):
        return " ".join(text.split())

    def remove_punc(text):
        # í•œê¸€, ì˜ë¬¸, ìˆ«ìë¥¼ ì œì™¸í•œ ë¬¸ì ì œê±°
        return re.sub(r"[^\w\s]", "", text)

    def lower(text):
        return text.lower()

    return white_space_fix(remove_articles(remove_punc(lower(s))))


def compute_exact_match(prediction: str, ground_truth: str) -> float:
    """Exact Match ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    return float(normalize_answer(prediction) == normalize_answer(ground_truth))


def compute_f1(prediction: str, ground_truth: str) -> float:
    """F1 ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    pred_tokens = normalize_answer(prediction).split()
    truth_tokens = normalize_answer(ground_truth).split()

    if len(pred_tokens) == 0 or len(truth_tokens) == 0:
        return float(pred_tokens == truth_tokens)

    common_tokens = Counter(pred_tokens) & Counter(truth_tokens)
    num_common = sum(common_tokens.values())

    if num_common == 0:
        return 0.0

    precision = num_common / len(pred_tokens)
    recall = num_common / len(truth_tokens)
    f1 = 2 * (precision * recall) / (precision + recall)

    return f1


def read_csv(file_path: Path) -> Dict[str, str]:
    """CSV íŒŒì¼ì„ ì½ì–´ {id: prediction} ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    predictions = {}
    with open(file_path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue

            # íƒ­, ì‰¼í‘œ, ê³µë°± ë“±ìœ¼ë¡œ ë¶„ë¦¬ ì‹œë„
            parts = None
            if "\t" in line:
                parts = line.split("\t", 1)
            elif "," in line:
                parts = line.split(",", 1)
            else:
                # ê³µë°±ìœ¼ë¡œ ë¶„ë¦¬ (ìµœì†Œ 2ê°œ ì´ìƒì˜ ê³µë°±)
                parts = line.split(None, 1)

            if parts and len(parts) >= 2:
                predictions[parts[0].strip()] = parts[1].strip()

    return predictions


def analyze_predictions(csv1_path: Path, csv2_path: Path) -> Tuple[List[Dict], Dict]:
    """ë‘ CSV íŒŒì¼ì„ ë¶„ì„í•˜ì—¬ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    pred1 = read_csv(csv1_path)
    pred2 = read_csv(csv2_path)

    # íŒŒì¼ ê¸¸ì´ ì²´í¬
    len1 = len(pred1)
    len2 = len(pred2)

    if len1 != len2:
        print(f"\nâš ï¸  ê²½ê³ : ë‘ CSV íŒŒì¼ì˜ í–‰ ê°œìˆ˜ê°€ ë‹¤ë¦…ë‹ˆë‹¤!")
        print(f"  - CSV 1: {len1}ê°œ")
        print(f"  - CSV 2: {len2}ê°œ")

        # ë°ì´í„°ì…‹ íƒ€ì… ì¶”ë¡ 
        if len1 == 240 or len2 == 240:
            print(f"  â†’ Validation ë°ì´í„°ì…‹ìœ¼ë¡œ ì¶”ì •ë©ë‹ˆë‹¤ (ê¸°ëŒ€ê°’: 240ê°œ)")
        elif len1 == 600 or len2 == 600:
            print(f"  â†’ Test ë°ì´í„°ì…‹ìœ¼ë¡œ ì¶”ì •ë©ë‹ˆë‹¤ (ê¸°ëŒ€ê°’: 600ê°œ)")
        print()

    # ëª¨ë“  ID ìˆ˜ì§‘
    all_ids = sorted(set(pred1.keys()) | set(pred2.keys()))

    results = []
    stats = {
        "total": len(all_ids),
        "both_correct": 0,
        "only_pred1_correct": 0,
        "only_pred2_correct": 0,
        "both_wrong": 0,
        "agreement": 0,
        "disagreement": 0,
        "pred1_em_sum": 0,
        "pred2_em_sum": 0,
        "pred1_f1_sum": 0,
        "pred2_f1_sum": 0,
        "errors": [],
        "answer_length_dist": defaultdict(int),
        "diff_patterns": defaultdict(int),
    }

    for qid in all_ids:
        p1 = pred1.get(qid, "")
        p2 = pred2.get(qid, "")

        # ë‘ ì˜ˆì¸¡ì´ ë™ì¼í•œì§€ í™•ì¸
        em_between = compute_exact_match(p1, p2)
        f1_between = compute_f1(p1, p2)

        result = {
            "id": qid,
            "pred1": p1,
            "pred2": p2,
            "em": em_between,
            "f1": f1_between,
        }

        results.append(result)

        # í†µê³„ ìˆ˜ì§‘
        stats["pred1_em_sum"] += em_between
        stats["pred1_f1_sum"] += f1_between

        if em_between == 1.0:
            stats["agreement"] += 1
        else:
            stats["disagreement"] += 1
            stats["errors"].append(
                {
                    "id": qid,
                    "pred1": p1,
                    "pred2": p2,
                    "f1": f1_between,
                }
            )

        # ë‹µë³€ ê¸¸ì´ ë¶„í¬
        len1 = len(p1.strip())
        len2 = len(p2.strip())
        stats["answer_length_dist"][f"{len1}-{len2}"] += 1

        # ì°¨ì´ íŒ¨í„´ ë¶„ì„
        if p1 and p2:
            if len(p1) > len(p2) * 2:
                stats["diff_patterns"]["pred1_much_longer"] += 1
            elif len(p2) > len(p1) * 2:
                stats["diff_patterns"]["pred2_much_longer"] += 1
            elif p1[:10] == p2[:10]:
                stats["diff_patterns"]["same_prefix"] += 1
            elif p1[-10:] == p2[-10:]:
                stats["diff_patterns"]["same_suffix"] += 1

    return results, stats


def save_comparison_csv(results: List[Dict], output_path: Path):
    """ë¹„êµ ê²°ê³¼ë¥¼ CSVë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    with open(output_path, "w", encoding="utf-8", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(["id", "pred1", "pred2", "EM", "F1"])

        for result in results:
            writer.writerow(
                [
                    result["id"],
                    result["pred1"],
                    result["pred2"],
                    f"{result['em']:.4f}",
                    f"{result['f1']:.4f}",
                ]
            )


def generate_analysis_report(
    results: List[Dict], stats: Dict, csv1_name: str, csv2_name: str, output_path: Path
):
    """ë¶„ì„ ë³´ê³ ì„œë¥¼ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤."""
    total = stats["total"]
    agreement_rate = stats["agreement"] / total * 100 if total > 0 else 0
    avg_f1 = stats["pred1_f1_sum"] / total if total > 0 else 0

    with open(output_path, "w", encoding="utf-8") as f:
        f.write(f"# Prediction ë¹„êµ ë¶„ì„ ë³´ê³ ì„œ\n\n")
        f.write(f"**ìƒì„± ì‹œê°**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        f.write(f"**ë¹„êµ íŒŒì¼**:\n")
        f.write(f"- Prediction 1: `{csv1_name}`\n")
        f.write(f"- Prediction 2: `{csv2_name}`\n\n")

        f.write(f"---\n\n")
        f.write(f"## ğŸ“Š ì „ì²´ í†µê³„\n\n")
        f.write(f"| í•­ëª© | ê°’ |\n")
        f.write(f"|------|------|\n")
        f.write(f"| ì „ì²´ ì˜ˆì‹œ ìˆ˜ | {total} |\n")
        f.write(
            f"| ì¼ì¹˜ (Agreement) | {stats['agreement']} ({agreement_rate:.2f}%) |\n"
        )
        f.write(
            f"| ë¶ˆì¼ì¹˜ (Disagreement) | {stats['disagreement']} ({100 - agreement_rate:.2f}%) |\n"
        )
        f.write(f"| í‰ê·  F1 Score | {avg_f1:.4f} |\n\n")

        f.write(f"---\n\n")
        f.write(f"## ğŸ” ë¶ˆì¼ì¹˜ ì˜ˆì‹œ ë¶„ì„\n\n")
        f.write(
            f"ë‘ predictionì´ ë‹¤ë¥¸ ê²½ìš°ëŠ” ì´ **{stats['disagreement']}ê°œ**ì…ë‹ˆë‹¤.\n\n"
        )

        if stats["errors"]:
            # F1 ì ìˆ˜ê°€ ë‚®ì€ ìˆœìœ¼ë¡œ ì •ë ¬
            sorted_errors = sorted(stats["errors"], key=lambda x: x["f1"])

            f.write(f"### Top 20 ë¶ˆì¼ì¹˜ ì˜ˆì‹œ (F1 ë‚®ì€ ìˆœ)\n\n")
            for i, error in enumerate(sorted_errors[:20], 1):
                f.write(f"#### {i}. ID: `{error['id']}`\n\n")
                f.write(f"- **Prediction 1**: {error['pred1']}\n")
                f.write(f"- **Prediction 2**: {error['pred2']}\n")
                f.write(f"- **F1 Score**: {error['f1']:.4f}\n\n")

        f.write(f"---\n\n")
        f.write(f"## ğŸ“ˆ ì°¨ì´ íŒ¨í„´ ë¶„ì„\n\n")

        if stats["diff_patterns"]:
            f.write(f"| íŒ¨í„´ | ë¹ˆë„ |\n")
            f.write(f"|------|------|\n")
            for pattern, count in sorted(
                stats["diff_patterns"].items(), key=lambda x: x[1], reverse=True
            ):
                pattern_name = {
                    "pred1_much_longer": "Pred1ì´ í›¨ì”¬ ê¸º (2ë°° ì´ìƒ)",
                    "pred2_much_longer": "Pred2ê°€ í›¨ì”¬ ê¸º (2ë°° ì´ìƒ)",
                    "same_prefix": "ë™ì¼í•œ ì ‘ë‘ì‚¬",
                    "same_suffix": "ë™ì¼í•œ ì ‘ë¯¸ì‚¬",
                }.get(pattern, pattern)
                f.write(f"| {pattern_name} | {count} |\n")
            f.write(f"\n")

        f.write(f"---\n\n")
        f.write(f"## ğŸ“ ë‹µë³€ ê¸¸ì´ ë¶„í¬\n\n")
        f.write(f"ìƒìœ„ 10ê°œ (Pred1 ê¸¸ì´ - Pred2 ê¸¸ì´ ìŒ):\n\n")

        if stats["answer_length_dist"]:
            sorted_lengths = sorted(
                stats["answer_length_dist"].items(), key=lambda x: x[1], reverse=True
            )[:10]
            f.write(f"| Pred1 ê¸¸ì´ - Pred2 ê¸¸ì´ | ë¹ˆë„ |\n")
            f.write(f"|-------------------------|------|\n")
            for length_pair, count in sorted_lengths:
                f.write(f"| {length_pair} | {count} |\n")
            f.write(f"\n")

        f.write(f"---\n\n")
        f.write(f"## ğŸ’¡ ì¸ì‚¬ì´íŠ¸ ë° ê¶Œì¥ì‚¬í•­\n\n")

        # ìë™ ì¸ì‚¬ì´íŠ¸ ìƒì„±
        insights = []

        if agreement_rate > 90:
            insights.append(
                f"âœ… ë‘ ëª¨ë¸ì˜ ì˜ˆì¸¡ì´ {agreement_rate:.1f}% ì¼ì¹˜í•˜ì—¬ ë§¤ìš° ë†’ì€ ì¼ê´€ì„±ì„ ë³´ì…ë‹ˆë‹¤."
            )
        elif agreement_rate > 70:
            insights.append(
                f"âš ï¸ ë‘ ëª¨ë¸ì˜ ì˜ˆì¸¡ì´ {agreement_rate:.1f}% ì¼ì¹˜í•©ë‹ˆë‹¤. ë¶ˆì¼ì¹˜ ì¼€ì´ìŠ¤ë¥¼ ê²€í† í•˜ì—¬ ê°œì„ ì ì„ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            )
        else:
            insights.append(
                f"ğŸš¨ ë‘ ëª¨ë¸ì˜ ì˜ˆì¸¡ì´ {agreement_rate:.1f}%ë§Œ ì¼ì¹˜í•©ë‹ˆë‹¤. í° ì°¨ì´ê°€ ìˆìœ¼ë¯€ë¡œ ì›ì¸ ë¶„ì„ì´ í•„ìš”í•©ë‹ˆë‹¤."
            )

        if stats["diff_patterns"].get("pred1_much_longer", 0) > 10:
            insights.append(
                f"ğŸ“ Pred1ì´ Pred2ë³´ë‹¤ í›¨ì”¬ ê¸´ ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤. ë‹µë³€ ì¶”ì¶œ ë²”ìœ„ë¥¼ ì¡°ì •í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            )

        if stats["diff_patterns"].get("same_prefix", 0) > 10:
            insights.append(
                f"ğŸ”¤ ë™ì¼í•œ ì ‘ë‘ì‚¬ë¥¼ ê°€ì§„ ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤. ë‹µë³€ì˜ ì‹œì‘ì ì€ ìœ ì‚¬í•˜ë‚˜ ëì ì—ì„œ ì°¨ì´ê°€ ë°œìƒí•©ë‹ˆë‹¤."
            )

        if avg_f1 < 0.5:
            insights.append(
                f"ğŸ“‰ í‰ê·  F1 ì ìˆ˜ê°€ {avg_f1:.4f}ë¡œ ë‚®ìŠµë‹ˆë‹¤. ë‘ ëª¨ë¸ì˜ ì˜ˆì¸¡ì´ í¬ê²Œ ë‹¤ë¦…ë‹ˆë‹¤."
            )
        elif avg_f1 > 0.8:
            insights.append(
                f"ğŸ“ˆ í‰ê·  F1 ì ìˆ˜ê°€ {avg_f1:.4f}ë¡œ ë†’ìŠµë‹ˆë‹¤. ë‘ ëª¨ë¸ì´ ìœ ì‚¬í•œ íŒ¨í„´ì„ í•™ìŠµí–ˆìŠµë‹ˆë‹¤."
            )

        for insight in insights:
            f.write(f"- {insight}\n")

        f.write(f"\n---\n\n")
        f.write(f"## ğŸ¯ ë‹¤ìŒ ë‹¨ê³„\n\n")
        f.write(
            f"1. **ë¶ˆì¼ì¹˜ ì˜ˆì‹œ ê²€í† **: ìœ„ì˜ Top 20 ë¶ˆì¼ì¹˜ ì˜ˆì‹œë¥¼ ìƒì„¸íˆ ë¶„ì„í•˜ì—¬ íŒ¨í„´ì„ íŒŒì•…í•©ë‹ˆë‹¤.\n"
        )
        f.write(
            f"2. **ì•™ìƒë¸” ê³ ë ¤**: ë‘ ëª¨ë¸ì˜ ì˜ˆì¸¡ì„ ê²°í•©í•˜ì—¬ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
        )
        f.write(
            f"3. **í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì •**: ì°¨ì´ê°€ í° ì˜ì—­ì—ì„œ ëª¨ë¸ ì„¤ì •ì„ ì¬ê²€í† í•©ë‹ˆë‹¤.\n"
        )
        f.write(
            f"4. **ë°ì´í„° ë¶„ì„**: íŠ¹ì • ì§ˆë¬¸ ìœ í˜•ì´ë‚˜ ë„ë©”ì¸ì—ì„œ ì°¨ì´ê°€ í°ì§€ í™•ì¸í•©ë‹ˆë‹¤.\n"
        )


def main():
    parser = argparse.ArgumentParser(
        description="ë‘ ê°œì˜ prediction CSV íŒŒì¼ì„ ë¹„êµ ë¶„ì„í•©ë‹ˆë‹¤."
    )
    parser.add_argument("csv1", type=str, help="ì²« ë²ˆì§¸ CSV íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("csv2", type=str, help="ë‘ ë²ˆì§¸ CSV íŒŒì¼ ê²½ë¡œ")
    parser.add_argument(
        "--output-dir",
        type=str,
        default="./analysis_results",
        help="ê²°ê³¼ë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: ./analysis_results)",
    )

    args = parser.parse_args()

    csv1_path = Path(args.csv1)
    csv2_path = Path(args.csv2)
    output_dir = Path(args.output_dir)

    # ì…ë ¥ íŒŒì¼ ê²€ì¦
    if not csv1_path.exists():
        print(f"âŒ ì˜¤ë¥˜: {csv1_path} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    if not csv2_path.exists():
        print(f"âŒ ì˜¤ë¥˜: {csv2_path} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_dir.mkdir(parents=True, exist_ok=True)

    print(f"ğŸ” ë¶„ì„ ì‹œì‘...")
    print(f"  - CSV 1: {csv1_path}")
    print(f"  - CSV 2: {csv2_path}")

    # ë¶„ì„ ìˆ˜í–‰
    results, stats = analyze_predictions(csv1_path, csv2_path)

    # ê²°ê³¼ íŒŒì¼ëª… ìƒì„±
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # ë°ì´í„°ì…‹ íƒ€ì… ìë™ ì¶”ë¡ 
    num_samples = stats["total"]
    if num_samples == 240:
        dataset_type = "val"
    elif num_samples == 600:
        dataset_type = "test"
    else:
        dataset_type = "unknown"

    comparison_csv = output_dir / f"comparison_{timestamp}.csv"
    report_md = output_dir / f"analysis_report_{timestamp}.md"

    # ê²°ê³¼ ì €ì¥
    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥ ì¤‘...")
    save_comparison_csv(results, comparison_csv)
    print(f"  âœ… ë¹„êµ CSV ì €ì¥: {comparison_csv}")

    generate_analysis_report(results, stats, csv1_path.name, csv2_path.name, report_md)
    print(f"  âœ… ë¶„ì„ ë³´ê³ ì„œ ì €ì¥: {report_md}")

    # ìš”ì•½ ì¶œë ¥
    print(f"\n" + "=" * 60)
    print(f"ğŸ“Š ë¶„ì„ ì™„ë£Œ!")
    print(f"=" * 60)
    print(f"ì „ì²´ ì˜ˆì‹œ ìˆ˜: {stats['total']}")
    print(
        f"ì¼ì¹˜ (Agreement): {stats['agreement']} ({stats['agreement'] / stats['total'] * 100:.2f}%)"
    )
    print(
        f"ë¶ˆì¼ì¹˜ (Disagreement): {stats['disagreement']} ({stats['disagreement'] / stats['total'] * 100:.2f}%)"
    )
    print(f"í‰ê·  F1 Score: {stats['pred1_f1_sum'] / stats['total']:.4f}")
    print(f"=" * 60)


if __name__ == "__main__":
    main()
