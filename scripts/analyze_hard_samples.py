"""
Hard Sample ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸

Validation ê²°ê³¼ë¥¼ ìƒì„¸íˆ ë¶„ì„í•˜ì—¬ ë‹¤ì–‘í•œ í˜•íƒœì˜ ì•„ì›ƒí’‹ì„ ìƒì„±í•©ë‹ˆë‹¤.

Usage:
    python scripts/analyze_hard_samples.py <output_dir>
    python scripts/analyze_hard_samples.py ./outputs/dahyeong/HANTAEK_roberta_large_vanilla

Outputs:
    1. val_simple_comparison.csv       - ground_truth vs prediction ë‹¨ìˆœ ë¹„êµ
    2. val_detailed_analysis.csv       - ë¬¸ì„œì •ë³´, EM/F1, retrieval ì„±ê³µì—¬ë¶€ í¬í•¨
    3. val_hard_samples.csv            - í‹€ë¦° ìƒ˜í”Œë§Œ (hard samples)
    4. val_retrieval_failures.csv      - retrievalì´ gold context ëª» ì°¾ì€ ì¼€ì´ìŠ¤
    5. val_error_analysis.json         - ì—ëŸ¬ ìœ í˜•ë³„ ë¶„ë¥˜ ë° í†µê³„
    6. val_analysis_summary.md         - ì „ì²´ ë¶„ì„ ìš”ì•½ (ë§ˆí¬ë‹¤ìš´)
"""

import json
import csv
import sys
import re
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
from collections import defaultdict
from datasets import load_from_disk

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ import
sys.path.insert(0, str(Path(__file__).parent.parent))
from src.retrieval.paths import get_path, get_analysis_dir, DATA_ROOT


def normalize_answer(s: str) -> str:
    """ì •ë‹µ ì •ê·œí™” (EM ê³„ì‚°ìš©)"""

    def remove_articles(text):
        return re.sub(r"\b(a|an|the)\b", " ", text)

    def white_space_fix(text):
        return " ".join(text.split())

    def remove_punc(text):
        # í•œê¸€ì€ ìœ ì§€, ì˜ì–´ punctuationë§Œ ì œê±°
        return re.sub(r"[^\w\sê°€-í£]", "", text)

    def lower(text):
        return text.lower()

    return white_space_fix(remove_punc(lower(s)))


def compute_em(prediction: str, ground_truths: List[str]) -> float:
    """Exact Match ê³„ì‚°"""
    norm_pred = normalize_answer(prediction)
    for gt in ground_truths:
        if normalize_answer(gt) == norm_pred:
            return 100.0
    return 0.0


def compute_f1(prediction: str, ground_truths: List[str]) -> float:
    """F1 Score ê³„ì‚°"""

    def get_tokens(s):
        return normalize_answer(s).split()

    def compute_single_f1(pred_tokens, gt_tokens):
        common = set(pred_tokens) & set(gt_tokens)
        if len(common) == 0:
            return 0.0
        precision = len(common) / len(pred_tokens) if pred_tokens else 0
        recall = len(common) / len(gt_tokens) if gt_tokens else 0
        if precision + recall == 0:
            return 0.0
        return 2 * precision * recall / (precision + recall) * 100

    pred_tokens = get_tokens(prediction)
    best_f1 = 0.0
    for gt in ground_truths:
        gt_tokens = get_tokens(gt)
        f1 = compute_single_f1(pred_tokens, gt_tokens)
        best_f1 = max(best_f1, f1)
    return best_f1


def load_wikipedia_documents() -> Dict[int, Dict]:
    """Wikipedia ë¬¸ì„œ ë¡œë“œ (paths ëª¨ë“ˆ ì‚¬ìš©)"""
    wiki_path = Path(get_path("wiki_corpus"))
    if not wiki_path.exists():
        print(f"âš ï¸ Wikipedia documents not found at {wiki_path}")
        return {}

    with open(wiki_path, "r", encoding="utf-8") as f:
        wiki_data = json.load(f)

    # document_idë¥¼ keyë¡œ í•˜ëŠ” dict ìƒì„±
    docs = {}
    for doc_id, doc in wiki_data.items():
        docs[int(doc_id)] = doc
    return docs


def load_retrieval_cache(cache_path: str = None) -> Dict[str, Dict]:
    """Retrieval ìºì‹œ ë¡œë“œ (paths ëª¨ë“ˆ ì‚¬ìš©)"""
    if cache_path is None:
        cache_path = get_path("val_cache")

    cache = {}
    if not Path(cache_path).exists():
        print(f"âš ï¸ Retrieval cache not found at {cache_path}")
        return cache

    with open(cache_path, "r", encoding="utf-8") as f:
        for line in f:
            item = json.loads(line)
            cache[item["id"]] = item
    return cache


def check_retrieval_found_gold(
    retrieved_docs: List[Dict], gold_doc_id: int, top_k: int = 10
) -> Tuple[bool, int]:
    """
    Retrievalì´ gold documentë¥¼ ì°¾ì•˜ëŠ”ì§€ í™•ì¸
    Returns: (found, rank) - rankëŠ” ëª»ì°¾ìœ¼ë©´ -1
    """
    for rank, doc in enumerate(retrieved_docs[:top_k], 1):
        if doc.get("doc_id") == gold_doc_id:
            return True, rank
    return False, -1


def categorize_error(
    em: float,
    f1: float,
    retrieval_found: bool,
    prediction: str,
    ground_truths: List[str],
) -> str:
    """ì—ëŸ¬ ìœ í˜• ë¶„ë¥˜"""
    if em == 100.0:
        return "correct"

    # Retrieval ì‹¤íŒ¨
    if not retrieval_found:
        return "retrieval_failure"

    # Partial match (F1ì€ ë†’ì€ë° EMì€ 0)
    if f1 >= 50:
        # ì˜ˆì¸¡ì´ ì •ë‹µì˜ ì¼ë¶€ì¸ ê²½ìš°
        norm_pred = normalize_answer(prediction)
        for gt in ground_truths:
            norm_gt = normalize_answer(gt)
            if norm_pred in norm_gt:
                return "partial_subset"  # ì˜ˆì¸¡ì´ ì •ë‹µì˜ ë¶€ë¶„ì§‘í•©
            if norm_gt in norm_pred:
                return "partial_superset"  # ì˜ˆì¸¡ì´ ì •ë‹µì„ í¬í•¨
        return "partial_overlap"

    # ì™„ì „íˆ ë‹¤ë¥¸ ë‹µ
    if f1 < 20:
        return "completely_wrong"

    return "low_overlap"


def analyze_samples(
    output_dir: Path,
    top_k: int = 10,
) -> Dict[str, Any]:
    """
    ë©”ì¸ ë¶„ì„ í•¨ìˆ˜

    Args:
        output_dir: ëª¨ë¸ ì¶œë ¥ ë””ë ‰í† ë¦¬
        top_k: Retrieval top-k ê¸°ì¤€
    """
    print("=" * 80)
    print("ğŸ” Hard Sample Analysis Tool")
    print("=" * 80)

    # 1. ë°ì´í„° ë¡œë“œ
    print("\n[1/6] Loading data...")

    # Validation dataset (paths ëª¨ë“ˆ ì‚¬ìš©)
    ds = load_from_disk(get_path("train_dataset"))
    val_ds = ds["validation"]
    print(f"   âœ“ Loaded {len(val_ds)} validation samples")

    # Predictions
    pred_path = output_dir / "predictions.json"
    if not pred_path.exists():
        pred_path = output_dir / "val_predictions.json"

    with open(pred_path, "r", encoding="utf-8") as f:
        predictions = json.load(f)
    print(f"   âœ“ Loaded {len(predictions)} predictions")

    # Wikipedia documents (paths ëª¨ë“ˆ ì‚¬ìš©)
    wiki_docs = load_wikipedia_documents()
    print(f"   âœ“ Loaded {len(wiki_docs)} wikipedia documents")

    # Retrieval cache (paths ëª¨ë“ˆ ì‚¬ìš©)
    retrieval_cache = load_retrieval_cache()
    print(f"   âœ“ Loaded {len(retrieval_cache)} retrieval cache entries")

    # Labels (ì´ë¯¸ ìƒì„±ëœ íŒŒì¼ ì‚¬ìš©)
    labels_path = output_dir / "eval_labels.json"
    if labels_path.exists():
        with open(labels_path, "r", encoding="utf-8") as f:
            labels = json.load(f)
        print(f"   âœ“ Loaded {len(labels)} labels from eval_labels.json")
    else:
        # validation datasetì—ì„œ ì§ì ‘ ìƒì„±
        labels = {}
        for ex in val_ds:
            labels[ex["id"]] = {
                "text": ex["answers"]["text"],
                "answer_start": ex["answers"]["answer_start"],
            }
        print(f"   âœ“ Generated {len(labels)} labels from dataset")

    # 2. ìƒ˜í”Œë³„ ë¶„ì„
    print("\n[2/6] Analyzing each sample...")

    analysis_results = []
    error_categories = defaultdict(list)
    retrieval_stats = {"found": 0, "not_found": 0, "no_cache": 0}

    for ex in val_ds:
        qid = ex["id"]
        question = ex["question"]
        gold_context = ex.get("context", "")
        gold_doc_id = ex.get("document_id", None)
        ground_truths = ex["answers"]["text"]

        # ì˜ˆì¸¡
        prediction = predictions.get(qid, "")

        # EM/F1 ê³„ì‚°
        em = compute_em(prediction, ground_truths)
        f1 = compute_f1(prediction, ground_truths)

        # Retrieval ë¶„ì„
        retrieval_found = False
        retrieval_rank = -1
        retrieved_doc_ids = []
        retrieved_titles = []

        if qid in retrieval_cache:
            retrieved = retrieval_cache[qid].get("retrieved", [])
            retrieved_doc_ids = [d.get("doc_id") for d in retrieved[:top_k]]

            # ì œëª© ê°€ì ¸ì˜¤ê¸°
            for doc_id in retrieved_doc_ids[:3]:  # ìƒìœ„ 3ê°œë§Œ
                if doc_id in wiki_docs:
                    retrieved_titles.append(wiki_docs[doc_id].get("title", "Unknown"))

            if gold_doc_id is not None:
                retrieval_found, retrieval_rank = check_retrieval_found_gold(
                    retrieved, gold_doc_id, top_k
                )
                if retrieval_found:
                    retrieval_stats["found"] += 1
                else:
                    retrieval_stats["not_found"] += 1
            else:
                retrieval_stats["no_cache"] += 1
        else:
            retrieval_stats["no_cache"] += 1

        # ì—ëŸ¬ ìœ í˜• ë¶„ë¥˜
        error_type = categorize_error(
            em, f1, retrieval_found, prediction, ground_truths
        )
        error_categories[error_type].append(qid)

        # Gold document ì •ë³´
        gold_title = ""
        gold_text_snippet = ""
        if gold_doc_id and gold_doc_id in wiki_docs:
            gold_title = wiki_docs[gold_doc_id].get("title", "")
            gold_text_snippet = wiki_docs[gold_doc_id].get("text", "")[:200] + "..."
        elif gold_context:
            gold_text_snippet = gold_context[:200] + "..."

        result = {
            "id": qid,
            "question": question,
            "gold_doc_id": gold_doc_id,
            "gold_title": gold_title,
            "gold_context_snippet": gold_text_snippet,
            "ground_truth": " | ".join(ground_truths),
            "prediction": prediction,
            "em": em,
            "f1": f1,
            "retrieval_found": retrieval_found,
            "retrieval_rank": retrieval_rank,
            "retrieved_top3_titles": " | ".join(retrieved_titles),
            "error_type": error_type,
        }
        analysis_results.append(result)

    print(f"   âœ“ Analyzed {len(analysis_results)} samples")

    # 3. Output 1: Simple comparison
    print("\n[3/6] Generating outputs...")

    # val_analysis í•˜ìœ„ ë””ë ‰í† ë¦¬ ìƒì„± (paths ëª¨ë“ˆ ì‚¬ìš©)
    analysis_dir = get_analysis_dir(output_dir, "val_analysis")

    simple_path = analysis_dir / "val_simple_comparison.csv"
    with open(simple_path, "w", encoding="utf-8-sig", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(["id", "ground_truth", "prediction", "match"])
        for r in analysis_results:
            match = "âœ“" if r["em"] == 100.0 else "âœ—"
            writer.writerow([r["id"], r["ground_truth"], r["prediction"], match])
    print(f"   ğŸ“„ {simple_path}")

    # 4. Output 2: Detailed analysis
    detailed_path = analysis_dir / "val_detailed_analysis.csv"
    with open(detailed_path, "w", encoding="utf-8-sig", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(
            [
                "id",
                "question",
                "gold_doc_id",
                "gold_title",
                "gold_context_snippet",
                "ground_truth",
                "prediction",
                "em",
                "f1",
                "retrieval_found",
                "retrieval_rank",
                "retrieved_top3_titles",
                "error_type",
            ]
        )
        for r in analysis_results:
            writer.writerow(
                [
                    r["id"],
                    r["question"],
                    r["gold_doc_id"],
                    r["gold_title"],
                    r["gold_context_snippet"],
                    r["ground_truth"],
                    r["prediction"],
                    f"{r['em']:.1f}",
                    f"{r['f1']:.1f}",
                    "Yes" if r["retrieval_found"] else "No",
                    r["retrieval_rank"] if r["retrieval_rank"] > 0 else "N/A",
                    r["retrieved_top3_titles"],
                    r["error_type"],
                ]
            )
    print(f"   ğŸ“„ {detailed_path}")

    # 5. Output 3: Hard samples only (wrong predictions)
    hard_samples = [r for r in analysis_results if r["em"] < 100.0]
    hard_samples.sort(key=lambda x: x["f1"])  # F1 ë‚®ì€ ìˆœìœ¼ë¡œ ì •ë ¬

    hard_path = analysis_dir / "val_hard_samples.csv"
    with open(hard_path, "w", encoding="utf-8-sig", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(
            [
                "id",
                "question",
                "ground_truth",
                "prediction",
                "em",
                "f1",
                "retrieval_found",
                "error_type",
                "gold_title",
            ]
        )
        for r in hard_samples:
            writer.writerow(
                [
                    r["id"],
                    r["question"],
                    r["ground_truth"],
                    r["prediction"],
                    f"{r['em']:.1f}",
                    f"{r['f1']:.1f}",
                    "Yes" if r["retrieval_found"] else "No",
                    r["error_type"],
                    r["gold_title"],
                ]
            )
    print(f"   ğŸ“„ {hard_path} ({len(hard_samples)} samples)")

    # 6. Output 4: Retrieval failures
    retrieval_failures = [
        r
        for r in analysis_results
        if not r["retrieval_found"] and r["retrieval_rank"] == -1
    ]

    retrieval_fail_path = analysis_dir / "val_retrieval_failures.csv"
    with open(retrieval_fail_path, "w", encoding="utf-8-sig", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(
            [
                "id",
                "question",
                "gold_title",
                "retrieved_top3_titles",
                "ground_truth",
                "prediction",
                "em",
                "f1",
            ]
        )
        for r in retrieval_failures:
            writer.writerow(
                [
                    r["id"],
                    r["question"],
                    r["gold_title"],
                    r["retrieved_top3_titles"],
                    r["ground_truth"],
                    r["prediction"],
                    f"{r['em']:.1f}",
                    f"{r['f1']:.1f}",
                ]
            )
    print(f"   ğŸ“„ {retrieval_fail_path} ({len(retrieval_failures)} samples)")

    # 7. Output 5: Error analysis JSON
    error_analysis = {
        "total_samples": len(analysis_results),
        "correct_count": len(error_categories["correct"]),
        "wrong_count": len(analysis_results) - len(error_categories["correct"]),
        "retrieval_stats": retrieval_stats,
        "error_categories": {
            cat: {
                "count": len(ids),
                "percentage": len(ids) / len(analysis_results) * 100,
                "sample_ids": ids[:10],  # ì²˜ìŒ 10ê°œë§Œ
            }
            for cat, ids in error_categories.items()
        },
        "metrics": {
            "overall_em": sum(r["em"] for r in analysis_results)
            / len(analysis_results),
            "overall_f1": sum(r["f1"] for r in analysis_results)
            / len(analysis_results),
            "em_when_retrieval_found": (
                sum(r["em"] for r in analysis_results if r["retrieval_found"])
                / max(1, sum(1 for r in analysis_results if r["retrieval_found"]))
            ),
            "em_when_retrieval_not_found": (
                sum(r["em"] for r in analysis_results if not r["retrieval_found"])
                / max(1, sum(1 for r in analysis_results if not r["retrieval_found"]))
            ),
        },
    }

    error_json_path = analysis_dir / "val_error_analysis.json"
    with open(error_json_path, "w", encoding="utf-8") as f:
        json.dump(error_analysis, f, indent=2, ensure_ascii=False)
    print(f"   ğŸ“„ {error_json_path}")

    # 8. Output 6: Summary markdown
    summary_md = generate_summary_markdown(
        error_analysis, analysis_results, analysis_dir
    )
    summary_path = analysis_dir / "val_analysis_summary.md"
    with open(summary_path, "w", encoding="utf-8") as f:
        f.write(summary_md)
    print(f"   ğŸ“„ {summary_path}")

    return error_analysis


def generate_summary_markdown(
    error_analysis: Dict, results: List[Dict], output_dir: Path
) -> str:
    """ë¶„ì„ ìš”ì•½ ë§ˆí¬ë‹¤ìš´ ìƒì„±"""

    total = error_analysis["total_samples"]
    correct = error_analysis["correct_count"]
    wrong = error_analysis["wrong_count"]
    metrics = error_analysis["metrics"]
    retrieval = error_analysis["retrieval_stats"]
    categories = error_analysis["error_categories"]

    # ì—ëŸ¬ ìœ í˜•ë³„ ì •ë ¬
    sorted_cats = sorted(
        [(k, v) for k, v in categories.items() if k != "correct"],
        key=lambda x: x[1]["count"],
        reverse=True,
    )

    md = f"""# ğŸ“Š Validation Analysis Summary

> Output directory: `{output_dir}`
> Generated by `analyze_hard_samples.py`

---

## 1. Overall Performance

| Metric | Value |
|--------|-------|
| **Total Samples** | {total} |
| **Correct (EM=100)** | {correct} ({correct / total * 100:.1f}%) |
| **Wrong** | {wrong} ({wrong / total * 100:.1f}%) |
| **Overall EM** | {metrics["overall_em"]:.2f} |
| **Overall F1** | {metrics["overall_f1"]:.2f} |

---

## 2. Retrieval Impact

| Status | Count | Percentage |
|--------|-------|------------|
| âœ… Gold in Top-{10} | {retrieval["found"]} | {retrieval["found"] / total * 100:.1f}% |
| âŒ Gold NOT in Top-{10} | {retrieval["not_found"]} | {retrieval["not_found"] / total * 100:.1f}% |
| âš ï¸ No Cache/Doc ID | {retrieval["no_cache"]} | {retrieval["no_cache"] / total * 100:.1f}% |

### Performance by Retrieval Success

| Condition | EM |
|-----------|-----|
| Retrieval Found Gold | {metrics["em_when_retrieval_found"]:.2f} |
| Retrieval Missed Gold | {metrics["em_when_retrieval_not_found"]:.2f} |

> ğŸ’¡ **Insight**: Retrievalì´ gold documentë¥¼ ì°¾ì•˜ì„ ë•Œ EMì´ {metrics["em_when_retrieval_found"] - metrics["em_when_retrieval_not_found"]:.1f}ì  ë” ë†’ìŠµë‹ˆë‹¤.

---

## 3. Error Type Analysis

| Error Type | Count | % | Description |
|------------|-------|---|-------------|
"""

    error_descriptions = {
        "correct": "ì •ë‹µê³¼ ì¼ì¹˜",
        "retrieval_failure": "Retrievalì´ gold contextë¥¼ ëª» ì°¾ìŒ",
        "partial_subset": "ì˜ˆì¸¡ì´ ì •ë‹µì˜ ë¶€ë¶„ì§‘í•© (ë” ì§§ê²Œ ì˜ˆì¸¡)",
        "partial_superset": "ì˜ˆì¸¡ì´ ì •ë‹µì„ í¬í•¨ (ë” ê¸¸ê²Œ ì˜ˆì¸¡)",
        "partial_overlap": "ë¶€ë¶„ì ìœ¼ë¡œ ê²¹ì¹¨ (F1 >= 50)",
        "low_overlap": "ë‚®ì€ ê²¹ì¹¨ (F1 20-50)",
        "completely_wrong": "ì™„ì „íˆ ë‹¤ë¥¸ ë‹µ (F1 < 20)",
    }

    for cat, info in sorted_cats:
        desc = error_descriptions.get(cat, cat)
        md += f"| {cat} | {info['count']} | {info['percentage']:.1f}% | {desc} |\n"

    md += f"""
---

## 4. Sample Hard Cases

### 4.1 Retrieval Failures (Top 5)

"""

    # Retrieval ì‹¤íŒ¨ ì¼€ì´ìŠ¤
    ret_failures = [r for r in results if r["error_type"] == "retrieval_failure"][:5]
    for i, r in enumerate(ret_failures, 1):
        md += f"""**{i}. [{r["id"]}]**
- **Question**: {r["question"][:100]}...
- **Gold Title**: {r["gold_title"]}
- **Retrieved**: {r["retrieved_top3_titles"]}
- **Answer**: {r["ground_truth"]} â†’ **Pred**: {r["prediction"]}
- **F1**: {r["f1"]:.1f}

"""

    md += """### 4.2 Partial Match Cases (Top 5)

"""

    # Partial match ì¼€ì´ìŠ¤
    partial_cases = [r for r in results if "partial" in r["error_type"]][:5]
    for i, r in enumerate(partial_cases, 1):
        md += f"""**{i}. [{r["id"]}]** ({r["error_type"]})
- **Question**: {r["question"][:100]}...
- **Answer**: `{r["ground_truth"]}` â†’ **Pred**: `{r["prediction"]}`
- **EM**: {r["em"]:.0f}, **F1**: {r["f1"]:.1f}

"""

    md += """### 4.3 Completely Wrong Cases (Top 5)

"""

    # ì™„ì „íˆ í‹€ë¦° ì¼€ì´ìŠ¤
    wrong_cases = [r for r in results if r["error_type"] == "completely_wrong"][:5]
    for i, r in enumerate(wrong_cases, 1):
        md += f"""**{i}. [{r["id"]}]**
- **Question**: {r["question"][:100]}...
- **Answer**: `{r["ground_truth"]}` â†’ **Pred**: `{r["prediction"]}`
- **Retrieval Found**: {r["retrieval_found"]}

"""

    md += f"""---

## 5. Generated Files

| File | Description |
|------|-------------|
| `val_simple_comparison.csv` | ground_truth vs prediction ë‹¨ìˆœ ë¹„êµ |
| `val_detailed_analysis.csv` | ë¬¸ì„œì •ë³´, EM/F1, retrieval ì„±ê³µì—¬ë¶€ ì „ì²´ |
| `val_hard_samples.csv` | í‹€ë¦° ìƒ˜í”Œë§Œ (F1 ë‚®ì€ ìˆœ ì •ë ¬) |
| `val_retrieval_failures.csv` | Retrievalì´ goldë¥¼ ëª» ì°¾ì€ ì¼€ì´ìŠ¤ |
| `val_error_analysis.json` | ì—ëŸ¬ ìœ í˜•ë³„ í†µê³„ (í”„ë¡œê·¸ë˜ë°ìš©) |
| `val_analysis_summary.md` | ì´ íŒŒì¼ |

---

## 6. Recommendations

"""

    # ì¶”ì²œì‚¬í•­ ìƒì„±
    if retrieval["not_found"] > total * 0.05:  # 5% ì´ìƒ retrieval ì‹¤íŒ¨
        md += f"""### ğŸ”§ Retrieval ê°œì„  í•„ìš”
- Retrieval ì‹¤íŒ¨ìœ¨ì´ {retrieval["not_found"] / total * 100:.1f}%ë¡œ ë†’ìŠµë‹ˆë‹¤.
- top_k ì¦ê°€, reranking ì¶”ê°€, ë˜ëŠ” hybrid ê°€ì¤‘ì¹˜ ì¡°ì •ì„ ê³ ë ¤í•˜ì„¸ìš”.

"""

    partial_count = sum(1 for r in results if "partial" in r["error_type"])
    if partial_count > total * 0.1:  # 10% ì´ìƒ partial
        md += f"""### ğŸ”§ Answer Span ê²½ê³„ ê°œì„  í•„ìš”
- Partial matchê°€ {partial_count}ê°œ ({partial_count / total * 100:.1f}%)ì…ë‹ˆë‹¤.
- doc_stride ì¡°ì • ë˜ëŠ” start/end ëª¨ë¸ ì„±ëŠ¥ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.

"""

    wrong_count = len([r for r in results if r["error_type"] == "completely_wrong"])
    if wrong_count > total * 0.1:
        md += f"""### ğŸ”§ Reader ëª¨ë¸ ê°œì„  í•„ìš”
- ì™„ì „íˆ í‹€ë¦° ì˜ˆì¸¡ì´ {wrong_count}ê°œ ({wrong_count / total * 100:.1f}%)ì…ë‹ˆë‹¤.
- ëª¨ë¸ fine-tuning, ë°ì´í„° augmentation, ë˜ëŠ” ë” í° ëª¨ë¸ ì‚¬ìš©ì„ ê³ ë ¤í•˜ì„¸ìš”.

"""

    return md


def main():
    if len(sys.argv) < 2:
        print("Usage: python scripts/analyze_hard_samples.py <output_dir> [top_k]")
        print(
            "Example: python scripts/analyze_hard_samples.py ./outputs/dahyeong/HANTAEK_roberta_large_vanilla"
        )
        print(
            "         python scripts/analyze_hard_samples.py ./outputs/dahyeong/model_name 20"
        )
        sys.exit(1)

    output_dir = Path(sys.argv[1])

    if not output_dir.exists():
        print(f"âŒ Output directory not found: {output_dir}")
        sys.exit(1)

    # ì˜µì…˜ íŒŒë¼ë¯¸í„°
    top_k = 10
    if len(sys.argv) > 2:
        top_k = int(sys.argv[2])

    try:
        result = analyze_samples(output_dir, top_k)

        print("\n" + "=" * 80)
        print("âœ… Analysis Complete!")
        print("=" * 80)
        print(f"\nğŸ“Š Quick Summary:")
        print(f"   - Total: {result['total_samples']} samples")
        print(
            f"   - Correct: {result['correct_count']} ({result['correct_count'] / result['total_samples'] * 100:.1f}%)"
        )
        print(f"   - EM: {result['metrics']['overall_em']:.2f}")
        print(f"   - F1: {result['metrics']['overall_f1']:.2f}")
        print(f"\nğŸ“‚ Check {output_dir}/val_analysis/ for detailed analysis files!")

    except Exception as e:
        print(f"âŒ Error during analysis: {e}")
        import traceback

        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
