"""
ì´ì¢… ëª¨ë¸ ì•™ìƒë¸” (Heterogeneous Model Ensemble)

ì„œë¡œ ë‹¤ë¥¸ tokenizer/architectureë¥¼ ê°€ì§„ ëª¨ë¸ë“¤ì˜ ì˜ˆì¸¡ì„ Text-level Votingìœ¼ë¡œ ì•™ìƒë¸”

ì§€ì› ëª¨ë¸ ì¡°í•© ì˜ˆì‹œ:
- RoBERTa-Large + KoELECTRA + BERT-Base + KoBigBird
- ì„œë¡œ ë‹¤ë¥¸ vocab_size, hidden_sizeë¥¼ ê°€ì§„ ëª¨ë¸ë“¤ë„ ì¡°í•© ê°€ëŠ¥

ì•™ìƒë¸” ë°©ì‹:
1. ê° ëª¨ë¸ì´ ë…ë¦½ì ìœ¼ë¡œ inference ìˆ˜í–‰ â†’ nbest_predictions.json ìƒì„±
2. ì •ê·œí™”ëœ answer text ê¸°ì¤€ìœ¼ë¡œ weighted voting
3. ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ answer ì„ íƒ (ì›ë³¸ í…ìŠ¤íŠ¸ ì¤‘ ìµœê³  í™•ë¥  ê²ƒ ë°˜í™˜)

ì‚¬ìš© ì˜ˆì‹œ:
  # ê¸°ë³¸ ì‚¬ìš© (nbest íŒŒì¼ë“¤ ì§ì ‘ ì§€ì •)
  python scripts/hetero_ensemble.py \\
    --nbest_paths outputs/roberta/nbest_predictions.json \\
                  outputs/koelectra/nbest_predictions.json \\
                  outputs/bert/nbest_predictions.json \\
    --weights 0.5 0.3 0.2 \\
    --output_path outputs/hetero_ensemble/predictions.json

  # output_dirë“¤ë¡œ ì§€ì • (ìë™ìœ¼ë¡œ nbest_predictions.json íƒìƒ‰)
  python scripts/hetero_ensemble.py \\
    --output_dirs outputs/roberta outputs/koelectra outputs/bert \\
    --weights 0.5 0.3 0.2 \\
    --output_path outputs/hetero_ensemble/predictions.json

  # Validation ëª¨ë“œ (ì •ë‹µê³¼ ë¹„êµí•˜ì—¬ EM/F1 ê³„ì‚°)
  python scripts/hetero_ensemble.py \\
    --output_dirs outputs/roberta outputs/koelectra \\
    --weights 0.6 0.4 \\
    --output_path outputs/hetero_ensemble/predictions.json \\
    --eval_file ./data/train_dataset/validation

ì°¨ì´ì  (ê¸°ì¡´ ensemble.py vs ì´ ìŠ¤í¬ë¦½íŠ¸):
- ensemble.py: Logit-level ì•™ìƒë¸” (ë™ì¼ tokenizer í•„ìˆ˜)
- hetero_ensemble.py: Text-level ì•™ìƒë¸” (ì´ì¢… ëª¨ë¸ ì¡°í•© ê°€ëŠ¥)
"""

import os
import sys
import json
import argparse
import logging
import re
import glob
from typing import Dict, List, Optional, Tuple
from collections import defaultdict
from dataclasses import dataclass

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

logger = logging.getLogger(__name__)
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)-8s | %(message)s",
    datefmt="%H:%M:%S",
)


# ============================================================
# ì •ê·œí™” í•¨ìˆ˜
# ============================================================


def normalize_answer(text: str) -> str:
    """
    Answer ì •ê·œí™” (EM í‰ê°€ì™€ ë™ì¼í•œ ë°©ì‹)

    - ì†Œë¬¸ìí™”
    - êµ¬ë‘ì  ì œê±°
    - ì—°ì† ê³µë°± â†’ ë‹¨ì¼ ê³µë°±
    - ì•ë’¤ ê³µë°± ì œê±°
    """
    if not text:
        return ""
    text = text.lower()
    text = re.sub(r"[^\w\s]", "", text)
    text = re.sub(r"\s+", " ", text).strip()
    return text


# ============================================================
# ë°ì´í„° ë¡œë”©
# ============================================================


def find_nbest_file(output_dir: str) -> Optional[str]:
    """
    output_dirì—ì„œ nbest_predictions.json íŒŒì¼ íƒìƒ‰

    íƒìƒ‰ ìˆœì„œ:
    1. output_dir/nbest_predictions.json
    2. output_dir/checkpoint-*/nbest_predictions.json (ìµœì‹  ê²ƒ)
    3. output_dir/**/nbest_predictions.json
    """
    # 1. ì§ì ‘ ê²½ë¡œ
    direct_path = os.path.join(output_dir, "nbest_predictions.json")
    if os.path.exists(direct_path):
        return direct_path

    # 2. checkpoint í´ë” ë‚´
    checkpoint_pattern = os.path.join(
        output_dir, "checkpoint-*", "nbest_predictions.json"
    )
    checkpoint_files = glob.glob(checkpoint_pattern)
    if checkpoint_files:
        # ê°€ì¥ ìµœì‹  checkpoint
        def get_step(path):
            try:
                return int(os.path.basename(os.path.dirname(path)).split("-")[1])
            except:
                return 0

        checkpoint_files.sort(key=get_step, reverse=True)
        return checkpoint_files[0]

    # 3. ì¬ê·€ íƒìƒ‰
    recursive_pattern = os.path.join(output_dir, "**", "nbest_predictions.json")
    found = glob.glob(recursive_pattern, recursive=True)
    if found:
        return found[0]

    return None


def load_nbest(path: str) -> Dict[str, List[Dict]]:
    """
    nbest_predictions.json ë¡œë“œ

    Expected format:
    {
        "qid1": [
            {"text": "answer1", "probability": 0.9, "start_logit": ..., "end_logit": ...},
            {"text": "answer2", "probability": 0.05, ...},
            ...
        ],
        ...
    }
    """
    logger.info(f"ğŸ“– Loading: {path}")

    with open(path, "r", encoding="utf-8") as f:
        data = json.load(f)

    # List í˜•íƒœì¸ ê²½ìš° dictë¡œ ë³€í™˜
    if isinstance(data, list):
        result = defaultdict(list)
        for item in data:
            qid = item.get("id")
            if qid:
                result[qid].append(item)
        return dict(result)

    return data


def load_answers(eval_file: str) -> Dict[str, List[str]]:
    """
    ì •ë‹µ íŒŒì¼ ë¡œë“œ (validation í‰ê°€ìš©)

    Returns:
        {qid: [answer1, answer2, ...]}  # ë³µìˆ˜ ì •ë‹µ ì§€ì›
    """
    from datasets import load_from_disk

    dataset = load_from_disk(eval_file)

    answers = {}
    for example in dataset:
        qid = example["id"]
        answer_texts = example.get("answers", {}).get("text", [])
        if answer_texts:
            answers[qid] = answer_texts

    return answers


# ============================================================
# ì•™ìƒë¸” ë¡œì§
# ============================================================


@dataclass
class EnsembleConfig:
    """ì•™ìƒë¸” ì„¤ì •"""

    top_k_candidates: int = 5  # ê° ëª¨ë¸ì—ì„œ ê³ ë ¤í•  í›„ë³´ ìˆ˜
    score_key: str = "probability"  # "probability" or "start_logit" + "end_logit"
    use_rank_score: bool = False  # Trueë©´ rank ê¸°ë°˜ ì ìˆ˜ ì‚¬ìš©


def ensemble_predictions(
    nbest_list: List[Dict[str, List[Dict]]],
    weights: List[float],
    config: EnsembleConfig = None,
) -> Tuple[Dict[str, str], Dict[str, Dict]]:
    """
    Text-level Weighted Voting ì•™ìƒë¸”

    Args:
        nbest_list: ê° ëª¨ë¸ì˜ nbest predictions [{qid: [candidates]}, ...]
        weights: ëª¨ë¸ë³„ ê°€ì¤‘ì¹˜ (ì •ê·œí™”ë¨)
        config: ì•™ìƒë¸” ì„¤ì •

    Returns:
        predictions: {qid: best_answer}
        details: {qid: {answer_scores, selected_answer, ...}}
    """
    if config is None:
        config = EnsembleConfig()

    # ëª¨ë“  question ID ìˆ˜ì§‘
    all_qids = set()
    for nbest_data in nbest_list:
        all_qids.update(nbest_data.keys())

    logger.info(f"ğŸ“Š Total questions: {len(all_qids)}")
    logger.info(f"ğŸ¯ Top-k candidates per model: {config.top_k_candidates}")

    predictions = {}
    details = {}

    for qid in all_qids:
        # ì •ê·œí™”ëœ answer â†’ ëˆ„ì  weighted score
        answer_scores = defaultdict(float)
        # ì •ê·œí™”ëœ answer â†’ (best_prob, original_text)
        answer_originals = {}

        for model_idx, nbest_data in enumerate(nbest_list):
            weight = weights[model_idx]

            if qid not in nbest_data:
                continue

            candidates = nbest_data[qid][: config.top_k_candidates]

            for rank, candidate in enumerate(candidates):
                text = candidate.get("text", "")
                if not text:
                    continue

                # Score ê³„ì‚°
                if config.use_rank_score:
                    # Rank ê¸°ë°˜: 1ìœ„=1.0, 2ìœ„=0.8, 3ìœ„=0.6, ...
                    score = max(0.2, 1.0 - rank * 0.2)
                else:
                    # Probability ê¸°ë°˜
                    score = candidate.get(config.score_key, 0.0)
                    if score <= 0:
                        # logit í•©ì‚° fallback
                        start_logit = candidate.get("start_logit", 0)
                        end_logit = candidate.get("end_logit", 0)
                        score = start_logit + end_logit

                normalized = normalize_answer(text)
                if not normalized:
                    continue

                # Weighted score ëˆ„ì 
                answer_scores[normalized] += weight * score

                # ê°€ì¥ ë†’ì€ probabilityë¥¼ ê°€ì§„ ì›ë³¸ í…ìŠ¤íŠ¸ ë³´ì¡´
                prob = candidate.get("probability", score)
                if (
                    normalized not in answer_originals
                    or prob > answer_originals[normalized][0]
                ):
                    answer_originals[normalized] = (prob, text)

        # ìµœê³  ì ìˆ˜ answer ì„ íƒ
        if answer_scores:
            best_normalized = max(answer_scores, key=answer_scores.get)
            best_answer = answer_originals[best_normalized][1]
            predictions[qid] = best_answer

            # ìƒì„¸ ì •ë³´ ì €ì¥ (ë””ë²„ê¹…ìš©)
            details[qid] = {
                "selected": best_answer,
                "normalized": best_normalized,
                "score": answer_scores[best_normalized],
                "all_scores": dict(
                    sorted(answer_scores.items(), key=lambda x: x[1], reverse=True)[:5]
                ),  # top-5ë§Œ ì €ì¥
            }
        else:
            predictions[qid] = ""
            details[qid] = {"selected": "", "error": "no_candidates"}
            logger.warning(f"âš ï¸ No valid candidates for qid: {qid}")

    return predictions, details


# ============================================================
# í‰ê°€
# ============================================================


def compute_em_f1(
    predictions: Dict[str, str], answers: Dict[str, List[str]]
) -> Dict[str, float]:
    """
    EM, F1 ê³„ì‚°
    """
    em_scores = []
    f1_scores = []

    for qid, pred in predictions.items():
        if qid not in answers:
            continue

        gold_answers = answers[qid]
        pred_normalized = normalize_answer(pred)

        # EM: í•˜ë‚˜ë¼ë„ ì¼ì¹˜í•˜ë©´ 1
        em = max(
            int(normalize_answer(gold) == pred_normalized) for gold in gold_answers
        )
        em_scores.append(em)

        # F1: ìµœëŒ€ F1
        def token_f1(pred_tokens, gold_tokens):
            common = set(pred_tokens) & set(gold_tokens)
            if not common:
                return 0.0
            precision = len(common) / len(pred_tokens) if pred_tokens else 0
            recall = len(common) / len(gold_tokens) if gold_tokens else 0
            if precision + recall == 0:
                return 0.0
            return 2 * precision * recall / (precision + recall)

        pred_tokens = pred_normalized.split()
        f1 = max(
            token_f1(pred_tokens, normalize_answer(gold).split())
            for gold in gold_answers
        )
        f1_scores.append(f1)

    return {
        "em": sum(em_scores) / len(em_scores) * 100 if em_scores else 0,
        "f1": sum(f1_scores) / len(f1_scores) * 100 if f1_scores else 0,
        "total": len(em_scores),
    }


# ============================================================
# ì¶œë ¥
# ============================================================


def save_predictions(
    predictions: Dict[str, str],
    output_path: str,
    details: Optional[Dict] = None,
    ordered_ids: Optional[List[str]] = None,
    ensemble_config: Optional[Dict] = None,
    metrics: Optional[Dict] = None,
):
    """
    ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë‹¨ì¼ ëª¨ë¸ê³¼ ë™ì¼í•œ êµ¬ì¡°ë¡œ ì €ì¥

    ì €ì¥ë˜ëŠ” íŒŒì¼ë“¤:
    - predictions.json: {qid: answer}
    - predictions_submit.csv: TSV í˜•ì‹ (ë¦¬ë”ë³´ë“œ ì œì¶œìš©)
    - nbest_predictions.json: ì•™ìƒë¸” ìƒì„¸ ì •ë³´ (ë‹¤ë¥¸ ì•™ìƒë¸”ì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥)
    - eval_results.json: EM/F1 í‰ê°€ ê²°ê³¼
    - config.json: ì•™ìƒë¸” ì„¤ì • ì •ë³´
    """
    # output_pathì—ì„œ ë””ë ‰í† ë¦¬ ì¶”ì¶œ
    if output_path.endswith(".json") or output_path.endswith(".csv"):
        output_dir = os.path.dirname(output_path) or "."
    else:
        output_dir = output_path

    os.makedirs(output_dir, exist_ok=True)

    # ìˆœì„œê°€ ì§€ì •ëœ ê²½ìš° OrderedDict ì‚¬ìš©
    if ordered_ids:
        from collections import OrderedDict

        ordered_preds = OrderedDict()
        for qid in ordered_ids:
            if qid in predictions:
                ordered_preds[qid] = predictions[qid]
        predictions = ordered_preds

    # 1. predictions.json ì €ì¥
    pred_path = os.path.join(output_dir, "predictions.json")
    with open(pred_path, "w", encoding="utf-8") as f:
        json.dump(predictions, f, ensure_ascii=False, indent=2)
    logger.info(f"ğŸ’¾ predictions.json saved: {pred_path}")

    # 2. predictions_submit.csv ì €ì¥ (ì œì¶œìš©)
    csv_path = os.path.join(output_dir, "predictions_submit.csv")
    with open(csv_path, "w", encoding="utf-8") as f:
        for qid, answer in predictions.items():
            answer = answer.replace("\t", " ").replace("\n", " ")
            f.write(f"{qid}\t{answer}\n")
    logger.info(f"ğŸ’¾ predictions_submit.csv saved: {csv_path}")

    # 3. nbest_predictions.json ì €ì¥ (ë‹¤ë¥¸ ì•™ìƒë¸” ì…ë ¥ìœ¼ë¡œ ì¬ì‚¬ìš© ê°€ëŠ¥)
    if details:
        nbest_path = os.path.join(output_dir, "nbest_predictions.json")
        # detailsë¥¼ nbest í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        nbest_format = {}
        for qid, detail in details.items():
            nbest_format[qid] = [
                {
                    "text": detail.get("selected", ""),
                    "probability": detail.get("score", 0.0),
                    "normalized": detail.get("normalized", ""),
                }
            ]
            # all_scoresì—ì„œ ì¶”ê°€ í›„ë³´ë“¤ë„ í¬í•¨
            if "all_scores" in detail:
                for norm_text, score in list(detail["all_scores"].items())[1:5]:
                    nbest_format[qid].append(
                        {
                            "text": norm_text,  # ì •ê·œí™”ëœ í…ìŠ¤íŠ¸
                            "probability": score,
                            "normalized": norm_text,
                        }
                    )

        with open(nbest_path, "w", encoding="utf-8") as f:
            json.dump(nbest_format, f, ensure_ascii=False, indent=2)
        logger.info(f"ğŸ’¾ nbest_predictions.json saved: {nbest_path}")

        # ìƒì„¸ ì •ë³´ë„ ë³„ë„ ì €ì¥
        details_path = os.path.join(output_dir, "ensemble_details.json")
        with open(details_path, "w", encoding="utf-8") as f:
            json.dump(details, f, ensure_ascii=False, indent=2)
        logger.info(f"ğŸ’¾ ensemble_details.json saved: {details_path}")

    # 4. eval_results.json ì €ì¥ (ë‹¤ë¥¸ ëª¨ë¸ê³¼ ë™ì¼í•œ í˜•ì‹)
    if metrics:
        eval_path = os.path.join(output_dir, "eval_results.json")
        eval_results = {
            "eval_exact_match": metrics.get("em", 0),
            "eval_f1": metrics.get("f1", 0),
            "eval_total": metrics.get("total", 0),
        }
        with open(eval_path, "w", encoding="utf-8") as f:
            json.dump(eval_results, f, ensure_ascii=False, indent=2)
        logger.info(f"ğŸ’¾ eval_results.json saved: {eval_path}")

    # 5. config.json ì €ì¥ (ì•™ìƒë¸” ì„¤ì •)
    if ensemble_config:
        config_path = os.path.join(output_dir, "config.json")
        with open(config_path, "w", encoding="utf-8") as f:
            json.dump(ensemble_config, f, ensure_ascii=False, indent=2)
        logger.info(f"ğŸ’¾ config.json saved: {config_path}")


# ============================================================
# Main
# ============================================================


def main():
    parser = argparse.ArgumentParser(
        description="ì´ì¢… ëª¨ë¸ ì•™ìƒë¸” (Text-level Weighted Voting)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ:
  # nbest íŒŒì¼ ì§ì ‘ ì§€ì •
  python scripts/hetero_ensemble.py \\
    --nbest_paths outputs/roberta/nbest_predictions.json \\
                  outputs/koelectra/nbest_predictions.json \\
    --weights 0.6 0.4 \\
    --output_path outputs/ensemble/hetero_pred.json

  # output_dirë¡œ ì§€ì • (ìë™ íƒìƒ‰)
  python scripts/hetero_ensemble.py \\
    --output_dirs outputs/roberta outputs/koelectra \\
    --weights 0.6 0.4 \\
    --output_path outputs/ensemble/hetero_pred.json

  # Validation í‰ê°€
  python scripts/hetero_ensemble.py \\
    --output_dirs outputs/roberta outputs/koelectra \\
    --weights 0.6 0.4 \\
    --output_path outputs/ensemble/hetero_pred.json \\
    --eval_file ./data/train_dataset/validation
        """,
    )

    # ì…ë ¥ (ë‘˜ ì¤‘ í•˜ë‚˜ í•„ìˆ˜)
    input_group = parser.add_mutually_exclusive_group(required=True)
    input_group.add_argument(
        "--nbest_paths",
        nargs="+",
        help="nbest_predictions.json íŒŒì¼ ê²½ë¡œë“¤",
    )
    input_group.add_argument(
        "--output_dirs",
        nargs="+",
        help="ëª¨ë¸ output directoryë“¤ (ìë™ìœ¼ë¡œ nbest íŒŒì¼ íƒìƒ‰)",
    )

    # ê°€ì¤‘ì¹˜
    parser.add_argument(
        "--weights",
        nargs="+",
        type=float,
        default=None,
        help="ê° ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ (ë¯¸ì§€ì • ì‹œ ê· ë“±)",
    )
    parser.add_argument(
        "--auto_weight_by_em",
        action="store_true",
        help="EM ì ìˆ˜ ê¸°ë°˜ ìë™ ê°€ì¤‘ì¹˜ ì„¤ì • (--eval_file í•„ìš”)",
    )

    # ì¶œë ¥
    parser.add_argument(
        "--output_path",
        type=str,
        required=True,
        help="ì¶œë ¥ íŒŒì¼ ê²½ë¡œ (.json ë˜ëŠ” .csv)",
    )

    # í‰ê°€
    parser.add_argument(
        "--eval_file",
        type=str,
        default=None,
        help="ì •ë‹µ íŒŒì¼ ê²½ë¡œ (ì§€ì • ì‹œ EM/F1 ê³„ì‚°)",
    )

    # ì•™ìƒë¸” ì„¤ì •
    parser.add_argument(
        "--top_k",
        type=int,
        default=5,
        help="ê° ëª¨ë¸ì—ì„œ ê³ ë ¤í•  í›„ë³´ ìˆ˜ (default: 5)",
    )
    parser.add_argument(
        "--score_key",
        type=str,
        default="probability",
        choices=["probability", "score"],
        help="ì‚¬ìš©í•  score í•„ë“œ (default: probability)",
    )
    parser.add_argument(
        "--use_rank_score",
        action="store_true",
        help="Rank ê¸°ë°˜ ì ìˆ˜ ì‚¬ìš© (1ìœ„=1.0, 2ìœ„=0.8, ...)",
    )
    parser.add_argument(
        "--save_details",
        action="store_true",
        help="ì•™ìƒë¸” ìƒì„¸ ì •ë³´ ì €ì¥",
    )
    parser.add_argument(
        "--dataset_path",
        type=str,
        default="./data/train_dataset/validation",
        help="ì›ë³¸ ë°ì´í„°ì…‹ ê²½ë¡œ (ìˆœì„œ ìœ ì§€ìš©, default: ./data/train_dataset/validation)",
    )

    args = parser.parse_args()

    # nbest íŒŒì¼ ê²½ë¡œ ìˆ˜ì§‘
    if args.nbest_paths:
        nbest_paths = args.nbest_paths
    else:
        nbest_paths = []
        for output_dir in args.output_dirs:
            nbest_file = find_nbest_file(output_dir)
            if nbest_file:
                nbest_paths.append(nbest_file)
            else:
                logger.error(f"âŒ nbest_predictions.json not found in: {output_dir}")
                sys.exit(1)

    num_models = len(nbest_paths)

    # ê°€ì¤‘ì¹˜ ì²˜ë¦¬
    if args.auto_weight_by_em:
        # EM ì ìˆ˜ ê¸°ë°˜ ìë™ ê°€ì¤‘ì¹˜
        if not args.eval_file:
            logger.error("âŒ --auto_weight_by_em ì‚¬ìš©ì‹œ --eval_file í•„ìš”")
            sys.exit(1)

        logger.info("ğŸ“Š Computing EM-based weights...")
        answers = load_answers(args.eval_file)
        em_scores = []

        for nbest_path in nbest_paths:
            # predictions.json ì°¾ê¸°
            pred_path = os.path.join(os.path.dirname(nbest_path), "predictions.json")
            if os.path.exists(pred_path):
                with open(pred_path) as f:
                    preds = json.load(f)
                em = compute_em_f1(preds, answers)["em"]
                em_scores.append(em)
                model_name = os.path.basename(os.path.dirname(nbest_path))
                logger.info(f"   {model_name}: EM = {em:.2f}%")
            else:
                em_scores.append(50.0)  # ê¸°ë³¸ê°’
                logger.warning(f"   predictions.json not found, using default EM=50")

        # EM ì ìˆ˜ë¥¼ ê°€ì¤‘ì¹˜ë¡œ ë³€í™˜ (ì •ê·œí™”)
        weights = [em / sum(em_scores) for em in em_scores]
        logger.info(f"ğŸ“Š Auto weights: {[f'{w:.3f}' for w in weights]}")
    elif args.weights:
        if len(args.weights) != num_models:
            logger.error(
                f"âŒ weights ê°œìˆ˜({len(args.weights)}) != ëª¨ë¸ ìˆ˜({num_models})"
            )
            sys.exit(1)
        weights = [w / sum(args.weights) for w in args.weights]
    else:
        weights = [1.0 / num_models] * num_models

    # ì„¤ì • ì¶œë ¥
    print("\n" + "=" * 60)
    print("ğŸ”€ Heterogeneous Model Ensemble")
    print("=" * 60)
    print(f"   Models: {num_models}")
    for i, (path, w) in enumerate(zip(nbest_paths, weights)):
        print(
            f"   [{i + 1}] {os.path.basename(os.path.dirname(path))} (weight: {w:.3f})"
        )
        print(f"       â†’ {path}")
    print(f"   Top-k candidates: {args.top_k}")
    print(f"   Score key: {args.score_key}")
    print(f"   Use rank score: {args.use_rank_score}")
    print("=" * 60)

    # nbest ë¡œë“œ
    nbest_list = [load_nbest(path) for path in nbest_paths]

    # ì•™ìƒë¸” ìˆ˜í–‰
    config = EnsembleConfig(
        top_k_candidates=args.top_k,
        score_key=args.score_key,
        use_rank_score=args.use_rank_score,
    )

    predictions, details = ensemble_predictions(nbest_list, weights, config)

    logger.info(f"âœ… Ensemble complete: {len(predictions)} predictions")

    # í‰ê°€ (ì˜µì…˜)
    metrics = None
    if args.eval_file:
        logger.info(f"\nğŸ“Š Evaluating against: {args.eval_file}")
        answers = load_answers(args.eval_file)
        metrics = compute_em_f1(predictions, answers)
        print("\n" + "=" * 40)
        print("ğŸ“ˆ Evaluation Results")
        print("=" * 40)
        print(f"   EM:  {metrics['em']:.2f}%")
        print(f"   F1:  {metrics['f1']:.2f}%")
        print(f"   Total: {metrics['total']} questions")
        print("=" * 40)

    # ì›ë³¸ ë°ì´í„°ì…‹ ìˆœì„œ ê°€ì ¸ì˜¤ê¸°
    ordered_ids = None
    if os.path.exists(args.dataset_path):
        try:
            from datasets import load_from_disk

            dataset = load_from_disk(args.dataset_path)
            ordered_ids = [ex["id"] for ex in dataset]
            logger.info(
                f"ğŸ“ Loaded original order from: {args.dataset_path} ({len(ordered_ids)} examples)"
            )
        except Exception as e:
            logger.warning(f"âš ï¸ Could not load dataset order: {e}")

    # ì•™ìƒë¸” ì„¤ì • ì €ì¥ìš©
    model_names = [os.path.basename(os.path.dirname(p)) for p in nbest_paths]
    ensemble_config = {
        "ensemble_type": "hetero_text_voting",
        "models": model_names,
        "weights": weights,
        "top_k_candidates": args.top_k,
        "score_key": args.score_key,
        "use_rank_score": args.use_rank_score,
        "eval_file": args.eval_file,
        "dataset_path": args.dataset_path,
    }

    # ì €ì¥
    save_predictions(
        predictions,
        args.output_path,
        details if args.save_details else None,
        ordered_ids=ordered_ids,
        ensemble_config=ensemble_config,
        metrics=metrics,
    )

    print("\nğŸ‰ Heterogeneous ensemble complete!")
    print(f"ğŸ“‚ Output directory: {os.path.dirname(args.output_path) or '.'}")


if __name__ == "__main__":
    main()
