"""
Top-K ì‹¤í—˜ ìŠ¤í¬ë¦½íŠ¸

ë‹¤ì–‘í•œ top_k_retrieval ê°’ìœ¼ë¡œ inferenceë¥¼ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë¹„êµí•©ë‹ˆë‹¤.
- Recall ë¶„ì„ í›„ ê° top-kì— ëŒ€í•´ inference ì‹¤í–‰
- EM, F1 ìŠ¤ì½”ì–´ ìë™ ìˆ˜ì§‘ ë° ë¹„êµí‘œ ìƒì„±
- ë¡œê·¸ íŒŒì¼ ìë™ ì €ì¥

ì‚¬ìš©ë²•:
    python scripts/run_topk_experiments.py --topk_values 10,20,30,40,50
"""

import argparse
import json
import os
import subprocess
import sys
import time
from pathlib import Path
from typing import List, Dict
import pandas as pd

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ
ROOT_DIR = Path(__file__).parent.parent


def run_retrieval_analysis(retriever: str = "koe5") -> None:
    """Retrieval recall ë¶„ì„ ì‹¤í–‰"""
    print("\n" + "=" * 80)
    print("ğŸ“Š STEP 1: Retrieval Recall ë¶„ì„")
    print("=" * 80)

    cmd = [
        sys.executable,
        str(ROOT_DIR / "tests" / "test_retrieval_recall.py"),
        "--retriever",
        retriever,
        "--analyze_full",
    ]

    subprocess.run(cmd, check=True)


def update_config_topk(config_path: Path, topk: int) -> None:
    """YAML configì˜ top_k_retrieval ê°’ ìˆ˜ì •"""
    import yaml

    with open(config_path, "r", encoding="utf-8") as f:
        config = yaml.safe_load(f)

    # top_k_retrievalì€ ìµœìƒìœ„ì— ìœ„ì¹˜
    config["top_k_retrieval"] = topk

    with open(config_path, "w", encoding="utf-8") as f:
        yaml.dump(
            config, f, allow_unicode=True, default_flow_style=False, sort_keys=False
        )

    print(f"   âœ“ Config updated: top_k_retrieval = {topk}")


def run_inference(config_path: Path, topk: int, backup_dir: Path) -> Dict:
    """Inference ì‹¤í–‰ ë° ê²°ê³¼ ìˆ˜ì§‘"""
    print(f"\n{'=' * 80}")
    print(f"ğŸ”¬ STEP 2-{topk}: Inference with top_k={topk}")
    print("=" * 80)

    # Config ìˆ˜ì • (validationìœ¼ë¡œ ë³€ê²½)
    update_config_topk(config_path, topk)
    update_config_split(config_path, "validation")  # validationìœ¼ë¡œ ê°•ì œ ì„¤ì •

    # Inference ì‹¤í–‰ (ì‹¤ì‹œê°„ ì¶œë ¥)
    start_time = time.time()
    cmd = [sys.executable, str(ROOT_DIR / "inference.py"), str(config_path)]
    result = subprocess.run(cmd)
    elapsed_time = time.time() - start_time

    if result.returncode != 0:
        print(f"   âŒ Inference failed for top_k={topk}")
        return None

    print(f"   âœ“ Inference completed in {elapsed_time:.1f}s")

    # ê²°ê³¼ íŒŒì¼ ì°¾ê¸° (validation ê²°ê³¼)
    output_dir = (
        ROOT_DIR
        / "outputs"
        / "dahyeong"
        / "HANTAEK_rob-large-kq-v1-qa-finetuned_stride64"
    )

    # ğŸ“Œ Top-Kë³„ ê²°ê³¼ íŒŒì¼ ë°±ì—… (ë®ì–´ì“°ê¸° ë°©ì§€)
    import shutil

    val_files = [
        "val_results.json",
        "predictions_val.json",
        "nbest_predictions_val.json",
        "val_pred.csv",
    ]

    print(f"   ğŸ’¾ Backing up results for top_k={topk}...")
    for filename in val_files:
        src = output_dir / filename
        if src.exists():
            dst = backup_dir / f"{filename.replace('val', f'val_topk{topk}')}"
            shutil.copy2(src, dst)

    # trainer.evaluate()ê°€ ìƒì„±í•œ val_results.jsonì—ì„œ ë©”íŠ¸ë¦­ ì½ê¸°
    eval_results_path = output_dir / "val_results.json"

    if not eval_results_path.exists():
        print(f"   âš ï¸  val_results.json not found")
        return None

    with open(eval_results_path, "r") as f:
        metrics = json.load(f)

    # í‚¤ í˜•ì‹: eval_exact_match, eval_f1
    em = metrics.get("eval_exact_match", 0.0)
    f1 = metrics.get("eval_f1", 0.0)
    print(f"   ğŸ“ˆ EM: {em:.2f}% | F1: {f1:.2f}%")
    print(f"   ğŸ“ˆ EM: {em:.2f}% | F1: {f1:.2f}%")

    return {"top_k": topk, "em": em, "f1": f1, "time": elapsed_time}


def update_config_split(config_path: Path, split: str) -> None:
    """YAML configì˜ inference_split ê°’ ìˆ˜ì •"""
    import yaml

    with open(config_path, "r", encoding="utf-8") as f:
        config = yaml.safe_load(f)

    config["inference_split"] = split

    with open(config_path, "w", encoding="utf-8") as f:
        yaml.dump(
            config, f, allow_unicode=True, default_flow_style=False, sort_keys=False
        )

    print(f"   âœ“ Config updated: inference_split = {split}")


def save_comparison_table(results: List[Dict], log_dir: Path) -> None:
    """ê²°ê³¼ ë¹„êµí‘œ ìƒì„± ë° ì €ì¥"""
    if not results:
        print("   âš ï¸  No results to save")
        return

    # DataFrame ìƒì„±
    df = pd.DataFrame(results)
    df = df.sort_values("top_k")

    # ì¦ê°ìœ¨ ê³„ì‚° (baseline: top_k=10)
    baseline_em = (
        df[df["top_k"] == 10]["em"].values[0]
        if 10 in df["top_k"].values
        else df.iloc[0]["em"]
    )
    baseline_f1 = (
        df[df["top_k"] == 10]["f1"].values[0]
        if 10 in df["top_k"].values
        else df.iloc[0]["f1"]
    )

    df["em_delta"] = df["em"] - baseline_em
    df["f1_delta"] = df["f1"] - baseline_f1

    # íƒ€ì„ìŠ¤íƒ¬í”„
    timestamp = time.strftime("%Y%m%d_%H%M%S")

    # CSV ì €ì¥
    csv_path = log_dir / f"topk_comparison_{timestamp}.csv"
    df.to_csv(csv_path, index=False, encoding="utf-8-sig")
    print(f"\n   ğŸ’¾ CSV saved: {csv_path}")

    # í…ìŠ¤íŠ¸ ë¡œê·¸ ì €ì¥
    log_path = log_dir / f"topk_comparison_{timestamp}.txt"
    with open(log_path, "w", encoding="utf-8") as f:
        f.write("=" * 80 + "\n")
        f.write("Top-K Retrieval Comparison Report\n")
        f.write("=" * 80 + "\n")
        f.write(f"Date: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"Baseline: top_k=10 (EM={baseline_em:.2f}%, F1={baseline_f1:.2f}%)\n")
        f.write("\n")

        f.write(
            f"{'Top-K':<8} {'EM (%)':<10} {'F1 (%)':<10} {'EM Î”':<10} {'F1 Î”':<10} {'Time (s)':<12}\n"
        )
        f.write("-" * 80 + "\n")

        for _, row in df.iterrows():
            f.write(f"{row['top_k']:<8} ")
            f.write(f"{row['em']:<10.2f} ")
            f.write(f"{row['f1']:<10.2f} ")
            f.write(f"{row['em_delta']:+10.2f} ")
            f.write(f"{row['f1_delta']:+10.2f} ")
            f.write(f"{row['time']:<12.1f}\n")

        f.write("=" * 80 + "\n")

        # ìµœê³  ì„±ëŠ¥
        best_em_row = df.loc[df["em"].idxmax()]
        best_f1_row = df.loc[df["f1"].idxmax()]

        f.write("\nğŸ† Best Results:\n")
        f.write(
            f"  - Best EM: top_k={int(best_em_row['top_k'])} with {best_em_row['em']:.2f}%\n"
        )
        f.write(
            f"  - Best F1: top_k={int(best_f1_row['top_k'])} with {best_f1_row['f1']:.2f}%\n"
        )

    print(f"   ğŸ’¾ Log saved: {log_path}")

    # ì½˜ì†” ì¶œë ¥
    print("\n" + "=" * 80)
    print("ğŸ“Š Top-K Comparison Summary")
    print("=" * 80)
    print(
        f"{'Top-K':<8} {'EM (%)':<10} {'F1 (%)':<10} {'EM Î”':<10} {'F1 Î”':<10} {'Time (s)':<12}"
    )
    print("-" * 80)

    for _, row in df.iterrows():
        print(f"{row['top_k']:<8} ", end="")
        print(f"{row['em']:<10.2f} ", end="")
        print(f"{row['f1']:<10.2f} ", end="")
        print(f"{row['em_delta']:+10.2f} ", end="")
        print(f"{row['f1_delta']:+10.2f} ", end="")
        print(f"{row['time']:<12.1f}")

    print("=" * 80)
    print(
        f"\nğŸ† Best EM: top_k={int(best_em_row['top_k'])} with {best_em_row['em']:.2f}%"
    )
    print(
        f"ğŸ† Best F1: top_k={int(best_f1_row['top_k'])} with {best_f1_row['f1']:.2f}%"
    )
    print("=" * 80)


def main():
    parser = argparse.ArgumentParser(
        description="Top-K Retrieval ì‹¤í—˜ - ë‹¤ì–‘í•œ top_k ê°’ìœ¼ë¡œ inference ì‹¤í–‰ ë° ë¹„êµ"
    )
    parser.add_argument(
        "--topk_values",
        type=str,
        default="10,20,30,40,50",
        help="ì‹¤í—˜í•  top_k ê°’ë“¤ (ì‰¼í‘œë¡œ êµ¬ë¶„, ì˜ˆ: 10,20,30,40,50)",
    )
    parser.add_argument(
        "--config",
        type=str,
        default="./configs/active/HANTAEK_roberta-large-korquad-v1-qa-finetuned_stride64.yaml",
        help="Inference config íŒŒì¼ ê²½ë¡œ",
    )
    parser.add_argument(
        "--skip_recall", action="store_true", help="Retrieval recall ë¶„ì„ ê±´ë„ˆë›°ê¸°"
    )
    parser.add_argument(
        "--retriever",
        type=str,
        default="koe5",
        choices=["koe5", "tfidf"],
        help="Retrieval ë°©ì‹",
    )

    args = parser.parse_args()

    # Top-K ê°’ íŒŒì‹±
    topk_values = sorted([int(k.strip()) for k in args.topk_values.split(",")])
    config_path = Path(args.config)

    print("=" * 80)
    print("ğŸš€ Top-K Retrieval Experiment")
    print("=" * 80)
    print(f"Config: {config_path.name}")
    print(f"Top-K values: {topk_values}")
    print(f"Retriever: {args.retriever}")
    print("=" * 80)

    # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
    log_dir = ROOT_DIR / "logs"
    log_dir.mkdir(exist_ok=True)

    # ë°±ì—… ë””ë ‰í† ë¦¬ ìƒì„± (ê° top_kì˜ predictions ì €ì¥)
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    backup_dir = log_dir / f"topk_experiment_{timestamp}"
    backup_dir.mkdir(exist_ok=True)
    print(f"\nğŸ“ Predictions backup directory: {backup_dir}")

    # Step 1: Retrieval Recall ë¶„ì„
    if not args.skip_recall:
        run_retrieval_analysis(args.retriever)
    else:
        print("\nâ­ï¸  Retrieval recall ë¶„ì„ ê±´ë„ˆë›°ê¸°")

    # Step 2: ê° top_kì— ëŒ€í•´ inference ì‹¤í–‰
    results = []
    for topk in topk_values:
        result = run_inference(config_path, topk, backup_dir)
        if result:
            results.append(result)

    # Step 3: ê²°ê³¼ ë¹„êµí‘œ ìƒì„±
    if results:
        save_comparison_table(results, log_dir)
    else:
        print("\n   âš ï¸  No successful results to compare")

    print("\nâœ… ì‹¤í—˜ ì™„ë£Œ!")


if __name__ == "__main__":
    main()
