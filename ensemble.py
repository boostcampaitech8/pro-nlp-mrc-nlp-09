"""
MRC ëª¨ë¸ ì•™ìƒë¸” (Soft Voting with Weighted Sum)

ì—¬ëŸ¬ í•™ìŠµëœ ëª¨ë¸ì˜ start/end logitsë¥¼ weighted sumí•˜ì—¬ ì•™ìƒë¸” ìˆ˜í–‰

ì‚¬ìš© ì˜ˆì‹œ:
    # Test ë°ì´í„°ì…‹ ì‚¬ìš© (ê¸°ë³¸)
    python ensemble.py --output_dir ./outputs/ensemble/test
    
    # Train ë°ì´í„°ì…‹ì˜ validation ì…‹ ì‚¬ìš©
    python ensemble.py --use_train_validation --train_dataset ./data/train_dataset --output_dir ./outputs/ensemble/validation --no_retrieval
    
    # ì»¤ë§¨ë“œë¼ì¸ì—ì„œ ëª¨ë¸ ê²½ë¡œ ì§€ì •
    python ensemble.py --model_paths ./outputs/model1 ./outputs/model2 --weights 0.6 0.4
"""

import os
import json
import csv
import argparse
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass

import torch
import numpy as np
from tqdm import tqdm
from datasets import load_from_disk, DatasetDict, Dataset, Features, Value, Sequence
from transformers import (
    AutoConfig,
    AutoModelForQuestionAnswering,
    AutoTokenizer,
    DataCollatorWithPadding,
)

from src.retrieval import get_retriever
from src.retrieval.reranker import CrossEncoderReranker # Reranker ì„í¬íŠ¸ ì¶”ê°€
from src.utils.tokenization import get_tokenizer
from src.utils.qa import postprocess_qa_predictions
from transformers import AutoTokenizer as HFAutoTokenizer


# ============================================================
# ğŸ¯ ì—¬ê¸°ì„œ ì•™ìƒë¸”í•  ëª¨ë¸ë“¤ì„ ì„¤ì •í•˜ì„¸ìš”!
# ============================================================
ENSEMBLE_MODELS = [
    # (ëª¨ë¸ ê²½ë¡œ, ê°€ì¤‘ì¹˜)
    # ê°€ì¤‘ì¹˜ëŠ” ìë™ìœ¼ë¡œ ì •ê·œí™”ë©ë‹ˆë‹¤ (í•©ì´ 1ì´ ë˜ë„ë¡)
    ("/data/ephemeral/home/junbeom/MRC/outputs/teawon/hanteck2", 1.0),
    ("/data/ephemeral/home/junbeom/MRC/outputs/teawon/oceann2", 1.0),
    ("/data/ephemeral/home/junbeom/MRC/outputs/teawon/roberta2", 1.0),
    ("/data/ephemeral/home/junbeom/MRC/outputs/teawon/uomnf2", 1.0),
    # ("./outputs/dahyeong/model", 0.5),
    
    # ğŸ’¡ ê°€ì¤‘ì¹˜ ì˜ˆì‹œ:
    # - ê· ë“±: ëª¨ë‘ 1.0
    # - ì„±ëŠ¥ ê¸°ë°˜: EM ì ìˆ˜ì— ë¹„ë¡€ (ì˜ˆ: 75ì  â†’ 0.75, 80ì  â†’ 0.80)
    # - ìˆ˜ë™ ì¡°ì ˆ: ì›í•˜ëŠ” ë¹„ìœ¨ë¡œ ì„¤ì •
]
# ============================================================


@dataclass
class EnsembleConfig:
    """ì•™ìƒë¸” ì„¤ì •"""
    model_paths: List[str]          # ëª¨ë¸ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
    weights: Optional[List[float]]  # ëª¨ë¸ë³„ ê°€ì¤‘ì¹˜ (Noneì´ë©´ ê· ë“±)
    output_dir: str                 # ê²°ê³¼ ì €ì¥ ê²½ë¡œ
    test_dataset_path: Optional[str] = None  # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ê²½ë¡œ
    train_dataset_path: Optional[str] = None  # í•™ìŠµ ë°ì´í„°ì…‹ ê²½ë¡œ (validation ì…‹ ì‚¬ìš© ì‹œ)
    use_train_validation: bool = False  # train_datasetì˜ validation ì…‹ ì‚¬ìš© ì—¬ë¶€
    max_seq_length: int = 384
    doc_stride: int = 128
    max_answer_length: int = 30
    top_k_retrieval: int = 10
    batch_size: int = 16
    use_retrieval: bool = True
    retrieval_alpha: float = 0.5  # Hybrid Retrievalì˜ BM25 ê°€ì¤‘ì¹˜
    retrieval_tokenizer_name: str = "kiwi"  # kiwi or auto
    bm25_impl: str = "rank_bm25"  # rank_bm25 or bm25s
    bm25_k1: float = 1.2
    bm25_b: float = 0.6
    bm25_delta: float = 0.5
    fusion_method: str = "rrf"  # rrf or score
    corpus_emb_path: Optional[str] = None  # KoE5 corpus embedding ê²½ë¡œ (Noneì´ë©´ ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©)
    dense_retriever_type: str = "koe5" # Hybrid ë‚´ë¶€ì—ì„œ ì‚¬ìš©í•  Dense Retriever íƒ€ì… ("koe5" or "kure")
    # Reranker Settings
    reranker_name: Optional[str] = "BAAI/bge-reranker-v2-m3"
    rerank_topk: int = 50


class MRCEnsemble:
    """MRC ëª¨ë¸ ì•™ìƒë¸” í´ë˜ìŠ¤"""
    
    def __init__(self, config: EnsembleConfig):
        self.config = config
        self.models = []
        self.tokenizers = []
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # ê°€ì¤‘ì¹˜ ì„¤ì • (Noneì´ë©´ ê· ë“± ë¶„ë°°)
        if config.weights is None:
            self.weights = [1.0 / len(config.model_paths)] * len(config.model_paths)
        else:
            # ê°€ì¤‘ì¹˜ ì •ê·œí™”
            total = sum(config.weights)
            self.weights = [w / total for w in config.weights]
        
        print(f"ğŸ”§ Device: {self.device}")
        print(f"ğŸ“Š Model weights: {self.weights}")
    
    def load_models(self):
        """ëª¨ë“  ëª¨ë¸ ë¡œë“œ"""
        print("\n" + "=" * 60)
        print("ğŸ“¦ Loading models for ensemble...")
        print("=" * 60)
        
        for i, model_path in enumerate(self.config.model_paths):
            print(f"\n[{i+1}/{len(self.config.model_paths)}] Loading: {model_path}")
            
            # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
            config = AutoConfig.from_pretrained(model_path)
            tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)
            model = AutoModelForQuestionAnswering.from_pretrained(model_path, config=config)
            model.to(self.device)
            model.eval()
            
            self.models.append(model)
            self.tokenizers.append(tokenizer)
            
            print(f"   âœ… Loaded: {config.model_type}")
        
        print(f"\nâœ… Total {len(self.models)} models loaded!")
    
    def load_dataset(self) -> DatasetDict:
        """ë°ì´í„°ì…‹ ë¡œë“œ (test ë˜ëŠ” trainì˜ validation)"""
        if self.config.use_train_validation:
            if self.config.train_dataset_path is None:
                raise ValueError("âŒ use_train_validation=Trueì¸ ê²½ìš° train_dataset_pathë¥¼ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.")
            print(f"\nğŸ“‚ Loading train dataset from: {self.config.train_dataset_path}")
            train_datasets = load_from_disk(self.config.train_dataset_path)
            print(f"   Train dataset splits: {list(train_datasets.keys())}")
            
            # validation ì…‹ì´ ìˆëŠ”ì§€ í™•ì¸
            if "validation" not in train_datasets:
                raise ValueError(
                    f"âŒ train_datasetì— 'validation' splitì´ ì—†ìŠµë‹ˆë‹¤.\n"
                    f"   Available splits: {list(train_datasets.keys())}"
                )
            
            # validation ì…‹ë§Œ ì‚¬ìš©
            datasets = DatasetDict({"validation": train_datasets["validation"]})
            print(f"   âœ… Using validation split: {len(datasets['validation'])} examples")
        else:
            if self.config.test_dataset_path is None:
                raise ValueError("âŒ use_train_validation=Falseì¸ ê²½ìš° test_dataset_pathë¥¼ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.")
            print(f"\nğŸ“‚ Loading test dataset from: {self.config.test_dataset_path}")
            datasets = load_from_disk(self.config.test_dataset_path)
            print(f"   Dataset: {datasets}")
        
        return datasets
    
    def run_retrieval(self, datasets: DatasetDict) -> DatasetDict:
        """Hybrid Retrieval ìˆ˜í–‰ (BM25Plus + KoE5) + Reranking"""
        if not self.config.use_retrieval:
            return datasets
        
        print("\nğŸ” Running Hybrid Retrieval (BM25Plus + KoE5)...")
        
        # Tokenizer ì„¤ì •
        print(f"[INIT] Setting up tokenizer: {self.config.retrieval_tokenizer_name}")
        model_tokenizer = HFAutoTokenizer.from_pretrained("klue/roberta-large")  # Default fallback
        tokenize_fn = get_tokenizer(self.config.retrieval_tokenizer_name, model_tokenizer)
        
        # Hybrid Retrieval ìƒì„±
        print(f"[INIT] Setting up Hybrid Retriever")
        print(f"       - BM25 Impl: {self.config.bm25_impl} (k1={self.config.bm25_k1}, b={self.config.bm25_b}, delta={self.config.bm25_delta})")
        print(f"       - Hybrid Alpha: {self.config.retrieval_alpha}")
        print(f"       - Fusion Method: {self.config.fusion_method}")
        print(f"       - Dense Retriever Type: {self.config.dense_retriever_type}") # ì¶”ê°€ëœ ë¶€ë¶„
        
        retriever = get_retriever(
            retrieval_type="hybrid",
            tokenize_fn=tokenize_fn,
            data_path="./data",
            context_path="wikipedia_documents_normalized.json",
            # Hybrid Args
            alpha=self.config.retrieval_alpha,
            fusion_method=self.config.fusion_method,
            dense_retriever_type=self.config.dense_retriever_type, # ì¶”ê°€ëœ ë¶€ë¶„
            # BM25 Args
            impl=self.config.bm25_impl,
            k1=self.config.bm25_k1,
            b=self.config.bm25_b,
            delta=self.config.bm25_delta,
            # KoE5/Kure Args
            corpus_emb_path=self.config.corpus_emb_path,
            passages_meta_path=None, # Kureê°€ ensemble.pyì—ì„œ í•„ìš”í•˜ë©´ ì¶”ê°€í•´ì¤˜ì•¼ í•¨. í˜„ì¬ëŠ” ì—†ìŒ.
                                     # but get_path() in retrieval/hybrid.py will handle default.
        )
        
        print("[INIT] Building retriever index...")
        retriever.build()
        
        # Reranker ì´ˆê¸°í™”
        reranker = None
        if self.config.reranker_name:
            print(f"[INIT] Setting up Reranker: {self.config.reranker_name}")
            reranker = CrossEncoderReranker(model_name=self.config.reranker_name)
        
        # Retrieval ìˆ˜í–‰
        # Rerankerê°€ ìˆìœ¼ë©´ ë” ë§ì´ ê°€ì ¸ì™€ì„œ ì¬ì •ë ¬
        top_k = self.config.rerank_topk if reranker else self.config.top_k_retrieval
        print(f"   - Retrieving top-{top_k} candidates...")
        
        queries = datasets["validation"]["question"]
        doc_scores, doc_indices = retriever.get_relevant_doc_bulk(queries, k=top_k)
        
        # Context êµ¬ì„± (Reranking í¬í•¨)
        final_contexts = []
        print(f"   - Constructing contexts{' (with Reranking)' if reranker else ''}...")
        
        for i in tqdm(range(len(queries)), desc="Context Processing"):
            query = queries[i]
            indices = doc_indices[i]
            passages = [retriever.contexts[idx] for idx in indices]
            
            if reranker:
                # Reranking
                r_scores = reranker.rerank(query, passages)
                scored = sorted(zip(passages, r_scores), key=lambda x: x[1], reverse=True)
                # ìµœì¢… Top-K ì„ íƒ
                selected_passages = [p for p, _ in scored][:self.config.top_k_retrieval]
                final_contexts.append(" ".join(selected_passages))
            else:
                # No Reranking
                final_contexts.append(" ".join(passages))
        
        # Dataset ì¬êµ¬ì„± (DataFrame ìƒì„± ì—†ì´ ì§ì ‘)
        # answersê°€ ìˆëŠ” ê²½ìš°ì™€ ì—†ëŠ” ê²½ìš° ì²˜ë¦¬
        data_dict = {
            "id": datasets["validation"]["id"],
            "question": queries,
            "context": final_contexts
        }
        
        if "answers" in datasets["validation"].column_names:
            data_dict["answers"] = datasets["validation"]["answers"]
            f = Features({
                "id": Value(dtype="string"),
                "question": Value(dtype="string"),
                "context": Value(dtype="string"),
                "answers": Sequence(feature={"text": Value(dtype="string"), "answer_start": Value(dtype="int32")})
            })
        else:
            f = Features({
                "id": Value(dtype="string"),
                "question": Value(dtype="string"),
                "context": Value(dtype="string")
            })
            
        new_ds = Dataset.from_dict(data_dict, features=f)
        datasets = DatasetDict({"validation": new_ds})
        
        print(f"   âœ… Retrieval complete: {len(datasets['validation'])} examples")
        
        return datasets
    
    def prepare_features(self, examples, tokenizer):
        """í† í°í™” ë° feature ìƒì„±"""
        pad_on_right = tokenizer.padding_side == "right"
        
        tokenized = tokenizer(
            examples["question"] if pad_on_right else examples["context"],
            examples["context"] if pad_on_right else examples["question"],
            truncation="only_second" if pad_on_right else "only_first",
            max_length=self.config.max_seq_length,
            stride=self.config.doc_stride,
            return_overflowing_tokens=True,
            return_offsets_mapping=True,
            padding="max_length",
        )
        
        sample_mapping = tokenized.pop("overflow_to_sample_mapping")
        tokenized["example_id"] = []
        
        for i in range(len(tokenized["input_ids"])):
            sequence_ids = tokenized.sequence_ids(i)
            context_index = 1 if pad_on_right else 0
            sample_index = sample_mapping[i]
            tokenized["example_id"].append(examples["id"][sample_index])
            
            tokenized["offset_mapping"][i] = [
                (o if sequence_ids[k] == context_index else None)
                for k, o in enumerate(tokenized["offset_mapping"][i])
            ]
        
        return tokenized
    
    def get_logits_from_model(
        self, 
        model, 
        tokenizer, 
        dataset
    ) -> Tuple[np.ndarray, np.ndarray]:
        """ë‹¨ì¼ ëª¨ë¸ì—ì„œ logits ì¶”ì¶œ"""
        
        # Feature ì¤€ë¹„
        features = dataset.map(
            lambda x: self.prepare_features(x, tokenizer),
            batched=True,
            remove_columns=dataset.column_names,
            load_from_cache_file=False,
        )
        
        # token_type_ids ì²˜ë¦¬
        model_type = getattr(model.config, "model_type", "").lower()
        type_vocab_size = getattr(model.config, "type_vocab_size", 0)
        use_token_type_ids = type_vocab_size > 1
        
        # DataLoader ì¤€ë¹„
        data_collator = DataCollatorWithPadding(tokenizer)
        
        all_start_logits = []
        all_end_logits = []
        
        # ë°°ì¹˜ ì²˜ë¦¬
        batch_size = self.config.batch_size
        
        for i in tqdm(range(0, len(features), batch_size), desc="Inference"):
            batch_indices = range(i, min(i + batch_size, len(features)))
            
            # ë°°ì¹˜ ë°ì´í„° ì¤€ë¹„
            batch = {
                "input_ids": torch.tensor([features[j]["input_ids"] for j in batch_indices]),
                "attention_mask": torch.tensor([features[j]["attention_mask"] for j in batch_indices]),
            }
            
            if use_token_type_ids and "token_type_ids" in features.column_names:
                batch["token_type_ids"] = torch.tensor(
                    [features[j]["token_type_ids"] for j in batch_indices]
                )
            
            # GPUë¡œ ì´ë™
            batch = {k: v.to(self.device) for k, v in batch.items()}
            
            # ì¶”ë¡ 
            with torch.no_grad():
                outputs = model(**batch)
            
            all_start_logits.append(outputs.start_logits.cpu().numpy())
            all_end_logits.append(outputs.end_logits.cpu().numpy())
        
        start_logits = np.concatenate(all_start_logits, axis=0)
        end_logits = np.concatenate(all_end_logits, axis=0)
        
        return start_logits, end_logits, features
    
    def ensemble_logits(
        self, 
        all_start_logits: List[np.ndarray], 
        all_end_logits: List[np.ndarray]
    ) -> Tuple[np.ndarray, np.ndarray]:
        """Weighted sumìœ¼ë¡œ logits ì•™ìƒë¸”"""
        
        print("\nğŸ¯ Ensembling logits with weighted sum...")
        
        # Weighted sum
        ensembled_start = np.zeros_like(all_start_logits[0])
        ensembled_end = np.zeros_like(all_end_logits[0])
        
        for i, (start, end) in enumerate(zip(all_start_logits, all_end_logits)):
            ensembled_start += self.weights[i] * start
            ensembled_end += self.weights[i] * end
            print(f"   Model {i+1}: weight={self.weights[i]:.3f}")
        
        return ensembled_start, ensembled_end
    
    def run(self):
        """ì•™ìƒë¸” ì‹¤í–‰"""
        print("\n" + "=" * 60)
        print("ğŸš€ MRC Ensemble (Soft Voting)")
        print("=" * 60)
        
        dataset_type = "train/validation" if self.config.use_train_validation else "test"
        print(f"ğŸ“‹ Dataset type: {dataset_type}")
        
        # 1. ëª¨ë¸ ë¡œë“œ
        self.load_models()
        
        # 2. ë°ì´í„°ì…‹ ë¡œë“œ
        datasets = self.load_dataset()
        
        # 3. Retrieval ìˆ˜í–‰
        if self.config.use_train_validation and self.config.use_retrieval:
            print("\nâš ï¸  Warning: validation ì…‹ ì‚¬ìš© ì‹œ ì¼ë°˜ì ìœ¼ë¡œ retrievalì„ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            print("   (validation ì…‹ì€ gold contextë¥¼ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤)")
            print("   retrievalì„ ê±´ë„ˆë›°ë ¤ë©´ --no_retrieval í”Œë˜ê·¸ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")
        
        datasets = self.run_retrieval(datasets)
        
        # 4. ê° ëª¨ë¸ì—ì„œ logits ì¶”ì¶œ
        print("\n" + "=" * 60)
        print("ğŸ“Š Extracting logits from each model...")
        print("=" * 60)
        
        all_start_logits = []
        all_end_logits = []
        features = None
        
        for i, (model, tokenizer) in enumerate(zip(self.models, self.tokenizers)):
            print(f"\n[Model {i+1}/{len(self.models)}]")
            start_logits, end_logits, features = self.get_logits_from_model(
                model, tokenizer, datasets["validation"]
            )
            all_start_logits.append(start_logits)
            all_end_logits.append(end_logits)
            print(f"   Logits shape: start={start_logits.shape}, end={end_logits.shape}")
        
        # 5. ì•™ìƒë¸”
        ensembled_start, ensembled_end = self.ensemble_logits(
            all_start_logits, all_end_logits
        )
        
        # 6. í›„ì²˜ë¦¬ ë° ë‹µë³€ ìƒì„±
        print("\n" + "=" * 60)
        print("ğŸ“ Post-processing predictions...")
        print("=" * 60)
        
        os.makedirs(self.config.output_dir, exist_ok=True)
        
        # prefix ì„¤ì • (validation ì…‹ì¸ì§€ test ì…‹ì¸ì§€ êµ¬ë¶„)
        prefix = "ensemble_validation" if self.config.use_train_validation else "ensemble"
        
        predictions = postprocess_qa_predictions(
            examples=datasets["validation"],
            features=features,
            predictions=(ensembled_start, ensembled_end),
            max_answer_length=self.config.max_answer_length,
            output_dir=self.config.output_dir,
            prefix=prefix,
        )
        
        # 7. CSV ì €ì¥
        csv_filename = "ensemble_predictions_validation.csv" if self.config.use_train_validation else "ensemble_predictions.csv"
        csv_path = os.path.join(self.config.output_dir, csv_filename)
        with open(csv_path, "w", encoding="utf-8") as f:
            writer = csv.writer(f, delimiter="\t")
            for key, value in predictions.items():
                writer.writerow([key, value])
        
        print(f"\nâœ… Ensemble complete!")
        dataset_type = "validation" if self.config.use_train_validation else "test"
        print(f"   ğŸ“Š Dataset type: {dataset_type}")
        print(f"   ğŸ“„ Predictions: {os.path.join(self.config.output_dir, f'predictions_{prefix}.json')}")
        print(f"   ğŸ“„ CSV: {csv_path}")
        
        return predictions


def main():
    parser = argparse.ArgumentParser(description="MRC Model Ensemble")
    parser.add_argument(
        "--model_paths", 
        nargs="+", 
        default=None,
        help="í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œë“¤ (ë¯¸ì§€ì •ì‹œ ENSEMBLE_MODELS ì‚¬ìš©)"
    )
    parser.add_argument(
        "--weights",
        nargs="+",
        type=float,
        default=None,
        help="ëª¨ë¸ë³„ ê°€ì¤‘ì¹˜ (ë¯¸ì§€ì •ì‹œ ENSEMBLE_MODELS ë˜ëŠ” ê· ë“± ë¶„ë°°)"
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        default="./outputs/taewon/ensemble/3_3_3_1",
        help="ê²°ê³¼ ì €ì¥ ê²½ë¡œ"
    )
    parser.add_argument(
        "--test_dataset",
        type=str,
        default="./data/test_dataset",
        help="í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ê²½ë¡œ (use_train_validation=Falseì¼ ë•Œ ì‚¬ìš©)"
    )
    parser.add_argument(
        "--train_dataset",
        type=str,
        default=None,
        help="í•™ìŠµ ë°ì´í„°ì…‹ ê²½ë¡œ (use_train_validation=Trueì¼ ë•Œ ì‚¬ìš©, validation ì…‹ì„ ê°€ì ¸ì˜´)"
    )
    parser.add_argument(
        "--use_train_validation",
        action="store_true",
        help="train_datasetì˜ validation ì…‹ ì‚¬ìš© (ê¸°ë³¸ê°’: False, test_dataset ì‚¬ìš©)"
    )
    parser.add_argument(
        "--top_k",
        type=int,
        default=10,
        help="Retrieval top-k"
    )
    parser.add_argument(
        "--batch_size",
        type=int,
        default=16,
        help="ë°°ì¹˜ ì‚¬ì´ì¦ˆ"
    )
    parser.add_argument(
        "--no_retrieval",
        action="store_true",
        help="Retrieval ì‚¬ìš© ì•ˆí•¨ (validationìš©)"
    )
    parser.add_argument(
        "--retrieval_alpha",
        type=float,
        default=0.5,
        help="Hybrid Retrievalì˜ BM25 ê°€ì¤‘ì¹˜ (0~1, ê¸°ë³¸ê°’: 0.5)"
    )
    parser.add_argument(
        "--retrieval_tokenizer_name",
        type=str,
        default="kiwi",
        help="Retrievalìš© tokenizer (kiwi or auto, ê¸°ë³¸ê°’: kiwi)"
    )
    parser.add_argument(
        "--bm25_impl",
        type=str,
        default="rank_bm25",
        help="BM25 êµ¬í˜„ì²´ (rank_bm25 or bm25s, ê¸°ë³¸ê°’: rank_bm25)"
    )
    parser.add_argument(
        "--bm25_k1",
        type=float,
        default=1.2,
        help="BM25 k1 íŒŒë¼ë¯¸í„° (ê¸°ë³¸ê°’: 1.2)"
    )
    parser.add_argument(
        "--bm25_b",
        type=float,
        default=0.6,
        help="BM25 b íŒŒë¼ë¯¸í„° (ê¸°ë³¸ê°’: 0.6)"
    )
    parser.add_argument(
        "--bm25_delta",
        type=float,
        default=0.5,
        help="BM25Plus delta íŒŒë¼ë¯¸í„° (ê¸°ë³¸ê°’: 0.5)"
    )
    parser.add_argument(
        "--fusion_method",
        type=str,
        default="rrf",
        help="Hybrid fusion ë°©ë²• (rrf or score, ê¸°ë³¸ê°’: rrf)"
    )
    parser.add_argument(
        "--corpus_emb_path",
        type=str,
        default=None,
        help="KoE5 corpus embedding ê²½ë¡œ (Noneì´ë©´ ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©)"
    )
    parser.add_argument(
        "--dense_retriever_type", # ì¶”ê°€ëœ ì¸ì
        type=str,
        default="koe5",
        help="Hybrid Retrieval ë‚´ë¶€ì—ì„œ ì‚¬ìš©í•  Dense Retriever íƒ€ì… (koe5 or kure, ê¸°ë³¸ê°’: koe5)"
    )
    
    args = parser.parse_args()
    
    # ëª¨ë¸ ê²½ë¡œì™€ ê°€ì¤‘ì¹˜ ê²°ì •
    if args.model_paths is not None:
        # ì»¤ë§¨ë“œë¼ì¸ì—ì„œ ì§€ì •í•œ ê²½ìš°
        model_paths = args.model_paths
        weights = args.weights
    else:
        # ENSEMBLE_MODELSì—ì„œ ê°€ì ¸ì˜¤ê¸°
        if not ENSEMBLE_MODELS:
            raise ValueError(
                "âŒ ì•™ìƒë¸”í•  ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤!\n"
                "ğŸ’¡ ensemble.py ìƒë‹¨ì˜ ENSEMBLE_MODELSì— ëª¨ë¸ì„ ì¶”ê°€í•˜ê±°ë‚˜\n"
                "   --model_paths ì¸ìë¥¼ ì‚¬ìš©í•˜ì„¸ìš”."
            )
        model_paths = [path for path, _ in ENSEMBLE_MODELS]
        weights = [weight for _, weight in ENSEMBLE_MODELS]
    
    print("\n" + "=" * 60)
    print("ğŸ“‹ Ensemble Configuration")
    print("=" * 60)
    for i, (path, w) in enumerate(zip(model_paths, weights or [1.0]*len(model_paths))):
        print(f"   [{i+1}] {path} (weight: {w})")
    print("=" * 60)
    
    # ì„¤ì • ìƒì„±
    config = EnsembleConfig(
        model_paths=model_paths,
        weights=weights,
        output_dir=args.output_dir,
        test_dataset_path=args.test_dataset if not args.use_train_validation else None,
        train_dataset_path=args.train_dataset if args.use_train_validation else None,
        use_train_validation=args.use_train_validation,
        top_k_retrieval=args.top_k,
        batch_size=args.batch_size,
        use_retrieval=not args.no_retrieval,
        retrieval_alpha=args.retrieval_alpha,
        retrieval_tokenizer_name=args.retrieval_tokenizer_name,
        bm25_impl=args.bm25_impl,
        bm25_k1=args.bm25_k1,
        bm25_b=args.bm25_b,
        bm25_delta=args.bm25_delta,
        fusion_method=args.fusion_method,
        corpus_emb_path=args.corpus_emb_path,
        dense_retriever_type=args.dense_retriever_type, # ì¶”ê°€ëœ ë¶€ë¶„
    )
    
    # ì•™ìƒë¸” ì‹¤í–‰
    ensemble = MRCEnsemble(config)
    predictions = ensemble.run()
    
    print("\n" + "=" * 60)
    print("ğŸ‰ Ensemble finished successfully!")
    print("=" * 60)


if __name__ == "__main__":
    main()

