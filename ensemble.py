"""
MRC ëª¨ë¸ ì•™ìƒë¸” (Soft Voting with Weighted Sum)

ì—¬ëŸ¬ í•™ìŠµëœ ëª¨ë¸ì˜ start/end logitsë¥¼ weighted sumí•˜ì—¬ ì•™ìƒë¸” ìˆ˜í–‰

ì‚¬ìš©ë²•:
  1. ì§ì ‘ ëª¨ë¸ ê²½ë¡œ ì§€ì •:
     python ensemble.py --model_paths ./outputs/model1 ./outputs/model2 --weights 0.5 0.5

  2. YAML config íŒŒì¼ ì‚¬ìš© (ì—¬ëŸ¬ ì‹¤í—˜ ê²°ê³¼ ì•™ìƒë¸”):
     python ensemble.py --configs configs/active/exp1.yaml configs/active/exp2.yaml

  3. íŒŒì¼ ìƒë‹¨ì˜ ENSEMBLE_MODELS ë¦¬ìŠ¤íŠ¸ ì‚¬ìš©:
     python ensemble.py

ì œì•½ì‚¬í•­:
  - ê°™ì€ í† í¬ë‚˜ì´ì €/ëª¨ë¸ ì•„í‚¤í…ì²˜ë¼ë¦¬ë§Œ ì•™ìƒë¸” ê°€ëŠ¥ (í…ì„œ shape ì¼ì¹˜ í•„ìš”)
"""

import os
import sys
import json
import csv
import glob
import argparse
import yaml
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass, field

import torch
import numpy as np
from tqdm import tqdm
from datasets import load_from_disk, DatasetDict, Dataset, Features, Value, Sequence
from transformers import (
    AutoConfig,
    AutoModelForQuestionAnswering,
    AutoTokenizer,
    DataCollatorWithPadding,
)

from src.retrieval.weighted_hybrid import WeightedHybridRetrieval
from src.retrieval.paths import get_path
from src.utils.qa import postprocess_qa_predictions
from src.utils import get_logger

logger = get_logger(__name__)


# ============================================================
# ğŸ¯ ì—¬ê¸°ì„œ ì•™ìƒë¸”í•  ëª¨ë¸ë“¤ì„ ì„¤ì •í•˜ì„¸ìš”!
# ============================================================
ENSEMBLE_MODELS = [
    # (ëª¨ë¸ ê²½ë¡œ, ê°€ì¤‘ì¹˜)
    # ê°€ì¤‘ì¹˜ëŠ” ìë™ìœ¼ë¡œ ì •ê·œí™”ë©ë‹ˆë‹¤ (í•©ì´ 1ì´ ë˜ë„ë¡)
    # ("./outputs/dahyeong/exp_ra_k3_ds128", 1.0),
    # ("./outputs/dahyeong/exp_ra_k5_ds128", 1.0),
    # ğŸ’¡ ê°€ì¤‘ì¹˜ ì˜ˆì‹œ:
    # - ê· ë“±: ëª¨ë‘ 1.0
    # - ì„±ëŠ¥ ê¸°ë°˜: EM ì ìˆ˜ì— ë¹„ë¡€ (ì˜ˆ: 75ì  â†’ 0.75, 80ì  â†’ 0.80)
    # - ìˆ˜ë™ ì¡°ì ˆ: ì›í•˜ëŠ” ë¹„ìœ¨ë¡œ ì„¤ì •
]
# ============================================================


def find_best_checkpoint(output_dir: str) -> str:
    """
    output_dirì—ì„œ best checkpoint ê²½ë¡œë¥¼ ì°¾ìŠµë‹ˆë‹¤.

    íƒìƒ‰ ìš°ì„ ìˆœìœ„:
    1. best_checkpoint_path.txt íŒŒì¼ì´ ìˆìœ¼ë©´ ê·¸ ë‚´ìš© ì‚¬ìš©
    2. checkpoint-* í´ë” ì¤‘ ê°€ì¥ ìµœì‹  ê²ƒ
    3. output_dir ìì²´ (model.safetensors/pytorch_model.binì´ ìˆëŠ” ê²½ìš°)
    """
    # 1. best_checkpoint_path.txt í™•ì¸
    best_path_file = os.path.join(output_dir, "best_checkpoint_path.txt")
    if os.path.exists(best_path_file):
        with open(best_path_file, "r") as f:
            checkpoint_path = f.read().strip()
            if os.path.exists(checkpoint_path):
                return checkpoint_path

    # 2. checkpoint-* í´ë” íƒìƒ‰
    checkpoint_dirs = glob.glob(os.path.join(output_dir, "checkpoint-*"))
    if checkpoint_dirs:
        # ìˆ«ìë¡œ ì •ë ¬í•˜ì—¬ ê°€ì¥ í° ê²ƒ ì„ íƒ
        def get_step(path):
            try:
                return int(os.path.basename(path).split("-")[1])
            except:
                return 0

        checkpoint_dirs.sort(key=get_step, reverse=True)
        return checkpoint_dirs[0]

    # 3. output_dir ìì²´ í™•ì¸
    model_files = ["model.safetensors", "pytorch_model.bin"]
    for model_file in model_files:
        if os.path.exists(os.path.join(output_dir, model_file)):
            return output_dir

    raise FileNotFoundError(
        f"âŒ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {output_dir}\n"
        f"ğŸ’¡ ì²´í¬í¬ì¸íŠ¸ ë˜ëŠ” model.safetensors/pytorch_model.binì´ í•„ìš”í•©ë‹ˆë‹¤."
    )


def load_config_from_yaml(yaml_path: str) -> Dict:
    """YAML config íŒŒì¼ ë¡œë“œ"""
    with open(yaml_path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)


def get_model_path_from_config(config: Dict) -> str:
    """YAML configì—ì„œ ëª¨ë¸ ê²½ë¡œ ì¶”ì¶œ (best checkpoint ìë™ íƒìƒ‰)"""
    output_dir = config.get("output_dir", "")
    if not output_dir:
        raise ValueError("configì— output_dirì´ ì—†ìŠµë‹ˆë‹¤.")

    return find_best_checkpoint(output_dir)


@dataclass
class EnsembleConfig:
    """ì•™ìƒë¸” ì„¤ì •"""

    model_paths: List[str]  # ëª¨ë¸ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
    weights: Optional[List[float]]  # ëª¨ë¸ë³„ ê°€ì¤‘ì¹˜ (Noneì´ë©´ ê· ë“±)
    output_dir: str  # ê²°ê³¼ ì €ì¥ ê²½ë¡œ
    test_dataset_path: str  # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ê²½ë¡œ
    max_seq_length: int = 384
    doc_stride: int = 128
    max_answer_length: int = 30
    top_k_retrieval: int = 10
    batch_size: int = 16
    use_retrieval: bool = True
    use_cache: bool = True  # Retrieval ìºì‹œ ì‚¬ìš© ì—¬ë¶€
    retrieval_alpha: float = 0.35  # WeightedHybridRetrievalì˜ BM25 ê°€ì¤‘ì¹˜
    corpus_emb_path: Optional[str] = None
    passages_meta_path: Optional[str] = None
    inference_split: str = "test"  # test / validation


class MRCEnsemble:
    """MRC ëª¨ë¸ ì•™ìƒë¸” í´ë˜ìŠ¤"""

    def __init__(self, config: EnsembleConfig):
        self.config = config
        self.models = []
        self.tokenizers = []
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # ê°€ì¤‘ì¹˜ ì„¤ì • (Noneì´ë©´ ê· ë“± ë¶„ë°°)
        if config.weights is None:
            self.weights = [1.0 / len(config.model_paths)] * len(config.model_paths)
        else:
            # ê°€ì¤‘ì¹˜ ì •ê·œí™”
            total = sum(config.weights)
            self.weights = [w / total for w in config.weights]

        logger.info(f"ğŸ”§ Device: {self.device}")
        logger.info(f"ğŸ“Š Model weights: {self.weights}")

    def load_models(self):
        """ëª¨ë“  ëª¨ë¸ ë¡œë“œ"""
        print("\n" + "=" * 60)
        print("ğŸ“¦ Loading models for ensemble...")
        print("=" * 60)

        for i, model_path in enumerate(self.config.model_paths):
            print(f"\n[{i + 1}/{len(self.config.model_paths)}] Loading: {model_path}")

            # Best checkpoint ìë™ íƒìƒ‰
            try:
                actual_path = find_best_checkpoint(model_path)
                if actual_path != model_path:
                    print(f"   ğŸ“ Found checkpoint: {actual_path}")
            except FileNotFoundError:
                actual_path = model_path  # ê·¸ëŒ€ë¡œ ì‹œë„

            # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
            config = AutoConfig.from_pretrained(actual_path)
            tokenizer = AutoTokenizer.from_pretrained(actual_path, use_fast=True)
            model = AutoModelForQuestionAnswering.from_pretrained(
                actual_path, config=config
            )
            model.to(self.device)
            model.eval()

            self.models.append(model)
            self.tokenizers.append(tokenizer)

            print(f"   âœ… Loaded: {config.model_type}")

        # í† í¬ë‚˜ì´ì € ì¼ê´€ì„± ê²€ì¦
        if len(self.tokenizers) > 1:
            base_vocab_size = len(self.tokenizers[0])
            for i, tok in enumerate(self.tokenizers[1:], 2):
                if len(tok) != base_vocab_size:
                    logger.warning(
                        f"âš ï¸ í† í¬ë‚˜ì´ì € vocab size ë¶ˆì¼ì¹˜: "
                        f"Model 1={base_vocab_size}, Model {i}={len(tok)}"
                    )

        print(f"\nâœ… Total {len(self.models)} models loaded!")

    def load_dataset(self) -> DatasetDict:
        """í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë¡œë“œ"""
        print(f"\nğŸ“‚ Loading dataset from: {self.config.test_dataset_path}")
        datasets = load_from_disk(self.config.test_dataset_path)
        print(f"   Dataset: {datasets}")
        return datasets

    def load_retrieval_from_cache(self, dataset: Dataset) -> Dataset:
        """ìºì‹œëœ retrieval ê²°ê³¼ ë¡œë“œ (inference.pyì™€ ë™ì¼í•œ ë¡œì§)"""
        # ìºì‹œ ê²½ë¡œ ê²°ì •
        if self.config.inference_split == "test":
            cache_path = get_path("test_cache")
        else:
            cache_path = get_path("val_cache")

        if not os.path.exists(cache_path):
            return None

        logger.info(f"ğŸ“¦ Loading retrieval cache from: {cache_path}")

        # ìºì‹œ ë¡œë“œ
        cache = {}
        with open(cache_path, "r", encoding="utf-8") as f:
            for line in f:
                item = json.loads(line.strip())
                cache[item["id"]] = item

        # Passages corpus ë¡œë“œ
        passages_meta_path = self.config.passages_meta_path or get_path(
            "kure_passages_meta"
        )
        wiki_path = get_path("wiki_corpus")

        if passages_meta_path and os.path.exists(passages_meta_path):
            passage_texts = []
            with open(passages_meta_path, "r", encoding="utf-8") as f:
                for line in f:
                    meta = json.loads(line.strip())
                    passage_texts.append(meta["text"])
        else:
            with open(wiki_path, "r", encoding="utf-8") as f:
                wiki = json.load(f)
            unique_texts = {}
            for doc_id, doc_info in wiki.items():
                text = doc_info["text"]
                if text not in unique_texts:
                    unique_texts[text] = text
            passage_texts = list(unique_texts.keys())

        # ê²°ê³¼ êµ¬ì„±
        result_data = {"id": [], "question": [], "context": []}
        top_k = self.config.top_k_retrieval
        alpha = self.config.retrieval_alpha

        for example in dataset:
            qid = example["id"]
            cache_entry = cache.get(qid)

            if cache_entry is None:
                logger.warning(f"âš ï¸ Cache miss for {qid}")
                context = ""
            else:
                candidates = cache_entry["retrieved"]
                if candidates:
                    bm25_scores = np.array([c["score_bm25"] for c in candidates])
                    dense_scores = np.array([c["score_dense"] for c in candidates])

                    eps = 1e-9
                    bm25_n = (bm25_scores - bm25_scores.min()) / (
                        bm25_scores.max() - bm25_scores.min() + eps
                    )
                    dense_n = (dense_scores - dense_scores.min()) / (
                        dense_scores.max() - dense_scores.min() + eps
                    )
                    hybrid_scores = alpha * bm25_n + (1 - alpha) * dense_n

                    sorted_indices = np.argsort(hybrid_scores)[::-1][:top_k]
                    contexts = []
                    for idx in sorted_indices:
                        passage_id = candidates[idx]["passage_id"]
                        if passage_id < len(passage_texts):
                            contexts.append(passage_texts[passage_id])
                    context = " ".join(contexts)
                else:
                    context = ""

            result_data["id"].append(qid)
            result_data["question"].append(example["question"])
            result_data["context"].append(context)

        features = Features(
            {
                "id": Value(dtype="string"),
                "question": Value(dtype="string"),
                "context": Value(dtype="string"),
            }
        )

        return Dataset.from_dict(result_data, features=features)

    def run_retrieval(self, datasets: DatasetDict) -> DatasetDict:
        """Weighted Hybrid Retrieval ìˆ˜í–‰ (BM25 + KURE)"""
        if not self.config.use_retrieval:
            return datasets

        print("\nğŸ” Running Weighted Hybrid Retrieval (BM25 + KURE)...")

        # ìºì‹œ ì‚¬ìš© ì‹œë„
        if self.config.use_cache:
            cached_dataset = self.load_retrieval_from_cache(datasets["validation"])
            if cached_dataset is not None:
                datasets = DatasetDict({"validation": cached_dataset})
                print(
                    f"   âœ… Loaded from cache: {len(datasets['validation'])} examples"
                )
                return datasets
            else:
                logger.info("âš ï¸ Cache not found, running live retrieval...")

        # ì‹¤ì‹œê°„ retrieval
        tokenizer = self.tokenizers[0]

        corpus_emb_path = self.config.corpus_emb_path or get_path("kure_corpus_emb")
        passages_meta_path = self.config.passages_meta_path or get_path(
            "kure_passages_meta"
        )

        retriever = WeightedHybridRetrieval(
            tokenize_fn=tokenizer.tokenize,
            data_path="./data",
            context_path="wikipedia_documents.json",
            corpus_emb_path=corpus_emb_path,
            passages_meta_path=passages_meta_path,
            alpha=self.config.retrieval_alpha,
        )
        retriever.build()

        df = retriever.retrieve(
            datasets["validation"], topk=self.config.top_k_retrieval
        )

        # DataFrameì„ Datasetìœ¼ë¡œ ë³€í™˜
        f = Features(
            {
                "context": Value(dtype="string", id=None),
                "id": Value(dtype="string", id=None),
                "question": Value(dtype="string", id=None),
            }
        )

        datasets = DatasetDict({"validation": Dataset.from_pandas(df, features=f)})
        print(f"   âœ… Retrieval complete: {len(datasets['validation'])} examples")

        return datasets

    def prepare_features(self, examples, tokenizer):
        """í† í°í™” ë° feature ìƒì„±"""
        pad_on_right = tokenizer.padding_side == "right"

        tokenized = tokenizer(
            examples["question"] if pad_on_right else examples["context"],
            examples["context"] if pad_on_right else examples["question"],
            truncation="only_second" if pad_on_right else "only_first",
            max_length=self.config.max_seq_length,
            stride=self.config.doc_stride,
            return_overflowing_tokens=True,
            return_offsets_mapping=True,
            padding="max_length",
        )

        sample_mapping = tokenized.pop("overflow_to_sample_mapping")
        tokenized["example_id"] = []

        for i in range(len(tokenized["input_ids"])):
            sequence_ids = tokenized.sequence_ids(i)
            context_index = 1 if pad_on_right else 0
            sample_index = sample_mapping[i]
            tokenized["example_id"].append(examples["id"][sample_index])

            tokenized["offset_mapping"][i] = [
                (o if sequence_ids[k] == context_index else None)
                for k, o in enumerate(tokenized["offset_mapping"][i])
            ]

        return tokenized

    def get_logits_from_model(
        self, model, tokenizer, dataset
    ) -> Tuple[np.ndarray, np.ndarray]:
        """ë‹¨ì¼ ëª¨ë¸ì—ì„œ logits ì¶”ì¶œ"""

        # Feature ì¤€ë¹„
        features = dataset.map(
            lambda x: self.prepare_features(x, tokenizer),
            batched=True,
            remove_columns=dataset.column_names,
            load_from_cache_file=False,
        )

        # token_type_ids ì²˜ë¦¬
        model_type = getattr(model.config, "model_type", "").lower()
        type_vocab_size = getattr(model.config, "type_vocab_size", 0)
        use_token_type_ids = type_vocab_size > 1

        # DataLoader ì¤€ë¹„
        data_collator = DataCollatorWithPadding(tokenizer)

        all_start_logits = []
        all_end_logits = []

        # ë°°ì¹˜ ì²˜ë¦¬
        batch_size = self.config.batch_size

        for i in tqdm(range(0, len(features), batch_size), desc="Inference"):
            batch_indices = range(i, min(i + batch_size, len(features)))

            # ë°°ì¹˜ ë°ì´í„° ì¤€ë¹„
            batch = {
                "input_ids": torch.tensor(
                    [features[j]["input_ids"] for j in batch_indices]
                ),
                "attention_mask": torch.tensor(
                    [features[j]["attention_mask"] for j in batch_indices]
                ),
            }

            if use_token_type_ids and "token_type_ids" in features.column_names:
                batch["token_type_ids"] = torch.tensor(
                    [features[j]["token_type_ids"] for j in batch_indices]
                )

            # GPUë¡œ ì´ë™
            batch = {k: v.to(self.device) for k, v in batch.items()}

            # ì¶”ë¡ 
            with torch.no_grad():
                outputs = model(**batch)

            all_start_logits.append(outputs.start_logits.cpu().numpy())
            all_end_logits.append(outputs.end_logits.cpu().numpy())

        start_logits = np.concatenate(all_start_logits, axis=0)
        end_logits = np.concatenate(all_end_logits, axis=0)

        return start_logits, end_logits, features

    def ensemble_logits(
        self, all_start_logits: List[np.ndarray], all_end_logits: List[np.ndarray]
    ) -> Tuple[np.ndarray, np.ndarray]:
        """Weighted sumìœ¼ë¡œ logits ì•™ìƒë¸”"""

        print("\nğŸ¯ Ensembling logits with weighted sum...")

        # Weighted sum
        ensembled_start = np.zeros_like(all_start_logits[0])
        ensembled_end = np.zeros_like(all_end_logits[0])

        for i, (start, end) in enumerate(zip(all_start_logits, all_end_logits)):
            ensembled_start += self.weights[i] * start
            ensembled_end += self.weights[i] * end
            print(f"   Model {i + 1}: weight={self.weights[i]:.3f}")

        return ensembled_start, ensembled_end

    def run(self):
        """ì•™ìƒë¸” ì‹¤í–‰"""
        print("\n" + "=" * 60)
        print("ğŸš€ MRC Ensemble (Soft Voting)")
        print("=" * 60)

        # 1. ëª¨ë¸ ë¡œë“œ
        self.load_models()

        # 2. ë°ì´í„°ì…‹ ë¡œë“œ
        datasets = self.load_dataset()

        # 3. Retrieval ìˆ˜í–‰
        datasets = self.run_retrieval(datasets)

        # 4. ê° ëª¨ë¸ì—ì„œ logits ì¶”ì¶œ
        print("\n" + "=" * 60)
        print("ğŸ“Š Extracting logits from each model...")
        print("=" * 60)

        all_start_logits = []
        all_end_logits = []
        features = None

        for i, (model, tokenizer) in enumerate(zip(self.models, self.tokenizers)):
            print(f"\n[Model {i + 1}/{len(self.models)}]")
            start_logits, end_logits, features = self.get_logits_from_model(
                model, tokenizer, datasets["validation"]
            )
            all_start_logits.append(start_logits)
            all_end_logits.append(end_logits)
            print(
                f"   Logits shape: start={start_logits.shape}, end={end_logits.shape}"
            )

        # 5. ì•™ìƒë¸”
        ensembled_start, ensembled_end = self.ensemble_logits(
            all_start_logits, all_end_logits
        )

        # 6. í›„ì²˜ë¦¬ ë° ë‹µë³€ ìƒì„±
        print("\n" + "=" * 60)
        print("ğŸ“ Post-processing predictions...")
        print("=" * 60)

        os.makedirs(self.config.output_dir, exist_ok=True)

        predictions = postprocess_qa_predictions(
            examples=datasets["validation"],
            features=features,
            predictions=(ensembled_start, ensembled_end),
            max_answer_length=self.config.max_answer_length,
            output_dir=self.config.output_dir,
            prefix="ensemble",
        )

        # 7. CSV ì €ì¥
        csv_path = os.path.join(self.config.output_dir, "ensemble_predictions.csv")
        with open(csv_path, "w", encoding="utf-8") as f:
            writer = csv.writer(f, delimiter="\t")
            for key, value in predictions.items():
                writer.writerow([key, value])

        print(f"\nâœ… Ensemble complete!")
        print(
            f"   ğŸ“„ Predictions: {os.path.join(self.config.output_dir, 'predictions_ensemble.json')}"
        )
        print(f"   ğŸ“„ CSV: {csv_path}")

        return predictions


def main():
    parser = argparse.ArgumentParser(
        description="MRC Model Ensemble (Soft Voting with Weighted Sum)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ:
  # 1. ì§ì ‘ ëª¨ë¸ ê²½ë¡œ ì§€ì •
  python ensemble.py --model_paths ./outputs/model1 ./outputs/model2 --weights 0.5 0.5

  # 2. YAML config íŒŒì¼ ì‚¬ìš© (ì—¬ëŸ¬ ì‹¤í—˜ ê²°ê³¼ ì•™ìƒë¸”)
  python ensemble.py --configs configs/active/exp1.yaml configs/active/exp2.yaml

  # 3. íŒŒì¼ ìƒë‹¨ì˜ ENSEMBLE_MODELS ë¦¬ìŠ¤íŠ¸ ì‚¬ìš©
  python ensemble.py
        """,
    )
    parser.add_argument(
        "--model_paths",
        nargs="+",
        default=None,
        help="í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œë“¤ (best checkpoint ìë™ íƒìƒ‰)",
    )
    parser.add_argument(
        "--configs",
        nargs="+",
        default=None,
        help="YAML config íŒŒì¼ ê²½ë¡œë“¤ (output_dirì—ì„œ ëª¨ë¸ ìë™ íƒìƒ‰)",
    )
    parser.add_argument(
        "--weights",
        nargs="+",
        type=float,
        default=None,
        help="ëª¨ë¸ë³„ ê°€ì¤‘ì¹˜ (ë¯¸ì§€ì •ì‹œ ê· ë“± ë¶„ë°°)",
    )
    parser.add_argument(
        "--output_dir", type=str, default="./outputs/ensemble", help="ê²°ê³¼ ì €ì¥ ê²½ë¡œ"
    )
    parser.add_argument(
        "--test_dataset",
        type=str,
        default="./data/test_dataset",
        help="í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ê²½ë¡œ",
    )
    parser.add_argument(
        "--split",
        type=str,
        choices=["test", "validation"],
        default="test",
        help="inference split (test: ì œì¶œìš©, validation: í‰ê°€ìš©)",
    )
    parser.add_argument("--top_k", type=int, default=10, help="Retrieval top-k")
    parser.add_argument(
        "--doc_stride", type=int, default=128, help="Document stride for tokenization"
    )
    parser.add_argument("--batch_size", type=int, default=16, help="ë°°ì¹˜ ì‚¬ì´ì¦ˆ")
    parser.add_argument(
        "--no_retrieval",
        action="store_true",
        help="Retrieval ì‚¬ìš© ì•ˆí•¨ (gold context ì‚¬ìš©)",
    )
    parser.add_argument(
        "--no_cache",
        action="store_true",
        help="Retrieval ìºì‹œ ì‚¬ìš© ì•ˆí•¨ (í•­ìƒ ì‹¤ì‹œê°„ retrieval)",
    )
    parser.add_argument(
        "--retrieval_alpha",
        type=float,
        default=0.35,
        help="WeightedHybridRetrievalì˜ BM25 ê°€ì¤‘ì¹˜ (0~1)",
    )

    args = parser.parse_args()

    # ëª¨ë¸ ê²½ë¡œì™€ ê°€ì¤‘ì¹˜ ê²°ì • (ìš°ì„ ìˆœìœ„: --model_paths > --configs > ENSEMBLE_MODELS)
    model_paths = []
    weights = args.weights

    if args.model_paths is not None:
        # 1. ì§ì ‘ ê²½ë¡œ ì§€ì •
        model_paths = args.model_paths
        logger.info("ğŸ“‹ Using model paths from command line")

    elif args.configs is not None:
        # 2. YAML configì—ì„œ ì¶”ì¶œ
        logger.info("ğŸ“‹ Extracting model paths from YAML configs...")
        for config_path in args.configs:
            config = load_config_from_yaml(config_path)
            try:
                model_path = get_model_path_from_config(config)
                model_paths.append(model_path)
                logger.info(f"   âœ… {config_path} -> {model_path}")
            except Exception as e:
                logger.error(f"   âŒ {config_path}: {e}")
                sys.exit(1)

    elif ENSEMBLE_MODELS:
        # 3. ìƒë‹¨ì˜ ENSEMBLE_MODELS ë¦¬ìŠ¤íŠ¸ ì‚¬ìš©
        model_paths = [path for path, _ in ENSEMBLE_MODELS]
        weights = [weight for _, weight in ENSEMBLE_MODELS]
        logger.info("ğŸ“‹ Using ENSEMBLE_MODELS from script")

    else:
        raise ValueError(
            "âŒ ì•™ìƒë¸”í•  ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤!\n"
            "ğŸ’¡ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”:\n"
            "   1. --model_paths ./outputs/model1 ./outputs/model2\n"
            "   2. --configs configs/exp1.yaml configs/exp2.yaml\n"
            "   3. ensemble.py ìƒë‹¨ì˜ ENSEMBLE_MODELS ë¦¬ìŠ¤íŠ¸"
        )

    if not model_paths:
        raise ValueError("âŒ ìœ íš¨í•œ ëª¨ë¸ ê²½ë¡œê°€ ì—†ìŠµë‹ˆë‹¤.")

    # ë°ì´í„°ì…‹ ê²½ë¡œ ê²°ì •
    if args.split == "validation":
        test_dataset_path = "./data/train_dataset"  # validation split í¬í•¨
    else:
        test_dataset_path = args.test_dataset

    print("\n" + "=" * 60)
    print("ğŸ“‹ Ensemble Configuration")
    print("=" * 60)
    print(f"   Split: {args.split}")
    print(f"   Dataset: {test_dataset_path}")
    print(
        f"   Retrieval: {'Enabled' if not args.no_retrieval else 'Disabled (gold context)'}"
    )
    print(f"   Cache: {'Enabled' if not args.no_cache else 'Disabled'}")
    print(f"   Top-k: {args.top_k}")
    print(f"   Alpha: {args.retrieval_alpha}")
    print(f"   Doc stride: {args.doc_stride}")
    print("-" * 60)
    print("   Models:")
    for i, path in enumerate(model_paths):
        w = weights[i] if weights else 1.0
        print(f"   [{i + 1}] {path} (weight: {w})")
    print("=" * 60)

    # ì„¤ì • ìƒì„±
    config = EnsembleConfig(
        model_paths=model_paths,
        weights=weights,
        output_dir=args.output_dir,
        test_dataset_path=test_dataset_path,
        top_k_retrieval=args.top_k,
        doc_stride=args.doc_stride,
        batch_size=args.batch_size,
        use_retrieval=not args.no_retrieval,
        use_cache=not args.no_cache,
        retrieval_alpha=args.retrieval_alpha,
        inference_split=args.split,
    )

    # ì•™ìƒë¸” ì‹¤í–‰
    ensemble = MRCEnsemble(config)
    predictions = ensemble.run()

    print("\n" + "=" * 60)
    print("ğŸ‰ Ensemble finished successfully!")
    print(f"   ğŸ“„ Output: {args.output_dir}")
    print("=" * 60)


if __name__ == "__main__":
    main()
