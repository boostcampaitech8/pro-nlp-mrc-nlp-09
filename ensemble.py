"""
MRC ëª¨ë¸ ì•™ìƒë¸” (Soft Voting with Weighted Sum)

ì—¬ëŸ¬ í•™ìŠµëœ ëª¨ë¸ì˜ start/end logitsë¥¼ weighted sumí•˜ì—¬ ì•™ìƒë¸” ìˆ˜í–‰
"""

import os
import json
import csv
import argparse
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass

import torch
import numpy as np
from tqdm import tqdm
from datasets import load_from_disk, DatasetDict, Dataset, Features, Value, Sequence
from transformers import (
    AutoConfig,
    AutoModelForQuestionAnswering,
    AutoTokenizer,
    DataCollatorWithPadding,
)

from src.retrieval.weighted_hybrid import WeightedHybridRetrieval
from src.utils.qa import postprocess_qa_predictions


# ============================================================
# ğŸ¯ ì—¬ê¸°ì„œ ì•™ìƒë¸”í•  ëª¨ë¸ë“¤ì„ ì„¤ì •í•˜ì„¸ìš”!
# ============================================================
ENSEMBLE_MODELS = [
    # (ëª¨ë¸ ê²½ë¡œ, ê°€ì¤‘ì¹˜)
    # ê°€ì¤‘ì¹˜ëŠ” ìë™ìœ¼ë¡œ ì •ê·œí™”ë©ë‹ˆë‹¤ (í•©ì´ 1ì´ ë˜ë„ë¡)
    ("./outputs/taewon/oceann315", 1.0),
    ("./outputs/taewon/roberta-large", 1.0),
    # ("./outputs/dahyeong/model", 0.5),
    
    # ğŸ’¡ ê°€ì¤‘ì¹˜ ì˜ˆì‹œ:
    # - ê· ë“±: ëª¨ë‘ 1.0
    # - ì„±ëŠ¥ ê¸°ë°˜: EM ì ìˆ˜ì— ë¹„ë¡€ (ì˜ˆ: 75ì  â†’ 0.75, 80ì  â†’ 0.80)
    # - ìˆ˜ë™ ì¡°ì ˆ: ì›í•˜ëŠ” ë¹„ìœ¨ë¡œ ì„¤ì •
]
# ============================================================


@dataclass
class EnsembleConfig:
    """ì•™ìƒë¸” ì„¤ì •"""
    model_paths: List[str]          # ëª¨ë¸ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
    weights: Optional[List[float]]  # ëª¨ë¸ë³„ ê°€ì¤‘ì¹˜ (Noneì´ë©´ ê· ë“±)
    output_dir: str                 # ê²°ê³¼ ì €ì¥ ê²½ë¡œ
    test_dataset_path: str          # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ê²½ë¡œ
    max_seq_length: int = 384
    doc_stride: int = 128
    max_answer_length: int = 30
    top_k_retrieval: int = 10
    batch_size: int = 16
    use_retrieval: bool = True
    retrieval_alpha: float = 0.35  # WeightedHybridRetrievalì˜ BM25 ê°€ì¤‘ì¹˜ (base.yamlê³¼ ë™ì¼)
    corpus_emb_path: Optional[str] = "./data/embeddings/kure_corpus_emb.npy"  # KURE corpus embedding ê²½ë¡œ
    passages_meta_path: Optional[str] = "./data/embeddings/kure_passages_meta.jsonl"  # KURE passages meta ê²½ë¡œ


class MRCEnsemble:
    """MRC ëª¨ë¸ ì•™ìƒë¸” í´ë˜ìŠ¤"""
    
    def __init__(self, config: EnsembleConfig):
        self.config = config
        self.models = []
        self.tokenizers = []
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # ê°€ì¤‘ì¹˜ ì„¤ì • (Noneì´ë©´ ê· ë“± ë¶„ë°°)
        if config.weights is None:
            self.weights = [1.0 / len(config.model_paths)] * len(config.model_paths)
        else:
            # ê°€ì¤‘ì¹˜ ì •ê·œí™”
            total = sum(config.weights)
            self.weights = [w / total for w in config.weights]
        
        print(f"ğŸ”§ Device: {self.device}")
        print(f"ğŸ“Š Model weights: {self.weights}")
    
    def load_models(self):
        """ëª¨ë“  ëª¨ë¸ ë¡œë“œ"""
        print("\n" + "=" * 60)
        print("ğŸ“¦ Loading models for ensemble...")
        print("=" * 60)
        
        for i, model_path in enumerate(self.config.model_paths):
            print(f"\n[{i+1}/{len(self.config.model_paths)}] Loading: {model_path}")
            
            # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
            config = AutoConfig.from_pretrained(model_path)
            tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)
            model = AutoModelForQuestionAnswering.from_pretrained(model_path, config=config)
            model.to(self.device)
            model.eval()
            
            self.models.append(model)
            self.tokenizers.append(tokenizer)
            
            print(f"   âœ… Loaded: {config.model_type}")
        
        print(f"\nâœ… Total {len(self.models)} models loaded!")
    
    def load_dataset(self) -> DatasetDict:
        """í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë¡œë“œ"""
        print(f"\nğŸ“‚ Loading dataset from: {self.config.test_dataset_path}")
        datasets = load_from_disk(self.config.test_dataset_path)
        print(f"   Dataset: {datasets}")
        return datasets
    
    def run_retrieval(self, datasets: DatasetDict) -> DatasetDict:
        """Weighted Hybrid Retrieval ìˆ˜í–‰ (BM25 + KURE)"""
        if not self.config.use_retrieval:
            return datasets
        
        print("\nğŸ” Running Weighted Hybrid Retrieval (BM25 + KURE)...")
        
        # ì²« ë²ˆì§¸ í† í¬ë‚˜ì´ì € ì‚¬ìš©
        tokenizer = self.tokenizers[0]
        
        # ê¸°ë³¸ ê²½ë¡œ ì„¤ì • (base.yamlê³¼ ë™ì¼)
        corpus_emb_path = self.config.corpus_emb_path or "./data/embeddings/kure_corpus_emb.npy"
        passages_meta_path = self.config.passages_meta_path or "./data/embeddings/kure_passages_meta.jsonl"
        
        retriever = WeightedHybridRetrieval(
            tokenize_fn=tokenizer.tokenize,
            data_path="./data",
            context_path="wikipedia_documents_normalized.json",
            corpus_emb_path=corpus_emb_path,
            passages_meta_path=passages_meta_path,
            alpha=self.config.retrieval_alpha,
        )
        retriever.build()
        
        df = retriever.retrieve(
            datasets["validation"], 
            topk=self.config.top_k_retrieval
        )
        
        # DataFrameì„ Datasetìœ¼ë¡œ ë³€í™˜
        f = Features({
            "context": Value(dtype="string", id=None),
            "id": Value(dtype="string", id=None),
            "question": Value(dtype="string", id=None),
        })
        
        datasets = DatasetDict({"validation": Dataset.from_pandas(df, features=f)})
        print(f"   âœ… Retrieval complete: {len(datasets['validation'])} examples")
        
        return datasets
    
    def prepare_features(self, examples, tokenizer):
        """í† í°í™” ë° feature ìƒì„±"""
        pad_on_right = tokenizer.padding_side == "right"
        
        tokenized = tokenizer(
            examples["question"] if pad_on_right else examples["context"],
            examples["context"] if pad_on_right else examples["question"],
            truncation="only_second" if pad_on_right else "only_first",
            max_length=self.config.max_seq_length,
            stride=self.config.doc_stride,
            return_overflowing_tokens=True,
            return_offsets_mapping=True,
            padding="max_length",
        )
        
        sample_mapping = tokenized.pop("overflow_to_sample_mapping")
        tokenized["example_id"] = []
        
        for i in range(len(tokenized["input_ids"])):
            sequence_ids = tokenized.sequence_ids(i)
            context_index = 1 if pad_on_right else 0
            sample_index = sample_mapping[i]
            tokenized["example_id"].append(examples["id"][sample_index])
            
            tokenized["offset_mapping"][i] = [
                (o if sequence_ids[k] == context_index else None)
                for k, o in enumerate(tokenized["offset_mapping"][i])
            ]
        
        return tokenized
    
    def get_logits_from_model(
        self, 
        model, 
        tokenizer, 
        dataset
    ) -> Tuple[np.ndarray, np.ndarray]:
        """ë‹¨ì¼ ëª¨ë¸ì—ì„œ logits ì¶”ì¶œ"""
        
        # Feature ì¤€ë¹„
        features = dataset.map(
            lambda x: self.prepare_features(x, tokenizer),
            batched=True,
            remove_columns=dataset.column_names,
            load_from_cache_file=False,
        )
        
        # token_type_ids ì²˜ë¦¬
        model_type = getattr(model.config, "model_type", "").lower()
        type_vocab_size = getattr(model.config, "type_vocab_size", 0)
        use_token_type_ids = type_vocab_size > 1
        
        # DataLoader ì¤€ë¹„
        data_collator = DataCollatorWithPadding(tokenizer)
        
        all_start_logits = []
        all_end_logits = []
        
        # ë°°ì¹˜ ì²˜ë¦¬
        batch_size = self.config.batch_size
        
        for i in tqdm(range(0, len(features), batch_size), desc="Inference"):
            batch_indices = range(i, min(i + batch_size, len(features)))
            
            # ë°°ì¹˜ ë°ì´í„° ì¤€ë¹„
            batch = {
                "input_ids": torch.tensor([features[j]["input_ids"] for j in batch_indices]),
                "attention_mask": torch.tensor([features[j]["attention_mask"] for j in batch_indices]),
            }
            
            if use_token_type_ids and "token_type_ids" in features.column_names:
                batch["token_type_ids"] = torch.tensor(
                    [features[j]["token_type_ids"] for j in batch_indices]
                )
            
            # GPUë¡œ ì´ë™
            batch = {k: v.to(self.device) for k, v in batch.items()}
            
            # ì¶”ë¡ 
            with torch.no_grad():
                outputs = model(**batch)
            
            all_start_logits.append(outputs.start_logits.cpu().numpy())
            all_end_logits.append(outputs.end_logits.cpu().numpy())
        
        start_logits = np.concatenate(all_start_logits, axis=0)
        end_logits = np.concatenate(all_end_logits, axis=0)
        
        return start_logits, end_logits, features
    
    def ensemble_logits(
        self, 
        all_start_logits: List[np.ndarray], 
        all_end_logits: List[np.ndarray]
    ) -> Tuple[np.ndarray, np.ndarray]:
        """Weighted sumìœ¼ë¡œ logits ì•™ìƒë¸”"""
        
        print("\nğŸ¯ Ensembling logits with weighted sum...")
        
        # Weighted sum
        ensembled_start = np.zeros_like(all_start_logits[0])
        ensembled_end = np.zeros_like(all_end_logits[0])
        
        for i, (start, end) in enumerate(zip(all_start_logits, all_end_logits)):
            ensembled_start += self.weights[i] * start
            ensembled_end += self.weights[i] * end
            print(f"   Model {i+1}: weight={self.weights[i]:.3f}")
        
        return ensembled_start, ensembled_end
    
    def run(self):
        """ì•™ìƒë¸” ì‹¤í–‰"""
        print("\n" + "=" * 60)
        print("ğŸš€ MRC Ensemble (Soft Voting)")
        print("=" * 60)
        
        # 1. ëª¨ë¸ ë¡œë“œ
        self.load_models()
        
        # 2. ë°ì´í„°ì…‹ ë¡œë“œ
        datasets = self.load_dataset()
        
        # 3. Retrieval ìˆ˜í–‰
        datasets = self.run_retrieval(datasets)
        
        # 4. ê° ëª¨ë¸ì—ì„œ logits ì¶”ì¶œ
        print("\n" + "=" * 60)
        print("ğŸ“Š Extracting logits from each model...")
        print("=" * 60)
        
        all_start_logits = []
        all_end_logits = []
        features = None
        
        for i, (model, tokenizer) in enumerate(zip(self.models, self.tokenizers)):
            print(f"\n[Model {i+1}/{len(self.models)}]")
            start_logits, end_logits, features = self.get_logits_from_model(
                model, tokenizer, datasets["validation"]
            )
            all_start_logits.append(start_logits)
            all_end_logits.append(end_logits)
            print(f"   Logits shape: start={start_logits.shape}, end={end_logits.shape}")
        
        # 5. ì•™ìƒë¸”
        ensembled_start, ensembled_end = self.ensemble_logits(
            all_start_logits, all_end_logits
        )
        
        # 6. í›„ì²˜ë¦¬ ë° ë‹µë³€ ìƒì„±
        print("\n" + "=" * 60)
        print("ğŸ“ Post-processing predictions...")
        print("=" * 60)
        
        os.makedirs(self.config.output_dir, exist_ok=True)
        
        predictions = postprocess_qa_predictions(
            examples=datasets["validation"],
            features=features,
            predictions=(ensembled_start, ensembled_end),
            max_answer_length=self.config.max_answer_length,
            output_dir=self.config.output_dir,
            prefix="ensemble",
        )
        
        # 7. CSV ì €ì¥
        csv_path = os.path.join(self.config.output_dir, "ensemble_predictions.csv")
        with open(csv_path, "w", encoding="utf-8") as f:
            writer = csv.writer(f, delimiter="\t")
            for key, value in predictions.items():
                writer.writerow([key, value])
        
        print(f"\nâœ… Ensemble complete!")
        print(f"   ğŸ“„ Predictions: {os.path.join(self.config.output_dir, 'predictions_ensemble.json')}")
        print(f"   ğŸ“„ CSV: {csv_path}")
        
        return predictions


def main():
    parser = argparse.ArgumentParser(description="MRC Model Ensemble")
    parser.add_argument(
        "--model_paths", 
        nargs="+", 
        default=None,
        help="í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œë“¤ (ë¯¸ì§€ì •ì‹œ ENSEMBLE_MODELS ì‚¬ìš©)"
    )
    parser.add_argument(
        "--weights",
        nargs="+",
        type=float,
        default=None,
        help="ëª¨ë¸ë³„ ê°€ì¤‘ì¹˜ (ë¯¸ì§€ì •ì‹œ ENSEMBLE_MODELS ë˜ëŠ” ê· ë“± ë¶„ë°°)"
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        default="./outputs/taewon/ensemble",
        help="ê²°ê³¼ ì €ì¥ ê²½ë¡œ"
    )
    parser.add_argument(
        "--test_dataset",
        type=str,
        default="./data/test_dataset",
        help="í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ê²½ë¡œ"
    )
    parser.add_argument(
        "--top_k",
        type=int,
        default=10,
        help="Retrieval top-k"
    )
    parser.add_argument(
        "--batch_size",
        type=int,
        default=16,
        help="ë°°ì¹˜ ì‚¬ì´ì¦ˆ"
    )
    parser.add_argument(
        "--no_retrieval",
        action="store_true",
        help="Retrieval ì‚¬ìš© ì•ˆí•¨ (validationìš©)"
    )
    parser.add_argument(
        "--retrieval_alpha",
        type=float,
        default=0.35,
        help="WeightedHybridRetrievalì˜ BM25 ê°€ì¤‘ì¹˜ (0~1, ê¸°ë³¸ê°’: 0.35, base.yamlê³¼ ë™ì¼)"
    )
    parser.add_argument(
        "--corpus_emb_path",
        type=str,
        default="./data/embeddings/kure_corpus_emb.npy",
        help="KURE corpus embedding ê²½ë¡œ (ê¸°ë³¸ê°’: ./data/embeddings/kure_corpus_emb.npy, base.yamlê³¼ ë™ì¼)"
    )
    parser.add_argument(
        "--passages_meta_path",
        type=str,
        default="./data/embeddings/kure_passages_meta.jsonl",
        help="KURE passages meta ê²½ë¡œ (ê¸°ë³¸ê°’: ./data/embeddings/kure_passages_meta.jsonl, base.yamlê³¼ ë™ì¼)"
    )
    
    args = parser.parse_args()
    
    # ëª¨ë¸ ê²½ë¡œì™€ ê°€ì¤‘ì¹˜ ê²°ì •
    if args.model_paths is not None:
        # ì»¤ë§¨ë“œë¼ì¸ì—ì„œ ì§€ì •í•œ ê²½ìš°
        model_paths = args.model_paths
        weights = args.weights
    else:
        # ENSEMBLE_MODELSì—ì„œ ê°€ì ¸ì˜¤ê¸°
        if not ENSEMBLE_MODELS:
            raise ValueError(
                "âŒ ì•™ìƒë¸”í•  ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤!\n"
                "ğŸ’¡ ensemble.py ìƒë‹¨ì˜ ENSEMBLE_MODELSì— ëª¨ë¸ì„ ì¶”ê°€í•˜ê±°ë‚˜\n"
                "   --model_paths ì¸ìë¥¼ ì‚¬ìš©í•˜ì„¸ìš”."
            )
        model_paths = [path for path, _ in ENSEMBLE_MODELS]
        weights = [weight for _, weight in ENSEMBLE_MODELS]
    
    print("\n" + "=" * 60)
    print("ğŸ“‹ Ensemble Configuration")
    print("=" * 60)
    for i, (path, w) in enumerate(zip(model_paths, weights or [1.0]*len(model_paths))):
        print(f"   [{i+1}] {path} (weight: {w})")
    print("=" * 60)
    
    # ì„¤ì • ìƒì„±
    config = EnsembleConfig(
        model_paths=model_paths,
        weights=weights,
        output_dir=args.output_dir,
        test_dataset_path=args.test_dataset,
        top_k_retrieval=args.top_k,
        batch_size=args.batch_size,
        use_retrieval=not args.no_retrieval,
        retrieval_alpha=args.retrieval_alpha,
        corpus_emb_path=args.corpus_emb_path,
        passages_meta_path=args.passages_meta_path,
    )
    
    # ì•™ìƒë¸” ì‹¤í–‰
    ensemble = MRCEnsemble(config)
    predictions = ensemble.run()
    
    print("\n" + "=" * 60)
    print("ğŸ‰ Ensemble finished successfully!")
    print("=" * 60)


if __name__ == "__main__":
    main()

