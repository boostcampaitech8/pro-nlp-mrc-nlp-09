import json
import re

INPUT_PATH = "./data/data/wikipedia_documents.json"
OUTPUT_PATH = "./data/data/wikipedia_documents_normalized.json"

def clean_wiki(text: str) -> str:
    if not isinstance(text, str):
        return text
    
    # 1) HTML íƒœê·¸ ì œê±°
    text = re.sub(r"<[^>]+>", " ", text)

    # 2) ìœ„í‚¤ citation ìˆ«ì ì œê±° [1], [ì£¼ 2], [edit], [citation needed]
    text = re.sub(r"\[[^\]]*\]", " ", text)

    # 3) URL ì œê±°
    text = re.sub(r"http\S+|www\.\S+", " ", text)

    # 4) íŠ¹ìˆ˜ê¸°í˜¸ ì œê±°
    text = re.sub(r"[â—â˜…â– â—†â–¼â–²â–¶â–·â—€â—â€¦â€»]", " ", text)

    # 5) ë‹¤ì¤‘ ê³µë°± ì •ë¦¬
    text = re.sub(r"\s+", " ", text)

    return text.strip()


def normalize_wikipedia():
    print("ğŸ“‚ Loading wikipedia_documents.json ...")
    with open(INPUT_PATH, "r", encoding="utf-8") as f:
        wiki = json.load(f)

    print("ğŸ”§ Cleaning documents... (may take 10~20 sec)")
    cleaned = {}

    for doc_id, content in wiki.items():
        cleaned[doc_id] = {
            "title": clean_wiki(content["title"]),
            "text": clean_wiki(content["text"])
        }

    print(f"ğŸ’¾ Saving cleaned file â†’ {OUTPUT_PATH}")
    with open(OUTPUT_PATH, "w", encoding="utf-8") as f:
        json.dump(cleaned, f, ensure_ascii=False, indent=2)

    print("ğŸ‰ Done! Wikipedia normalized version saved.")


if __name__ == "__main__":
    normalize_wikipedia()
